# BíŒ€ ì‘ì—… ì§€ì‹œì„œ: LLM/Media Gateway êµ¬ì¶•

**ì‘ì„±ì¼**: 2025-11-16
**ë‹´ë‹¹ íŒ€**: BíŒ€ (Backend ê°œë°œ)
**í”„ë¡œì íŠ¸**: Sparklio v4 - AI Gateway ì•„í‚¤í…ì²˜ êµ¬ì¶•
**ì´ ì˜ˆìƒ ê¸°ê°„**: 5ì¼
**ìš°ì„ ìˆœìœ„**: ğŸ”´ **ìµœê³ **

---

## ğŸ¯ í•µì‹¬ ëª©í‘œ

**ì§€ê¸ˆ ë‹¹ì¥**: 006ë²ˆ ë°©ì‹ìœ¼ë¡œ ìµœì†Œ ë™ì‘ ë²„ì „ ì™„ì„± (Ollama + ComfyUI)
**ì¤‘ìš” ì›ì¹™**: â­ **í–¥í›„ í™•ì¥ì„ ê³ ë ¤í•œ ì„¤ê³„** (GPT, Claude, DALLÂ·E, Veo3 ë“±)

### âš ï¸ ì ˆëŒ€ ì›ì¹™ (ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ)

1. **í™•ì¥ ê°€ëŠ¥í•œ Provider íŒ¨í„´ ì‚¬ìš©**
   - í˜„ì¬ëŠ” Ollamaë§Œ êµ¬í˜„í•˜ì§€ë§Œ, OpenAI/Anthropic/Google ì¶”ê°€ ì‹œ **ì½”ë“œ ìˆ˜ì • ìµœì†Œí™”**
   - Provider ì¸í„°í˜ì´ìŠ¤ë¥¼ ëª…í™•íˆ ì •ì˜í•˜ê³  **ëª¨ë“  Providerê°€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„**

2. **ì„¤ì • ê¸°ë°˜ ë¼ìš°íŒ…**
   - ëª¨ë¸ ì„ íƒ ë¡œì§ì„ **í•˜ë“œì½”ë”© ê¸ˆì§€**
   - `config.yaml` ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ **Provider/ëª¨ë¸ ë³€ê²½ ê°€ëŠ¥**í•˜ê²Œ

3. **API Contract ë¶ˆë³€ì„±**
   - `/api/v1/llm/generate` ìŠ¤í™ì€ **Provider ì¶”ê°€ì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼**
   - ìƒìœ„ ë ˆì´ì–´(Agent, Editor)ëŠ” **Gatewayë§Œ ì˜ì¡´**, Provider ëª¨ë¦„

4. **ë¯¸ë˜ Providerë¥¼ ìœ„í•œ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ í¬í•¨**
   - `OpenAIProvider`, `AnthropicProvider`, `DalleProvider` ë“± **í´ë˜ìŠ¤ëŠ” ìƒì„±**
   - ì‹¤ì œ êµ¬í˜„ì€ `TODO` ì£¼ì„, ë‚˜ì¤‘ì— ì±„ìš°ê¸°ë§Œ í•˜ë©´ ë¨

---

## ğŸ“‹ ì‘ì—… ë²”ìœ„ ìš”ì•½

### Phase 1: Gateway ê¸°ì´ˆ êµ¬ì¶• (2.5ì¼, 19ì‹œê°„)
LLM Gateway + Media Gatewayë¥¼ Mock/Live ëª¨ë“œë¡œ êµ¬í˜„

### Phase 2: Agent ë¦¬íŒ©í„°ë§ (1.25ì¼, 10ì‹œê°„)
6ê°œ Agentê°€ Gatewayë§Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •

### Phase 3: E2E ìŠ¤í¬ë¦½íŠ¸ (0.75ì¼, 6ì‹œê°„)
"ìƒí’ˆ ìƒì„¸ + ì´ë¯¸ì§€ 1ì¥" ì „ì²´ í”Œë¡œìš° ìŠ¤í¬ë¦½íŠ¸

### Phase 4: í…ŒìŠ¤íŠ¸ ì§€ì› (0.25ì¼, 2ì‹œê°„)
Mock ì‘ë‹µ ê°œì„  ë° íƒ€ì„ì•„ì›ƒ ìµœì í™”

---

## ğŸ—ï¸ Phase 1: Gateway ê¸°ì´ˆ êµ¬ì¶• (19ì‹œê°„)

### ì‘ì—… 1.1: Backend ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (0.5ì‹œê°„)

**ëª©í‘œ**: í™•ì¥ ê°€ëŠ¥í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
backend/app/
â”œâ”€â”€ api/v1/endpoints/
â”‚   â”œâ”€â”€ llm_gateway.py          # LLM Gateway ì—”ë“œí¬ì¸íŠ¸
â”‚   â””â”€â”€ media_gateway.py        # Media Gateway ì—”ë“œí¬ì¸íŠ¸
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ gateway.py          # LLM Gateway ë©”ì¸ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ router.py           # role/task â†’ Provider/Model ë¼ìš°íŒ…
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py   # í”„ë¡¬í”„íŠ¸ ì •ê·œí™”
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â”œâ”€â”€ base.py         # LLMProvider ì¸í„°í˜ì´ìŠ¤ (ABC)
â”‚   â”‚       â”œâ”€â”€ ollama.py       # OllamaProvider êµ¬í˜„ âœ…
â”‚   â”‚       â”œâ”€â”€ openai.py       # OpenAIProvider ìŠ¤ì¼ˆë ˆí†¤ (TODO)
â”‚   â”‚       â”œâ”€â”€ anthropic.py    # AnthropicProvider ìŠ¤ì¼ˆë ˆí†¤ (TODO)
â”‚   â”‚       â””â”€â”€ gemini.py       # GeminiProvider ìŠ¤ì¼ˆë ˆí†¤ (TODO)
â”‚   â”‚
â”‚   â””â”€â”€ media/
â”‚       â”œâ”€â”€ gateway.py          # Media Gateway ë©”ì¸ ë¡œì§
â”‚       â””â”€â”€ providers/
â”‚           â”œâ”€â”€ base.py         # ImageProvider ì¸í„°í˜ì´ìŠ¤ (ABC)
â”‚           â”œâ”€â”€ comfyui.py      # ComfyUIProvider êµ¬í˜„ âœ…
â”‚           â”œâ”€â”€ dalle.py        # DalleProvider ìŠ¤ì¼ˆë ˆí†¤ (TODO)
â”‚           â””â”€â”€ nanobanana.py   # NanobananaProvider ìŠ¤ì¼ˆë ˆí†¤ (TODO)
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py               # GENERATOR_MODE, Provider ì„¤ì •
â”‚   â””â”€â”€ provider_config.yaml    # Provider í™œì„±í™”/ë¹„í™œì„±í™”, ëª¨ë¸ ë§µí•‘
â”‚
â””â”€â”€ schemas/
    â”œâ”€â”€ llm_gateway.py          # LLMGatewayRequest/Response
    â””â”€â”€ media_gateway.py        # ImageRequest/Response
```

**ì¤‘ìš”**:
- `providers/base.py`ëŠ” **Abstract Base Class(ABC)** ì‚¬ìš©
- ìƒˆ Provider ì¶”ê°€ ì‹œ **base.py ìˆ˜ì • ì—†ì´** ìƒˆ íŒŒì¼ë§Œ ì¶”ê°€

---

### ì‘ì—… 1.2: GENERATOR_MODE í™˜ê²½ë³€ìˆ˜ ì¶”ê°€ (0.5ì‹œê°„)

**íŒŒì¼**: `backend/.env`

```env
# ============================================
# Generator Mode (â­ í•µì‹¬ ì„¤ì •)
# ============================================
GENERATOR_MODE=mock  # mock | live

# ============================================
# LLM Providers
# ============================================

# Ollama (Desktop Docker) - í˜„ì¬ ì‚¬ìš© ì¤‘
OLLAMA_BASE_URL=http://100.120.180.42:11434
OLLAMA_DEFAULT_MODEL=qwen2.5:7b
OLLAMA_TIMEOUT=120

# OpenAI (ë¯¸ë˜ í™•ì¥)
# OPENAI_API_KEY=sk-...
# OPENAI_DEFAULT_MODEL=gpt-4o
# OPENAI_TIMEOUT=60

# Anthropic (ë¯¸ë˜ í™•ì¥)
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_TIMEOUT=60

# Google Gemini (ë¯¸ë˜ í™•ì¥)
# GOOGLE_API_KEY=AIza...
# GEMINI_DEFAULT_MODEL=gemini-2.0-flash-exp
# GEMINI_TIMEOUT=60

# ============================================
# Media Providers
# ============================================

# ComfyUI (Desktop Standalone) - í˜„ì¬ ì‚¬ìš© ì¤‘
COMFYUI_BASE_URL=http://100.120.180.42:8188
COMFYUI_WORKFLOW_DIR=workflows/
COMFYUI_TIMEOUT=300

# DALLÂ·E (ë¯¸ë˜ í™•ì¥)
# DALLE_API_KEY=sk-...  # OpenAI API í‚¤ ì¬ì‚¬ìš©
# DALLE_MODEL=dall-e-3
# DALLE_TIMEOUT=60

# Nanobanana (ë¯¸ë˜ í™•ì¥)
# NANOBANANA_API_KEY=...
# NANOBANANA_BASE_URL=https://api.nanobanana.ai/v1
# NANOBANANA_TIMEOUT=60
```

**ì¤‘ìš”**:
- ì£¼ì„ ì²˜ë¦¬ëœ Provider ì„¤ì •ë„ **ëª¨ë‘ í¬í•¨**
- ë‚˜ì¤‘ì— ì£¼ì„ í•´ì œë§Œ í•˜ë©´ í™œì„±í™”ë˜ë„ë¡

---

### ì‘ì—… 1.3: Provider ì„¤ì • íŒŒì¼ (YAML)

**íŒŒì¼**: `backend/app/core/provider_config.yaml`

```yaml
# ============================================
# Provider í™œì„±í™” ì„¤ì •
# ============================================
providers:
  llm:
    # í˜„ì¬ í™œì„±í™”ëœ Provider
    active:
      - ollama

    # ë¯¸ë˜ í™•ì¥ìš© (í˜„ì¬ ë¹„í™œì„±)
    available:
      - openai
      - anthropic
      - gemini

  media:
    # í˜„ì¬ í™œì„±í™”ëœ Provider
    active:
      - comfyui

    # ë¯¸ë˜ í™•ì¥ìš© (í˜„ì¬ ë¹„í™œì„±)
    available:
      - dalle
      - nanobanana

# ============================================
# LLM Router ê·œì¹™ (role/task â†’ provider/model)
# ============================================
llm_routing:
  # í˜„ì¬: ëª¨ë‘ Ollama
  rules:
    - role: [strategist, copywriter]
      provider: ollama
      model: qwen2.5:14b
      priority: 1

    - role: [brief, brand, editor, reviewer]
      provider: ollama
      model: qwen2.5:7b
      priority: 2

    - task: [heavy_reasoning]
      provider: ollama
      model: mistral-small
      priority: 3

    - task: [short_summary, tagging]
      provider: ollama
      model: llama3.2
      priority: 4

  # ë¯¸ë˜: Cloud Provider ì¶”ê°€ ì‹œ
  # future_rules:
  #   - role: [strategist]
  #     mode: final
  #     provider: anthropic
  #     model: claude-3-5-sonnet-20241022
  #
  #   - role: [copywriter]
  #     mode: final
  #     provider: openai
  #     model: gpt-4o

# ============================================
# Media Router ê·œì¹™ (kind â†’ provider)
# ============================================
media_routing:
  image:
    - kind: [product_shot, hero, concept]
      provider: comfyui
      priority: 1

    # ë¯¸ë˜ í™•ì¥
    # - kind: [thumbnail]
    #   mode: final
    #   provider: dalle
    #   priority: 2
```

**ì¤‘ìš”**:
- ì£¼ì„ìœ¼ë¡œ ë¯¸ë˜ í™•ì¥ ê·œì¹™ **ì˜ˆì‹œ í¬í•¨**
- Priority ê¸°ë°˜ í´ë°± ê°€ëŠ¥í•˜ê²Œ ì„¤ê³„

---

### ì‘ì—… 1.4: Provider ì¸í„°í˜ì´ìŠ¤ ì •ì˜

**íŒŒì¼**: `backend/app/services/llm/providers/base.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List
from pydantic import BaseModel

class LLMProviderResponse(BaseModel):
    """ëª¨ë“  LLM Providerì˜ ì‘ë‹µ í‘œì¤€ í¬ë§·"""
    provider: str
    model: str
    usage: Dict[str, int]  # {prompt_tokens, completion_tokens, total_tokens}
    output: Dict[str, Any]  # {type, content, parsed}
    meta: Dict[str, Any]  # {latency_ms, ...}

class LLMProvider(ABC):
    """
    LLM Provider ê³µí†µ ì¸í„°í˜ì´ìŠ¤

    â­ í™•ì¥ ì›ì¹™:
    1. ëª¨ë“  ProviderëŠ” ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼ í•¨
    2. ìƒˆ Provider ì¶”ê°€ ì‹œ ì´ íŒŒì¼ ìˆ˜ì • ê¸ˆì§€
    3. generate() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ëŠ” ë¶ˆë³€
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = self.__class__.__name__

    @property
    @abstractmethod
    def vendor(self) -> str:
        """
        Provider ë²¤ë”ëª…
        Returns: 'ollama' | 'openai' | 'anthropic' | 'google'
        """
        pass

    @property
    @abstractmethod
    def supports_json(self) -> bool:
        """JSON mode ì§€ì› ì—¬ë¶€"""
        pass

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        role: str,
        task: str,
        mode: str,
        options: Dict[str, Any]
    ) -> LLMProviderResponse:
        """
        LLM ìƒì„± ìš”ì²­

        Args:
            prompt: ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ (Prompt Builderì—ì„œ ìƒì„±)
            role: Agent ì—­í•  (strategist, copywriter, ...)
            task: ë¹„ì¦ˆë‹ˆìŠ¤ íƒœìŠ¤í¬ (product_detail, ...)
            mode: chat | json | tools
            options: {temperature, max_tokens, model, ...}

        Returns:
            LLMProviderResponse: í‘œì¤€ ì‘ë‹µ í¬ë§·
        """
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Provider ì—°ê²° ìƒíƒœ í™•ì¸"""
        pass
```

**ì¤‘ìš”**:
- `ABC` ì‚¬ìš©í•˜ì—¬ ì¸í„°í˜ì´ìŠ¤ ê°•ì œ
- ëª¨ë“  Providerê°€ **ë™ì¼í•œ ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜** ì‚¬ìš©
- ì£¼ì„ì— í™•ì¥ ì›ì¹™ ëª…ì‹œ

---

### ì‘ì—… 1.5: OllamaProvider êµ¬í˜„ (4ì‹œê°„)

**íŒŒì¼**: `backend/app/services/llm/providers/ollama.py`

```python
import httpx
import time
from typing import Dict, Any
from .base import LLMProvider, LLMProviderResponse

class OllamaProvider(LLMProvider):
    """
    Ollama Provider êµ¬í˜„

    ì—°ê²° ëŒ€ìƒ: Desktop Docker (http://100.120.180.42:11434)
    ëª¨ë¸: qwen2.5:7b/14b, mistral-small, llama3.2
    """

    @property
    def vendor(self) -> str:
        return "ollama"

    @property
    def supports_json(self) -> bool:
        return True  # OllamaëŠ” JSON mode ì§€ì›

    async def generate(
        self,
        prompt: str,
        role: str,
        task: str,
        mode: str,
        options: Dict[str, Any]
    ) -> LLMProviderResponse:
        """Ollama API í˜¸ì¶œ"""

        start_time = time.time()

        # Ollama ìš”ì²­ í¬ë§·
        model = options.get("model", self.config.get("default_model", "qwen2.5:7b"))

        request_data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": options.get("temperature", 0.7),
                "num_predict": options.get("max_tokens", 2048)
            }
        }

        # JSON mode ì²˜ë¦¬
        if mode == "json":
            request_data["format"] = "json"

        # Ollama API í˜¸ì¶œ
        base_url = self.config.get("base_url", "http://100.120.180.42:11434")
        timeout = self.config.get("timeout", 120)

        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                f"{base_url}/api/generate",
                json=request_data
            )
            response.raise_for_status()
            result = response.json()

        # ì‘ë‹µ ë³€í™˜
        latency_ms = int((time.time() - start_time) * 1000)

        output_content = result.get("response", "")
        parsed = None

        if mode == "json":
            try:
                import json
                parsed = json.loads(output_content)
            except:
                pass

        return LLMProviderResponse(
            provider="ollama",
            model=model,
            usage={
                "prompt_tokens": result.get("prompt_eval_count", 0),
                "completion_tokens": result.get("eval_count", 0),
                "total_tokens": result.get("prompt_eval_count", 0) + result.get("eval_count", 0)
            },
            output={
                "type": "json" if mode == "json" else "text",
                "content": output_content,
                "parsed": parsed
            },
            meta={
                "role": role,
                "task": task,
                "mode": mode,
                "latency_ms": latency_ms
            }
        )

    async def health_check(self) -> bool:
        """Ollama ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            base_url = self.config.get("base_url")
            async with httpx.AsyncClient(timeout=5) as client:
                response = await client.get(f"{base_url}/api/tags")
                return response.status_code == 200
        except:
            return False
```

**ì¤‘ìš”**:
- `LLMProvider` ì¸í„°í˜ì´ìŠ¤ **ì™„ë²½ êµ¬í˜„**
- ì‘ë‹µ í¬ë§·ì„ **í‘œì¤€ LLMProviderResponse**ë¡œ ë³€í™˜
- ë‹¤ë¥¸ Providerë„ ë™ì¼í•œ íŒ¨í„´ ë”°ë¦„

---

### ì‘ì—… 1.6: ë¯¸ë˜ Provider ìŠ¤ì¼ˆë ˆí†¤ (1ì‹œê°„)

**íŒŒì¼**: `backend/app/services/llm/providers/openai.py`

```python
from typing import Dict, Any
from .base import LLMProvider, LLMProviderResponse

class OpenAIProvider(LLMProvider):
    """
    OpenAI Provider (GPT-4, GPT-4o ë“±)

    â­ í˜„ì¬: ìŠ¤ì¼ˆë ˆí†¤ë§Œ (TODO)
    â­ ë¯¸ë˜: OpenAI SDK ì‚¬ìš©í•˜ì—¬ êµ¬í˜„

    í™•ì¥ ì‹œ í•´ì•¼ í•  ì¼:
    1. openai íŒ¨í‚¤ì§€ ì„¤ì¹˜ (pip install openai)
    2. generate() ë©”ì„œë“œ êµ¬í˜„
    3. provider_config.yamlì—ì„œ 'openai' í™œì„±í™”
    4. .envì—ì„œ OPENAI_API_KEY ì„¤ì •
    """

    @property
    def vendor(self) -> str:
        return "openai"

    @property
    def supports_json(self) -> bool:
        return True  # GPT-4oëŠ” JSON mode ì§€ì›

    async def generate(
        self,
        prompt: str,
        role: str,
        task: str,
        mode: str,
        options: Dict[str, Any]
    ) -> LLMProviderResponse:
        """
        TODO: OpenAI API í˜¸ì¶œ êµ¬í˜„

        êµ¬í˜„ ì˜ˆì‹œ:
        1. openai.ChatCompletion.create() ì‚¬ìš©
        2. ì‘ë‹µì„ LLMProviderResponse í¬ë§·ìœ¼ë¡œ ë³€í™˜
        3. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
        """
        raise NotImplementedError(
            "OpenAIProvider is not implemented yet. "
            "See provider_config.yaml to enable it later."
        )

    async def health_check(self) -> bool:
        # TODO: OpenAI API í‚¤ ìœ íš¨ì„± í™•ì¸
        return False
```

**ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ìƒì„±**:
- `anthropic.py` (AnthropicProvider)
- `gemini.py` (GeminiProvider)

**ì¤‘ìš”**:
- ì¸í„°í˜ì´ìŠ¤ëŠ” **ì™„ë²½íˆ êµ¬í˜„** (ì—ëŸ¬ ë°œìƒí•˜ì§€ë§Œ íƒ€ì…ì€ ë§ìŒ)
- TODO ì£¼ì„ìœ¼ë¡œ **ë‚˜ì¤‘ì— í•  ì¼ ëª…ì‹œ**
- `NotImplementedError`ë¡œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

---

### ì‘ì—… 1.7: LLM Gateway API ì—”ë“œí¬ì¸íŠ¸ (4ì‹œê°„)

**íŒŒì¼**: `backend/app/api/v1/endpoints/llm_gateway.py`

```python
from fastapi import APIRouter, HTTPException, Depends
from app.schemas.llm_gateway import LLMGatewayRequest, LLMGatewayResponse
from app.services.llm.gateway import LLMGatewayService
from app.core.config import settings

router = APIRouter(prefix="/llm", tags=["LLM Gateway"])

@router.post("/generate", response_model=LLMGatewayResponse)
async def generate(request: LLMGatewayRequest):
    """
    LLM ìƒì„± ìš”ì²­ (ë‹¨ì¼ ì§„ì…ì )

    â­ ì¤‘ìš”:
    - ëª¨ë“  Agent/EditorëŠ” ì´ ì—”ë“œí¬ì¸íŠ¸ë§Œ í˜¸ì¶œ
    - Provider ì„ íƒì€ ë‚´ë¶€ Routerì—ì„œ ìë™ ì²˜ë¦¬
    - Mock/Live ëª¨ë“œëŠ” GENERATOR_MODE í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´
    """

    gateway = LLMGatewayService()

    try:
        response = await gateway.generate(request)
        return response

    except Exception as e:
        # í‘œì¤€ ì—ëŸ¬ í¬ë§· ë°˜í™˜
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "code": "LLM_GATEWAY_ERROR",
                    "message": str(e),
                    "request_id": request.request_id
                }
            }
        )

@router.get("/health")
async def health():
    """
    Gateway í—¬ìŠ¤ ì²´í¬

    Returns:
        í™œì„±í™”ëœ Providerë³„ ìƒíƒœ
    """
    gateway = LLMGatewayService()
    return await gateway.health_check()
```

**íŒŒì¼**: `backend/app/services/llm/gateway.py`

```python
from app.schemas.llm_gateway import LLMGatewayRequest, LLMGatewayResponse
from app.services.llm.router import LLMRouter
from app.services.llm.prompt_builder import PromptBuilder
from app.core.config import settings
import time

class LLMGatewayService:
    """
    LLM Gateway ë©”ì¸ ë¡œì§

    ì±…ì„:
    1. Mock/Live ëª¨ë“œ ë¶„ê¸°
    2. Routerë¥¼ í†µí•œ Provider ì„ íƒ
    3. Prompt ì •ê·œí™”
    4. Provider í˜¸ì¶œ ë° ì‘ë‹µ ë³€í™˜
    """

    def __init__(self):
        self.router = LLMRouter()
        self.prompt_builder = PromptBuilder()

    async def generate(self, request: LLMGatewayRequest) -> LLMGatewayResponse:
        """LLM ìƒì„± ìš”ì²­ ì²˜ë¦¬"""

        # Mock ëª¨ë“œ
        if settings.GENERATOR_MODE == "mock":
            return self._mock_response(request)

        # Live ëª¨ë“œ
        start_time = time.time()

        # 1. Router: role/task â†’ Provider + Model ì„ íƒ
        provider = self.router.route(request)

        # 2. Prompt ì •ê·œí™”
        prompt = self.prompt_builder.build(request)

        # 3. Provider í˜¸ì¶œ
        provider_response = await provider.generate(
            prompt=prompt,
            role=request.role,
            task=request.task,
            mode=request.mode,
            options=request.options or {}
        )

        # 4. Gateway ì‘ë‹µìœ¼ë¡œ ë³€í™˜
        latency_ms = int((time.time() - start_time) * 1000)

        return LLMGatewayResponse(
            provider=provider_response.provider,
            model=provider_response.model,
            usage=provider_response.usage,
            output=provider_response.output,
            meta={
                **provider_response.meta,
                "latency_ms": latency_ms,
                "generator_mode": "live"
            }
        )

    def _mock_response(self, request: LLMGatewayRequest) -> LLMGatewayResponse:
        """Mock ëª¨ë“œ ì‘ë‹µ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)"""

        # role/taskì— ë”°ë¥¸ Mock ë°ì´í„°
        mock_content = self._get_mock_content(request.role, request.task)

        return LLMGatewayResponse(
            provider="mock",
            model="mock-model",
            usage={"prompt_tokens": 100, "completion_tokens": 200, "total_tokens": 300},
            output={
                "type": "json" if request.mode == "json" else "text",
                "content": mock_content,
                "parsed": {"status": "mock"} if request.mode == "json" else None
            },
            meta={
                "role": request.role,
                "task": request.task,
                "mode": request.mode,
                "latency_ms": 50,
                "generator_mode": "mock"
            }
        )

    def _get_mock_content(self, role: str, task: str) -> str:
        """role/taskë³„ Mock ì‘ë‹µ ë‚´ìš©"""
        # TODO: ì‹¤ì œì™€ ìœ ì‚¬í•œ Mock ë°ì´í„° ì¶”ê°€
        return f"Mock {role} response for {task}"

    async def health_check(self):
        """í™œì„±í™”ëœ Provider ìƒíƒœ í™•ì¸"""
        return await self.router.health_check_all()
```

**ì¤‘ìš”**:
- Mock/Live ë¶„ê¸°ëŠ” **Gateway ë ˆë²¨ì—ì„œë§Œ** ì²˜ë¦¬
- ProviderëŠ” Mock/Live êµ¬ë¶„ ì—†ì´ **í•­ìƒ ì‹¤ì œ êµ¬í˜„ë§Œ**
- í™•ì¥ ì‹œ ì´ íŒŒì¼ ìˆ˜ì • ë¶ˆí•„ìš”

---

### ì‘ì—… 1.8: LLM Router êµ¬í˜„ (2ì‹œê°„)

**íŒŒì¼**: `backend/app/services/llm/router.py`

```python
import yaml
from pathlib import Path
from typing import Dict, Any
from app.services.llm.providers.base import LLMProvider
from app.services.llm.providers.ollama import OllamaProvider
# ë¯¸ë˜ í™•ì¥
# from app.services.llm.providers.openai import OpenAIProvider
# from app.services.llm.providers.anthropic import AnthropicProvider
# from app.services.llm.providers.gemini import GeminiProvider
from app.core.config import settings

class LLMRouter:
    """
    LLM Provider ë¼ìš°íŒ…

    â­ í™•ì¥ ì›ì¹™:
    1. provider_config.yaml ê¸°ë°˜ ë¼ìš°íŒ…
    2. ìƒˆ Provider ì¶”ê°€ ì‹œ _initialize_providers()ë§Œ ìˆ˜ì •
    3. route() ë©”ì„œë“œëŠ” ìˆ˜ì • ë¶ˆí•„ìš” (YAML ê·œì¹™ ê¸°ë°˜)
    """

    def __init__(self):
        self.config = self._load_config()
        self.providers = self._initialize_providers()

    def _load_config(self) -> Dict[str, Any]:
        """provider_config.yaml ë¡œë“œ"""
        config_path = Path(__file__).parent.parent.parent / "core" / "provider_config.yaml"
        with open(config_path) as f:
            return yaml.safe_load(f)

    def _initialize_providers(self) -> Dict[str, LLMProvider]:
        """
        í™œì„±í™”ëœ Provider ì´ˆê¸°í™”

        â­ ìƒˆ Provider ì¶”ê°€ ì‹œ ì—¬ê¸°ì— ë“±ë¡
        """
        providers = {}

        active_providers = self.config["providers"]["llm"]["active"]

        if "ollama" in active_providers:
            providers["ollama"] = OllamaProvider({
                "base_url": settings.OLLAMA_BASE_URL,
                "default_model": settings.OLLAMA_DEFAULT_MODEL,
                "timeout": settings.OLLAMA_TIMEOUT
            })

        # ë¯¸ë˜ í™•ì¥ (ì£¼ì„ í•´ì œë§Œ í•˜ë©´ ë¨)
        # if "openai" in active_providers:
        #     providers["openai"] = OpenAIProvider({
        #         "api_key": settings.OPENAI_API_KEY,
        #         "default_model": settings.OPENAI_DEFAULT_MODEL,
        #         "timeout": settings.OPENAI_TIMEOUT
        #     })

        # if "anthropic" in active_providers:
        #     providers["anthropic"] = AnthropicProvider({...})

        # if "gemini" in active_providers:
        #     providers["gemini"] = GeminiProvider({...})

        return providers

    def route(self, request) -> LLMProvider:
        """
        role/task â†’ Provider ì„ íƒ

        Args:
            request: LLMGatewayRequest

        Returns:
            LLMProvider: ì„ íƒëœ Provider ì¸ìŠ¤í„´ìŠ¤
        """
        routing_rules = self.config["llm_routing"]["rules"]

        # ëª…ì‹œì  provider ì§€ì •
        if request.options and request.options.get("provider"):
            provider_name = request.options["provider"]
            if provider_name in self.providers:
                return self.providers[provider_name]

        # YAML ê·œì¹™ ê¸°ë°˜ ìë™ ì„ íƒ
        for rule in routing_rules:
            # role ë§¤ì¹­
            if "role" in rule and request.role in rule["role"]:
                provider_name = rule["provider"]
                provider = self.providers.get(provider_name)
                if provider:
                    return provider

            # task ë§¤ì¹­
            if "task" in rule and request.task in rule["task"]:
                provider_name = rule["provider"]
                provider = self.providers.get(provider_name)
                if provider:
                    return provider

        # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ í™œì„± Provider
        default_provider = list(self.providers.values())[0]
        return default_provider

    async def health_check_all(self) -> Dict[str, bool]:
        """ëª¨ë“  Provider í—¬ìŠ¤ ì²´í¬"""
        results = {}
        for name, provider in self.providers.items():
            results[name] = await provider.health_check()
        return results
```

**ì¤‘ìš”**:
- ë¼ìš°íŒ… ë¡œì§ì€ **YAML ì„¤ì • ê¸°ë°˜**
- ìƒˆ ProviderëŠ” `_initialize_providers()`ì— **3ì¤„ ì¶”ê°€**ë§Œ í•˜ë©´ ë¨
- `route()` ë©”ì„œë“œëŠ” **ìˆ˜ì • ë¶ˆí•„ìš”**

---

### ì‘ì—… 1.9: Media Gateway êµ¬í˜„ (3-4ì‹œê°„)

**ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„**:
- `media/providers/base.py` (ImageProvider ì¸í„°í˜ì´ìŠ¤)
- `media/providers/comfyui.py` (ComfyUIProvider êµ¬í˜„)
- `media/providers/dalle.py` (DalleProvider ìŠ¤ì¼ˆë ˆí†¤)
- `media/providers/nanobanana.py` (NanobananaProvider ìŠ¤ì¼ˆë ˆí†¤)
- `media/gateway.py` (MediaGatewayService)
- `api/v1/endpoints/media_gateway.py` (API ì—”ë“œí¬ì¸íŠ¸)

**ì¤‘ìš” ì°¨ì´ì **:
- ImageëŠ” **ë™ê¸° ê°€ëŠ¥**, í•˜ì§€ë§Œ **Job ì²˜ë¦¬ ì˜µì…˜** ì œê³µ
- `kind` ê¸°ë°˜ ë¼ìš°íŒ… (`product_shot`, `hero`, `concept` ë“±)

---

### ì‘ì—… 1.10: Mock ì‘ë‹µ êµ¬í˜„ (2ì‹œê°„)

**íŒŒì¼**: `backend/app/services/llm/mock_data.py`

```python
"""
Mock ëª¨ë“œ ì‘ë‹µ ë°ì´í„°
â­ ì‹¤ì œ ì‘ë‹µê³¼ ë™ì¼í•œ êµ¬ì¡° ìœ ì§€
"""

MOCK_LLM_RESPONSES = {
    ("brief", "marketing_brief"): {
        "type": "json",
        "content": '{"target":"20-30ëŒ€ ì§ì¥ì¸","positioning":"ê°„í¸ ê³ ë‹¨ë°± ì˜ì–‘ì‹","key_messages":["1íšŒë¶„ 30g ë‹¨ë°±ì§ˆ","5ë¶„ ì™„ì„±","ë§›ìˆëŠ” ì´ˆì½”ë§›"]}',
        "parsed": {
            "target": "20-30ëŒ€ ì§ì¥ì¸",
            "positioning": "ê°„í¸ ê³ ë‹¨ë°± ì˜ì–‘ì‹",
            "key_messages": ["1íšŒë¶„ 30g ë‹¨ë°±ì§ˆ", "5ë¶„ ì™„ì„±", "ë§›ìˆëŠ” ì´ˆì½”ë§›"]
        }
    },

    ("strategist", "content_plan"): {
        "type": "json",
        "content": '{"sections":[{"id":"hero","type":"hero","title":"ë©”ì¸ ë¹„ì£¼ì–¼"},{"id":"features","type":"features","title":"ì œí’ˆ íŠ¹ì§•"}]}',
        "parsed": {
            "sections": [
                {"id": "hero", "type": "hero", "title": "ë©”ì¸ ë¹„ì£¼ì–¼"},
                {"id": "features", "type": "features", "title": "ì œí’ˆ íŠ¹ì§•"}
            ]
        }
    },

    # ë‚˜ë¨¸ì§€ role/task ì¡°í•© ì¶”ê°€...
}

MOCK_IMAGE_RESPONSES = {
    "product_shot": {
        "id": "img_mock_001",
        "url": "https://via.placeholder.com/1280x720/FF6600/FFFFFF?text=Product+Shot+Mock",
        "meta": {"workflow": "mock", "seed": 12345}
    },

    "hero": {
        "id": "img_mock_002",
        "url": "https://via.placeholder.com/1920x1080/0066FF/FFFFFF?text=Hero+Image+Mock",
        "meta": {"workflow": "mock", "seed": 12346}
    }
}
```

**ì¤‘ìš”**:
- Mock ë°ì´í„°ëŠ” **ì‹¤ì œ ì‘ë‹µê³¼ êµ¬ì¡° ë™ì¼**
- Liveë¡œ ì „í™˜ ì‹œ **í…ŒìŠ¤íŠ¸ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”**

---

## ğŸ—ï¸ Phase 2: Agent ë¦¬íŒ©í„°ë§ (10ì‹œê°„)

### ì‘ì—… 2.1: Gateway Client êµ¬í˜„ (2-3ì‹œê°„)

**íŒŒì¼**: `backend/app/services/clients/llm_client.py`

```python
import httpx
from typing import Dict, Any, Optional

class LLMGatewayClient:
    """
    LLM Gateway í´ë¼ì´ì–¸íŠ¸

    â­ Agentì—ì„œ ì‚¬ìš©í•˜ëŠ” ìœ ì¼í•œ LLM ì¸í„°í˜ì´ìŠ¤
    â­ Ollama/GPT/Claude ëª¨ë‘ ì´ í´ë¼ì´ì–¸íŠ¸ë¡œ ì ‘ê·¼
    """

    def __init__(self, base_url: str = None):
        self.base_url = base_url or "http://localhost:8000/api/v1"

    async def generate(
        self,
        role: str,
        task: str,
        payload: Dict[str, Any],
        mode: str = "chat",
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        LLM ìƒì„± ìš”ì²­

        Args:
            role: Agent ì—­í•  (brief, strategist, copywriter, ...)
            task: ë¹„ì¦ˆë‹ˆìŠ¤ íƒœìŠ¤í¬ (marketing_brief, content_plan, ...)
            payload: ì…ë ¥ ë°ì´í„° (brand, context, payload)
            mode: chat | json
            options: {temperature, max_tokens, provider, ...}

        Returns:
            Gateway ì‘ë‹µ JSON
        """

        request_data = {
            "role": role,
            "task": task,
            "mode": mode,
            "input": payload,
            "options": options or {}
        }

        async with httpx.AsyncClient(timeout=120) as client:
            response = await client.post(
                f"{self.base_url}/llm/generate",
                json=request_data
            )
            response.raise_for_status()
            return response.json()
```

**íŒŒì¼**: `backend/app/services/clients/media_client.py`

```python
import httpx
from typing import Dict, Any, Optional, List

class MediaGatewayClient:
    """Media Gateway í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str = None):
        self.base_url = base_url or "http://localhost:8000/api/v1"

    async def generate_image(
        self,
        kind: str,
        prompt: str,
        brand: Optional[Dict[str, Any]] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ìƒì„± ìš”ì²­

        Args:
            kind: product_shot | hero | concept | thumbnail
            prompt: ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸
            brand: ë¸Œëœë“œ ì •ë³´
            options: {aspect_ratio, width, height, workflow, ...}

        Returns:
            Gateway ì‘ë‹µ JSON
        """

        request_data = {
            "provider": "auto",  # Gatewayê°€ ìë™ ì„ íƒ
            "kind": kind,
            "prompt": prompt,
            "brand": brand or {},
            "options": options or {}
        }

        async with httpx.AsyncClient(timeout=300) as client:
            response = await client.post(
                f"{self.base_url}/media/image/generate",
                json=request_data
            )
            response.raise_for_status()
            return response.json()
```

---

### ì‘ì—… 2.2~2.8: Agent ë¦¬íŒ©í„°ë§ (ê° 1-1.5ì‹œê°„)

**ì˜ˆì‹œ**: BriefAgent ìˆ˜ì •

**ìˆ˜ì • ì „**:
```python
# âŒ ì§ì ‘ Ollama í˜¸ì¶œ
import ollama

class BriefAgent:
    async def execute(self, brand_info, product_info):
        prompt = f"Generate brief for {product_info}"
        response = ollama.generate(
            model="qwen2.5:7b",
            prompt=prompt
        )
        return response["response"]
```

**ìˆ˜ì • í›„**:
```python
# âœ… Gateway Client ì‚¬ìš©
from app.services.clients.llm_client import LLMGatewayClient

class BriefAgent:
    def __init__(self):
        self.llm_client = LLMGatewayClient()

    async def execute(self, brand_info, product_info):
        response = await self.llm_client.generate(
            role="brief",
            task="marketing_brief",
            mode="json",
            payload={
                "brand": brand_info,
                "product": product_info
            }
        )

        # ì‘ë‹µ íŒŒì‹±
        return response["output"]["parsed"]
```

**í•µì‹¬ ë³€ê²½ì‚¬í•­**:
1. âŒ `import ollama` ì œê±°
2. âœ… `LLMGatewayClient` ì‚¬ìš©
3. âœ… `role`, `task` ëª…ì‹œ
4. âœ… Provider/ëª¨ë¸ ì„ íƒì€ Gatewayì— ìœ„ì„

**ë™ì¼í•˜ê²Œ ìˆ˜ì •í•  Agent**:
- BrandAgent (role="brand", task="brand_summary")
- StrategistAgent (role="strategist", task="content_plan")
- CopywriterAgent (role="copywriter", task="product_detail")
- ReviewerAgent (role="reviewer", task="style_check")

---

### ì‘ì—… 2.7: VisionGeneratorAgent íŠ¹ë³„ ì²˜ë¦¬ (1.5ì‹œê°„)

**íŒŒì¼**: `backend/app/agents/vision_generator.py`

```python
from app.services.clients.llm_client import LLMGatewayClient
from app.services.clients.media_client import MediaGatewayClient

class VisionGeneratorAgent:
    """
    Vision Generator Agent

    â­ LLM + Media ë‘ Gateway ì‚¬ìš©
    1. LLM Gateway: ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    2. Media Gateway: ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±
    """

    def __init__(self):
        self.llm_client = LLMGatewayClient()
        self.media_client = MediaGatewayClient()

    async def execute(
        self,
        brief: Dict[str, Any],
        section: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ìƒì„±

        Args:
            brief: ë§ˆì¼€íŒ… ë¸Œë¦¬í”„
            section: ì„¹ì…˜ ì •ë³´ (hero, features, ...)

        Returns:
            {image_id, url, prompt_used}
        """

        # 1ë‹¨ê³„: LLMìœ¼ë¡œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_response = await self.llm_client.generate(
            role="vision",
            task="image_prompt",
            mode="json",
            payload={
                "brief": brief,
                "section": section
            }
        )

        image_prompt_data = prompt_response["output"]["parsed"]

        # 2ë‹¨ê³„: Media Gatewayë¡œ ì´ë¯¸ì§€ ìƒì„±
        image_response = await self.media_client.generate_image(
            kind="product_shot",  # or section["type"]
            prompt=image_prompt_data["prompt"],
            brand=brief.get("brand"),
            options={
                "aspect_ratio": "16:9",
                "workflow": "product_shot_v1",
                "negative_prompt": image_prompt_data.get("negative_prompt")
            }
        )

        # ê²°ê³¼ ë°˜í™˜
        return {
            "image_id": image_response["images"][0]["id"],
            "url": image_response["images"][0]["url"],
            "prompt_used": image_prompt_data["prompt"]
        }
```

---

## ğŸ—ï¸ Phase 3: P0 E2E ìŠ¤í¬ë¦½íŠ¸ (6ì‹œê°„)

**íŒŒì¼**: `backend/scripts/run_p0_product_detail_flow.py`

```python
#!/usr/bin/env python3
"""
P0 E2E: ìƒí’ˆ ìƒì„¸ + ì´ë¯¸ì§€ 1ì¥ ìƒì„± í”Œë¡œìš°

ì‹¤í–‰ ë°©ë²•:
  Mock ëª¨ë“œ: GENERATOR_MODE=mock python scripts/run_p0_product_detail_flow.py
  Live ëª¨ë“œ: GENERATOR_MODE=live python scripts/run_p0_product_detail_flow.py
"""

import asyncio
import json
import sys
import os
from pathlib import Path

# Backend ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.agents.brand import BrandAgent
from app.agents.brief import BriefAgent
from app.agents.strategist import StrategistAgent
from app.agents.copywriter import CopywriterAgent
from app.agents.vision_generator import VisionGeneratorAgent
from app.agents.reviewer import ReviewerAgent
from app.core.config import settings

async def run_product_detail_flow():
    """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ìƒì„± E2E í”Œë¡œìš°"""

    print(f"ğŸš€ Starting P0 E2E Flow (Mode: {settings.GENERATOR_MODE})")
    print("=" * 60)

    # Input
    brand_input = {
        "name": "í”„ë¡œí‹´í”ŒëŸ¬ìŠ¤",
        "industry": "ê±´ê°•ì‹í’ˆ",
        "uploaded_materials": []
    }

    product_input = {
        "name": "í”„ë¡œí‹´í”ŒëŸ¬ìŠ¤ ì´ˆì½”ë§›",
        "category": "ë‹¨ë°±ì§ˆ ë³´ì¶©ì œ",
        "target": "20-30ëŒ€ ì§ì¥ì¸",
        "key_features": ["ê³ ë‹¨ë°± 30g", "ì €ë‹¹", "ê°„í¸ ì‰ì´í¬"]
    }

    try:
        # 1. BrandAgent
        print("\nğŸ“‹ Step 1: BrandAgent - ë¸Œëœë“œ ìš”ì•½")
        brand_agent = BrandAgent()
        brand_summary = await brand_agent.execute(brand_input)
        print(f"âœ… Brand Summary: {json.dumps(brand_summary, ensure_ascii=False, indent=2)[:200]}...")

        # 2. BriefAgent
        print("\nğŸ“‹ Step 2: BriefAgent - ë§ˆì¼€íŒ… ë¸Œë¦¬í”„")
        brief_agent = BriefAgent()
        brief = await brief_agent.execute({
            "brand": brand_summary,
            "product": product_input
        })
        print(f"âœ… Brief: {json.dumps(brief, ensure_ascii=False, indent=2)[:200]}...")

        # 3. StrategistAgent
        print("\nğŸ“‹ Step 3: StrategistAgent - ì„¹ì…˜ êµ¬ì¡°")
        strategist = StrategistAgent()
        sections = await strategist.execute(brief)
        print(f"âœ… Sections: {len(sections.get('sections', []))}ê°œ")

        # 4. CopywriterAgent
        print("\nğŸ“‹ Step 4: CopywriterAgent - ì¹´í”¼ ì‘ì„±")
        copywriter = CopywriterAgent()
        copy = await copywriter.execute({
            "brief": brief,
            "sections": sections
        })
        print(f"âœ… Copy: {json.dumps(copy, ensure_ascii=False, indent=2)[:200]}...")

        # 5. VisionGeneratorAgent
        print("\nğŸ“‹ Step 5: VisionGeneratorAgent - ë©”ì¸ ì´ë¯¸ì§€")
        vision = VisionGeneratorAgent()
        image = await vision.execute({
            "brief": brief,
            "section": sections["sections"][0]  # Hero
        })
        print(f"âœ… Image: {image['image_id']} - {image['url']}")

        # 6. ReviewerAgent
        print("\nğŸ“‹ Step 6: ReviewerAgent - ì¹´í”¼ ë¦¬ë·°")
        reviewer = ReviewerAgent()
        review = await reviewer.execute({
            "brand": brand_summary,
            "copy": copy
        })
        print(f"âœ… Review: {json.dumps(review, ensure_ascii=False, indent=2)[:200]}...")

        # Final Output
        result = {
            "brand_summary": brand_summary,
            "brief": brief,
            "sections": sections,
            "copy": copy,
            "image": image,
            "review": review
        }

        # ê²°ê³¼ ì €ì¥
        output_path = Path(__file__).parent.parent / "test_results" / "p0_e2e_result.json"
        output_path.parent.mkdir(exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print("\n" + "=" * 60)
        print(f"âœ… P0 E2E Flow Completed Successfully!")
        print(f"ğŸ“„ Result saved: {output_path}")
        print("=" * 60)

        return result

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(run_product_detail_flow())
```

**ì‹¤í–‰ ì˜ˆì‹œ**:
```bash
# Mock ëª¨ë“œ (ë¹ ë¦„, 30ì´ˆ)
GENERATOR_MODE=mock python backend/scripts/run_p0_product_detail_flow.py

# Live ëª¨ë“œ (ëŠë¦¼, 2-3ë¶„)
GENERATOR_MODE=live python backend/scripts/run_p0_product_detail_flow.py
```

---

## ğŸ—ï¸ Phase 4: í…ŒìŠ¤íŠ¸ ì§€ì› (2ì‹œê°„)

### ì‘ì—… 4.1: Mock ì‘ë‹µ í’ˆì§ˆ ê°œì„  (1.5ì‹œê°„)

- Mock ë°ì´í„°ë¥¼ **ì‹¤ì œ ì‘ë‹µê³¼ ë” ìœ ì‚¬í•˜ê²Œ** ê°œì„ 
- role/task ì¡°í•©ë³„ **ë‹¤ì–‘í•œ Mock ìƒ˜í”Œ** ì¶”ê°€

### ì‘ì—… 4.2: íƒ€ì„ì•„ì›ƒ ìµœì í™” (0.5ì‹œê°„)

```python
# backend/app/core/config.py

class Settings(BaseSettings):
    # Timeout ì„¤ì •
    GATEWAY_TIMEOUT_MOCK: int = 5  # Mock ëª¨ë“œëŠ” 5ì´ˆë©´ ì¶©ë¶„
    GATEWAY_TIMEOUT_LIVE: int = 180  # Live ëª¨ë“œëŠ” 3ë¶„

    def get_timeout(self, mode: str = None) -> int:
        """Modeì— ë”°ë¥¸ íƒ€ì„ì•„ì›ƒ ë°˜í™˜"""
        if mode == "mock" or self.GENERATOR_MODE == "mock":
            return self.GATEWAY_TIMEOUT_MOCK
        return self.GATEWAY_TIMEOUT_LIVE
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€ (Definition of Done)

### Phase 1 ì™„ë£Œ ì¡°ê±´
- [ ] `/api/v1/llm/generate` Mock ëª¨ë“œ ì •ìƒ ë™ì‘
- [ ] `/api/v1/llm/generate` Live ëª¨ë“œë¡œ Ollama ì—°ê²° ì„±ê³µ
- [ ] Postmanìœ¼ë¡œ 4ê°€ì§€ role (brief, strategist, copywriter, vision) í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] `/api/v1/media/image/generate` ComfyUI í˜¸ì¶œ ì„±ê³µ
- [ ] `provider_config.yaml`ì— ë¯¸ë˜ Provider ì£¼ì„ í¬í•¨
- [ ] OpenAI/Anthropic/Gemini Provider ìŠ¤ì¼ˆë ˆí†¤ íŒŒì¼ ì¡´ì¬

### Phase 2 ì™„ë£Œ ì¡°ê±´
- [ ] 6ê°œ Agent ëª¨ë‘ Gateway Client ì‚¬ìš©
- [ ] Agent íŒŒì¼ì—ì„œ `import ollama` ì™„ì „ ì œê±°
- [ ] VisionGeneratorAgentê°€ LLM + Media Gateway ì‚¬ìš©
- [ ] ê° Agent Mock ëª¨ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ê° Agent Live ëª¨ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼ (Ollama ì‹¤ì œ í˜¸ì¶œ)

### Phase 3 ì™„ë£Œ ì¡°ê±´
- [ ] `run_p0_product_detail_flow.py` Mock ëª¨ë“œ 30ì´ˆ ì´ë‚´ ì™„ë£Œ
- [ ] `run_p0_product_detail_flow.py` Live ëª¨ë“œ 3ë¶„ ì´ë‚´ ì™„ë£Œ
- [ ] ìµœì¢… JSON íŒŒì¼ ìƒì„± (6ê°œ Agent ê²°ê³¼ í¬í•¨)
- [ ] ì´ë¯¸ì§€ URLì´ ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥ (ComfyUI ìƒì„±)
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ë° ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤

### Phase 4 ì™„ë£Œ ì¡°ê±´
- [ ] Mock ì‘ë‹µ ë°ì´í„°ê°€ Live ì‘ë‹µê³¼ êµ¬ì¡° ë™ì¼
- [ ] role/task ì¡°í•©ë³„ Mock ë°ì´í„° 10ê°œ ì´ìƒ
- [ ] íƒ€ì„ì•„ì›ƒ ì„¤ì •ì´ Modeë³„ë¡œ ìë™ ì ìš©

---

## ğŸ“š í•„ìˆ˜ ì½ê¸° ë¬¸ì„œ

ì‘ì—… ì‹œì‘ ì „ ë°˜ë“œì‹œ ì½ì–´ì•¼ í•  ë¬¸ì„œ:

1. **LLM_CONNECTION_ANALYSIS_REPORT.md** (ì¢…í•© ë¶„ì„)
   - ìœ„ì¹˜: `docs/reports/LLM_CONNECTION_ANALYSIS_REPORT.md`
   - ì™œ 006ë²ˆ ë°©ì‹ì¸ì§€, í™•ì¥ ì „ëµì€ ë¬´ì—‡ì¸ì§€

2. **002. LLM Gateway Spec v1.0.md** (ìƒì„¸ ìŠ¤í™)
   - ìœ„ì¹˜: `K:\obsidian-k\Sparklio_ai_marketing_studio\ìµœì¢…ê³„íš\LLM\002. LLM Gateway Spec v1.0.md`
   - Provider ì¸í„°í˜ì´ìŠ¤, Router ì„¤ê³„

3. **003. Media Gateway Spec v1.0.md** (ìƒì„¸ ìŠ¤í™)
   - ìœ„ì¹˜: `K:\obsidian-k\Sparklio_ai_marketing_studio\ìµœì¢…ê³„íš\LLM\003. Media Gateway Spec v1.0.md`
   - Image/Video/Audio Provider ì„¤ê³„

4. **006. 005ì˜ ì¶•ì†Œë²„ì ¼.md** (í˜„ì‹¤ì  ì‹¤í–‰ ê°€ì´ë“œ)
   - ìœ„ì¹˜: `K:\obsidian-k\Sparklio_ai_marketing_studio\ìµœì¢…ê³„íš\LLM\006. 005ì˜ ì¶•ì†Œë²„ì ¼.md`
   - Mock/Live ë¶„ë¦¬, í…ŒìŠ¤íŠ¸ ì „ëµ

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­ (Critical Points)

### â­ í™•ì¥ì„± ê´€ë ¨ (ê°€ì¥ ì¤‘ìš”!)

1. **Provider ì¸í„°í˜ì´ìŠ¤ëŠ” ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€**
   - ìƒˆ ProviderëŠ” **ìƒˆ íŒŒì¼ë§Œ ì¶”ê°€**
   - `base.py` ìˆ˜ì • ê¸ˆì§€

2. **API Contract ë¶ˆë³€ì„± ìœ ì§€**
   - `/api/v1/llm/generate` ìš”ì²­/ì‘ë‹µ ìŠ¤í™ **ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€**
   - AgentëŠ” Gatewayë§Œ ì˜ì¡´, Provider ëª°ë¼ì•¼ í•¨

3. **ì„¤ì • ê¸°ë°˜ í™•ì¥**
   - í•˜ë“œì½”ë”© ê¸ˆì§€ (ëª¨ë¸ëª…, Providerëª…, ë¼ìš°íŒ… ê·œì¹™)
   - ëª¨ë‘ `provider_config.yaml` ë˜ëŠ” `.env`ì—

4. **ë¯¸ë˜ Provider ìŠ¤ì¼ˆë ˆí†¤ ë°˜ë“œì‹œ í¬í•¨**
   - OpenAI, Anthropic, Gemini Provider í´ë˜ìŠ¤ ìƒì„±
   - `NotImplementedError` + TODO ì£¼ì„
   - ë‚˜ì¤‘ì— **ì½”ë“œ ì¶”ê°€ë§Œ** í•˜ë©´ ë™ì‘í•˜ë„ë¡

### âš ï¸ Mock/Live ë¶„ë¦¬

1. **Mock ëª¨ë“œëŠ” Gateway ë ˆë²¨ì—ì„œë§Œ**
   - ProviderëŠ” í•­ìƒ ì‹¤ì œ êµ¬í˜„
   - `if GENERATOR_MODE == "mock"`ì€ Gatewayì—ë§Œ

2. **Mock ì‘ë‹µ = Live ì‘ë‹µ êµ¬ì¡°**
   - í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ ëª¨ë“œ ì „í™˜ ì‹œ **ìˆ˜ì • ë¶ˆí•„ìš”**
   - í•„ë“œëª…, íƒ€ì… ì™„ì „ ë™ì¼

### âŒ ì ˆëŒ€ ê¸ˆì§€

1. âŒ Agentì—ì„œ `import ollama`, `import openai` ì§ì ‘ ì‚¬ìš©
2. âŒ Gatewayë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ëª¨ë¸ ì§ì ‘ í˜¸ì¶œ
3. âŒ ëª¨ë¸ëª… í•˜ë“œì½”ë”© (`qwen2.5:7b` ê°™ì€ ë¬¸ìì—´ ì§ì ‘ ì‚¬ìš©)
4. âŒ Providerë³„ ë¡œì§ì„ Gateway APIì— ë…¸ì¶œ
5. âŒ í™•ì¥ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ if/else ë¶„ê¸°

---

## ğŸ†˜ ë¬¸ì œ ë°œìƒ ì‹œ

### Ollama ì—°ê²° ì‹¤íŒ¨
```
Error: Connection refused to http://100.120.180.42:11434
```
**í•´ê²°**: AíŒ€ì— Desktop Docker Ollama ìƒíƒœ í™•ì¸ ìš”ì²­

### ComfyUI ì›Œí¬í”Œë¡œ ì˜¤ë¥˜
```
Error: Workflow 'product_shot_v1' not found
```
**í•´ê²°**: AíŒ€ì— ComfyUI ì›Œí¬í”Œë¡œ íŒŒì¼ í™•ì¸ ìš”ì²­

### Gateway íƒ€ì„ì•„ì›ƒ
```
Error: Request timeout after 120s
```
**í•´ê²°**: Live ëª¨ë“œ íƒ€ì„ì•„ì›ƒ 180ì´ˆë¡œ ì¦ê°€, ë˜ëŠ” Mock ëª¨ë“œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸

---

## ğŸ“Š AíŒ€ ì „ë‹¬ ì‚¬í•­

### Phase 1 ì™„ë£Œ ì‹œ
- [ ] Gateway API ì—”ë“œí¬ì¸íŠ¸ URL
- [ ] Postman Collection íŒŒì¼
- [ ] Mock/Live ëª¨ë“œ í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

### Phase 2 ì™„ë£Œ ì‹œ
- [ ] Agentë³„ ì…ë ¥/ì¶œë ¥ JSON ìƒ˜í”Œ
- [ ] Gateway Client ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ

### Phase 3 ì™„ë£Œ ì‹œ
- [ ] E2E ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë°©ë²•
- [ ] ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (Mock: 30ì´ˆ, Live: 3ë¶„)

---

**ì‘ì„± ì™„ë£Œ**: 2025-11-16
**ì˜ˆìƒ ì™„ë£Œì¼**: 2025-11-21 (5ì¼ í›„)
**ë‹´ë‹¹**: BíŒ€ Backend ê°œë°œì
**ê²€ì¦**: AíŒ€ QA
