# BíŒ€ ìµì¼ ì‘ì—… ê³„íš (2025-11-17)

**ì‘ì„±ì¼**: 2025-11-16
**ì‘ì„±ì**: BíŒ€ (Backend)
**ì „ì¼ ì‘ì—…**: Phase 1-3 ì™„ë£Œ (ee19f82)

---

## ğŸ“‹ ì‘ì—… ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸ (ìµœìš°ì„ )

**Windows í™˜ê²½ë³€ìˆ˜ ì •ë¦¬ í•„ìˆ˜**:

```powershell
# PowerShell ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰

# 1. í˜„ì¬ ì„¤ì • í™•ì¸
[Environment]::GetEnvironmentVariable("OLLAMA_BASE_URL", "User")
[Environment]::GetEnvironmentVariable("OLLAMA_BASE_URL", "Machine")
[Environment]::GetEnvironmentVariable("COMFYUI_BASE_URL", "User")
[Environment]::GetEnvironmentVariable("COMFYUI_BASE_URL", "Machine")

# 2. ì‚­ì œ (êµ¬ IPê°€ ì„¤ì •ë˜ì–´ ìˆëŠ” ê²½ìš°)
[Environment]::SetEnvironmentVariable("OLLAMA_BASE_URL", $null, "User")
[Environment]::SetEnvironmentVariable("OLLAMA_BASE_URL", $null, "Machine")
[Environment]::SetEnvironmentVariable("COMFYUI_BASE_URL", $null, "User")
[Environment]::SetEnvironmentVariable("COMFYUI_BASE_URL", $null, "Machine")

# 3. í™•ì¸ (null ë˜ëŠ” ë¹ˆ ê°’ì´ì–´ì•¼ í•¨)
[Environment]::GetEnvironmentVariable("OLLAMA_BASE_URL", "User")
[Environment]::GetEnvironmentVariable("OLLAMA_BASE_URL", "Machine")
```

**ì¬ì‹œì‘**:
- âœ… ëª¨ë“  í„°ë¯¸ë„ ì¢…ë£Œ
- âœ… VSCode ì™„ì „ ì¢…ë£Œ
- âœ… (ê¶Œì¥) PC ì¬ë¶€íŒ… ë˜ëŠ” ë¡œê·¸ì•„ì›ƒ/ë¡œê·¸ì¸

### ğŸ“š í•„ìˆ˜ ì½ê¸° ë¬¸ì„œ

ì‘ì—… ì‹œì‘ ì „ ë°˜ë“œì‹œ ì½ì–´ì•¼ í•  ë¬¸ì„œë“¤:

1. **[Phase 1-3 ì™„ë£Œë³´ê³ ](./BíŒ€_Phase1-3_ì™„ë£Œë³´ê³ _2025-11-16.md)** â­
   - ì–´ì œ ì‘ì—… ë‚´ìš© ì „ì²´ ìš”ì•½
   - ë°œê²¬ëœ í™˜ê²½ë³€ìˆ˜ ì´ìŠˆ ìƒì„¸ ì„¤ëª…
   - Live ëª¨ë“œ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

2. **[ARCH-002: LLM Gateway Architecture](../architecture/ARCH-002-LLM_Gateway_Architecture.md)**
   - Gateway ì „ì²´ êµ¬ì¡°
   - Provider íŒ¨í„´
   - Router ì—­í• 

3. **[SPEC-001: API Specification](../architecture/SPEC-001-LLM_Gateway_API_Specification.md)**
   - API ì—”ë“œí¬ì¸íŠ¸ ëª…ì„¸
   - ìš”ì²­/ì‘ë‹µ í¬ë§·
   - ì—ëŸ¬ ì²˜ë¦¬ ê·œê²©

---

## ğŸ¯ Phase 1-4: Live ëª¨ë“œ ìµœì¢… ê²€ì¦ ë° ë§ˆë¬´ë¦¬

### Task 1: í™˜ê²½ ê²€ì¦ (30ë¶„)

**ëª©í‘œ**: í™˜ê²½ë³€ìˆ˜ ì •ë¦¬ í›„ ì˜¬ë°”ë¥¸ IPë¡œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸

```bash
# ì„œë²„ ì‹œì‘
cd /k/sparklio_ai_marketing_studio/backend
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

**ì²´í¬í¬ì¸íŠ¸**:

```bash
# 1. ì„¤ì • í™•ì¸
curl http://localhost:8000/api/v1/debug/settings | python -m json.tool

# âœ… ì˜ˆìƒ ê²°ê³¼:
{
  "generator_mode": "mock",
  "ollama_base_url": "http://100.120.180.42:11434",  # â† ì˜¬ë°”ë¥¸ IP
  "ollama_timeout": 120,
  "ollama_default_model": "qwen2.5:7b",
  "comfyui_base_url": "http://100.120.180.42:8188"
}
```

**âš ï¸ ë§Œì•½ ì—¬ì „íˆ `192.168.0.100`ì´ ë‚˜ì˜¨ë‹¤ë©´**:
- í„°ë¯¸ë„ì„ ì™„ì „íˆ ì¬ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸
- PC ì¬ë¶€íŒ… í›„ ì¬ì‹œë„
- ë˜ëŠ” ì„ì‹œ í•´ê²°ì±…: `config.py`ì—ì„œ `.env` íŒŒì¼ ìš°ì„  ë¡œë“œí•˜ë„ë¡ ìˆ˜ì •

### Task 2: Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ (30ë¶„)

**ëª©í‘œ**: FastAPI ì»¨í…ìŠ¤íŠ¸ì—ì„œ Ollama ì„œë²„ ì •ìƒ ì—°ê²° í™•ì¸

```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://localhost:8000/api/v1/debug/ollama | python -m json.tool

# âœ… ì˜ˆìƒ ê²°ê³¼:
{
  "target_url": "http://100.120.180.42:11434/api/tags",
  "success": true,
  "models": ["qwen2.5:7b", "qwen2.5:14b", "llama2:7b", ...],
  "total_models": 5,
  "elapsed_ms": 245.67
}
```

**âŒ ì‹¤íŒ¨ ì‹œ ë””ë²„ê¹…**:
1. Desktop GPU ì„œë²„(100.120.180.42) ìƒíƒœ í™•ì¸
2. Tailscale VPN ì—°ê²° í™•ì¸
3. Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸:
   ```bash
   # Desktopì—ì„œ
   curl http://localhost:11434/api/tags
   ```

### Task 3: Ollama ìƒì„± í…ŒìŠ¤íŠ¸ (30ë¶„)

**ëª©í‘œ**: ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì „ì²´ ê²€ì¦

```bash
# ìƒì„± í…ŒìŠ¤íŠ¸
curl http://localhost:8000/api/v1/debug/ollama/generate | python -m json.tool

# âœ… ì˜ˆìƒ ê²°ê³¼:
{
  "success": true,
  "response_text": "Hello! How can I help you today?",
  "full_length": 156,
  "elapsed_ms": 1234.56
}
```

**ì„±ëŠ¥ í™•ì¸**:
- ì‘ë‹µ ì‹œê°„: 1~3ì´ˆ ë‚´ì™¸ (qwen2.5:7b ê¸°ì¤€)
- ë” ì˜¤ë˜ ê±¸ë¦¬ë©´ Desktop GPU ë¶€í•˜ í™•ì¸

### Task 4: Live ëª¨ë“œ E2E í…ŒìŠ¤íŠ¸ (1ì‹œê°„)

**ëª©í‘œ**: LLM Gatewayë¥¼ í†µí•œ ì‹¤ì œ Ollama í˜¸ì¶œ ê²€ì¦

#### 4-1. Live ëª¨ë“œ ì „í™˜

`.env` íŒŒì¼ ìˆ˜ì •:
```bash
# backend/.env
GENERATOR_MODE=live  # mock â†’ liveë¡œ ë³€ê²½
```

ì„œë²„ ìë™ ì¬ë¡œë“œ í™•ì¸ (uvicorn --reload)

#### 4-2. LLM Gateway í˜¸ì¶œ

```bash
# SNS ì½˜í…ì¸  ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "role": "copywriter",
    "task": "sns",
    "payload": {
      "product": "ë¬´ì„  ì´ì–´í°",
      "target": "2030 ì—¬ì„±",
      "platform": "instagram"
    },
    "mode": "json"
  }' | python -m json.tool
```

**âœ… ì„±ê³µ ê¸°ì¤€**:
```json
{
  "provider": "ollama",
  "model": "qwen2.5:7b",
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 120,
    "total_tokens": 170
  },
  "output": {
    "platform": "instagram",
    "caption": "ğŸ§ ìƒˆë¡œìš´ ìŒì•… ê²½í—˜...",
    "hashtags": ["ë¬´ì„ ì´ì–´í°", "í”„ë¦¬ë¯¸ì—„ì‚¬ìš´ë“œ"],
    "call_to_action": "ì§€ê¸ˆ ë°”ë¡œ í™•ì¸í•˜ê¸° ğŸ‘‰"
  },
  "meta": {
    "latency_ms": 1234.56,
    "mode": "json",
    "role": "copywriter",
    "task": "sns"
  }
}
```

**ê²€ì¦ í¬ì¸íŠ¸**:
- âœ… `provider: "ollama"` (Mockì´ ì•„ë‹˜)
- âœ… `model: "qwen2.5:7b"` (ì‹¤ì œ ëª¨ë¸)
- âœ… `output`ì´ ì‹¤ì œ LLMì´ ìƒì„±í•œ ë‚´ìš© (Mock í…œí”Œë¦¿ ì•„ë‹˜)
- âœ… `latency_ms`ê°€ 1~3ì´ˆ ë²”ìœ„
- âœ… JSON êµ¬ì¡°ê°€ ì˜¬ë°”ë¦„

#### 4-3. ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

```bash
# 1. ìƒí’ˆ ìƒì„¸ ì„¤ëª… ìƒì„±
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "role": "copywriter",
    "task": "product_detail",
    "payload": {"product": "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜"},
    "mode": "json"
  }' | python -m json.tool

# 2. ë¸Œëœë“œí‚· ìƒì„±
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "role": "strategist",
    "task": "brand_kit",
    "payload": {"brand": "ì—ì½”í…Œí¬"},
    "mode": "json"
  }' | python -m json.tool

# 3. Text ëª¨ë“œ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "role": "copywriter",
    "task": "sns",
    "payload": {"product": "ì¹œí™˜ê²½ í…€ë¸”ëŸ¬"},
    "mode": "text"
  }' | python -m json.tool
```

#### 4-4. ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸

```bash
# ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìš”ì²­
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "role": "copywriter",
    "task": "sns",
    "payload": {"product": "í…ŒìŠ¤íŠ¸"},
    "mode": "json",
    "options": {"model": "nonexistent-model"}
  }' | python -m json.tool

# âœ… ì˜ˆìƒ: ì ì ˆí•œ ì—ëŸ¬ ì‘ë‹µ (ProviderError)
```

### Task 5: ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦ (30ë¶„)

#### 5-1. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# ì—°ì† 5íšŒ ìš”ì²­
for i in {1..5}; do
  echo "=== Request $i ==="
  curl -X POST http://localhost:8000/api/v1/llm/generate \
    -H "Content-Type: application/json" \
    -d '{
      "role": "copywriter",
      "task": "sns",
      "payload": {"product": "í…ŒìŠ¤íŠ¸ ìƒí’ˆ '$i'"},
      "mode": "json"
    }' -w "\nTime: %{time_total}s\n"
  sleep 1
done
```

**í™•ì¸ì‚¬í•­**:
- ëª¨ë“  ìš”ì²­ ì„±ê³µ
- ì‘ë‹µ ì‹œê°„ ì¼ê´€ì„±
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ

#### 5-2. ë¡œê·¸ ë¶„ì„

ì„œë²„ ë¡œê·¸ì—ì„œ í™•ì¸:
```
INFO: Ollama API call: url=http://100.120.180.42:11434/api/generate, model=qwen2.5:7b
INFO: Ollama success: qwen2.5:7b - elapsed=1.23s
INFO: LLM Success: ollama/qwen2.5:7b - elapsed=1.25s, tokens=170
```

---

## ğŸ“ Task 6: ë¬¸ì„œí™” ë° ì»¤ë°‹ (1ì‹œê°„)

### 6-1. Phase 1-4 ì™„ë£Œ ë³´ê³ ì„œ ì‘ì„±

**íŒŒì¼**: `docs/requests/responses/BíŒ€_Phase1-4_ì™„ë£Œë³´ê³ _2025-11-17.md`

**í¬í•¨ ë‚´ìš©**:
- í™˜ê²½ë³€ìˆ˜ ì´ìŠˆ í•´ê²° ê³¼ì •
- Live ëª¨ë“œ ê²€ì¦ ê²°ê³¼
- ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼
- Mock vs Live ë¹„êµ ë¶„ì„
- Phase 1 ì „ì²´ íšŒê³ 

### 6-2. Git ì»¤ë°‹

```bash
git add docs/requests/responses/BíŒ€_Phase1-4_ì™„ë£Œë³´ê³ _2025-11-17.md
git commit -m "docs(gateway): Phase 1-4 ì™„ë£Œ ë³´ê³  - Live ëª¨ë“œ ê²€ì¦ ì™„ë£Œ

- Windows í™˜ê²½ë³€ìˆ˜ ì´ìŠˆ í•´ê²°
- Live ëª¨ë“œ E2E í…ŒìŠ¤íŠ¸ ì„±ê³µ
- ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦ ì™„ë£Œ
- Phase 1 ì „ì²´ ì™„ë£Œ"
```

---

## ğŸš€ Phase 2 ì¤€ë¹„ (ì‹œê°„ ì—¬ìœ  ì‹œ)

Phase 1ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ Phase 2 ì¤€ë¹„ ì‘ì—…:

### Router ê³ ë„í™” (Phase 2-1 ì˜ˆìƒ)

**ëª©í‘œ**: ì‘ì—…ë³„ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ

```python
# router.py í™•ì¥ ì•„ì´ë””ì–´
TASK_MODEL_MAP = {
    "sns": "qwen2.5:7b",        # ë¹ ë¥¸ ì‘ë‹µ í•„ìš”
    "product_detail": "qwen2.5:14b",  # í’ˆì§ˆ ìš°ì„ 
    "brand_kit": "qwen2.5:14b",
    "review": "qwen2.5:7b"
}
```

### ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”

- Retry ë¡œì§ ì¶”ê°€ (ì§€ìˆ˜ ë°±ì˜¤í”„)
- Circuit Breaker íŒ¨í„´
- Fallback Provider

### ëª¨ë‹ˆí„°ë§ ì¶”ê°€

- Prometheus metrics
- ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
- ì—ëŸ¬ìœ¨ ì¶”ì 

---

## âš ï¸ ì˜ˆìƒ ì´ìŠˆ ë° í•´ê²° ë°©ì•ˆ

### Issue 1: í™˜ê²½ë³€ìˆ˜ ì—¬ì „íˆ ë¡œë“œ ì•ˆ ë¨

**ì¦ìƒ**: ì¬ë¶€íŒ… í›„ì—ë„ ì—¬ì „íˆ `192.168.0.100` ë¡œë“œ

**í•´ê²°**:
```python
# config.py ì„ì‹œ ìˆ˜ì • (ê¸´ê¸‰)
class Settings(BaseSettings):
    OLLAMA_BASE_URL: str = "http://100.120.180.42:11434"

    class Config:
        env_file = ".env"
        env_file_encoding = 'utf-8'
        case_sensitive = True
        # í™˜ê²½ë³€ìˆ˜ë³´ë‹¤ .env ìš°ì„ í•˜ë„ë¡ ê°•ì œ
        @classmethod
        def customise_sources(cls, init_settings, env_settings, file_secret_settings):
            return (
                init_settings,
                file_secret_settings,  # .env ìš°ì„ 
                env_settings,
            )
```

### Issue 2: Ollama ì„œë²„ ì—°ê²° ë¶ˆì•ˆì •

**ì¦ìƒ**: ê°„í—ì  ConnectError

**í™•ì¸**:
1. Tailscale ìƒíƒœ: `tailscale status`
2. Desktop GPU ì„œë²„ ë„¤íŠ¸ì›Œí¬
3. Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘

**ì„ì‹œ í•´ê²°**: Retry ë¡œì§ ì¶”ê°€

### Issue 3: JSON íŒŒì‹± ì‹¤íŒ¨

**ì¦ìƒ**: Ollama ì‘ë‹µì´ JSONì´ ì•„ë‹˜

**ì›ì¸**: ëª¨ë¸ì´ JSON í˜•ì‹ì„ ì§€í‚¤ì§€ ì•ŠìŒ

**í•´ê²°**:
- í”„ë¡¬í”„íŠ¸ì— JSON êµ¬ì¡° ëª…ì‹œ ê°•í™”
- íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ í´ë°±

---

## ğŸ“Š ì„±ê³µ ê¸°ì¤€

### Phase 1 ì „ì²´ ì™„ë£Œ ì¡°ê±´

- âœ… Mock ëª¨ë“œ ì •ìƒ ë™ì‘
- âœ… Live ëª¨ë“œ ì •ìƒ ë™ì‘ (Ollama)
- âœ… Debug ì—”ë“œí¬ì¸íŠ¸ 3ì¢… ì‘ë™
- âœ… E2E í…ŒìŠ¤íŠ¸ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í†µê³¼
- âœ… ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡± (ì‘ë‹µ ì‹œê°„ < 3ì´ˆ)
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ì ì ˆ
- âœ… ë¬¸ì„œí™” ì™„ë£Œ

### í’ˆì§ˆ ê¸°ì¤€

- ì½”ë“œ ë¦¬ë·° ê°€ëŠ¥ ìƒíƒœ
- ë¡œê¹… ì¶©ë¶„íˆ ìƒì„¸
- ì—ëŸ¬ ë©”ì‹œì§€ ëª…í™•
- API ëª…ì„¸ ì¤€ìˆ˜

---

## ğŸ“… íƒ€ì„ë¼ì¸

| ì‹œê°„ | ì‘ì—… | ë¹„ê³  |
|------|------|------|
| 09:00-09:30 | í™˜ê²½ ê²€ì¦ | Task 1 |
| 09:30-10:00 | Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ | Task 2 |
| 10:00-10:30 | Ollama ìƒì„± í…ŒìŠ¤íŠ¸ | Task 3 |
| 10:30-11:30 | Live ëª¨ë“œ E2E | Task 4 |
| 11:30-12:00 | ì„±ëŠ¥ ê²€ì¦ | Task 5 |
| 13:00-14:00 | ë¬¸ì„œí™” ë° ì»¤ë°‹ | Task 6 |
| 14:00-17:00 | Phase 2 ì¤€ë¹„ (ì„ íƒ) | ì—¬ìœ  ì‹œ |

---

## ğŸ”— ë¹ ë¥¸ ì°¸ì¡°

### ì£¼ìš” ëª…ë ¹ì–´

```bash
# ì„œë²„ ì‹œì‘
cd /k/sparklio_ai_marketing_studio/backend
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# ì„¤ì • í™•ì¸
curl http://localhost:8000/api/v1/debug/settings | python -m json.tool

# Ollama ì—°ê²°
curl http://localhost:8000/api/v1/debug/ollama | python -m json.tool

# ìƒì„± í…ŒìŠ¤íŠ¸
curl http://localhost:8000/api/v1/debug/ollama/generate | python -m json.tool

# LLM Gateway (Live)
curl -X POST http://localhost:8000/api/v1/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"role": "copywriter", "task": "sns", "payload": {"product": "í…ŒìŠ¤íŠ¸"}, "mode": "json"}' \
  | python -m json.tool
```

### ì£¼ìš” íŒŒì¼

- `backend/.env` - í™˜ê²½ ì„¤ì •
- `backend/app/core/config.py` - ì„¤ì • í´ë˜ìŠ¤
- `backend/app/services/llm/providers/ollama.py` - Ollama Provider
- `backend/app/services/llm/gateway.py` - LLM Gateway
- `backend/app/api/v1/endpoints/debug.py` - Debug API

### ì„œë²„ ì •ë³´

- **Mac mini (API ì„œë²„)**: 100.123.51.5
- **Desktop GPU (Ollama)**: 100.120.180.42:11434
- **Desktop GPU (ComfyUI)**: 100.120.180.42:8188

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‘ì—… ì‹œì‘ ì „:
- [ ] Windows í™˜ê²½ë³€ìˆ˜ ì •ë¦¬ ì™„ë£Œ
- [ ] PC ì¬ë¶€íŒ… ë˜ëŠ” ë¡œê·¸ì•„ì›ƒ/ë¡œê·¸ì¸
- [ ] Phase 1-3 ì™„ë£Œë³´ê³  ì½ê¸°
- [ ] ARCH-002, SPEC-001 ì¬í™•ì¸

ì‘ì—… ì¤‘:
- [ ] Task 1: í™˜ê²½ ê²€ì¦ âœ…
- [ ] Task 2: Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ âœ…
- [ ] Task 3: Ollama ìƒì„± í…ŒìŠ¤íŠ¸ âœ…
- [ ] Task 4: Live ëª¨ë“œ E2E âœ…
- [ ] Task 5: ì„±ëŠ¥ ê²€ì¦ âœ…
- [ ] Task 6: ë¬¸ì„œí™” ë° ì»¤ë°‹ âœ…

ì‘ì—… ì™„ë£Œ í›„:
- [ ] Phase 1-4 ì™„ë£Œë³´ê³  ì‘ì„±
- [ ] Git ì»¤ë°‹ ë° í‘¸ì‹œ
- [ ] ë‹¤ìŒ ì‘ì—… ê³„íš ì‘ì„±
- [ ] íŒ€ ê³µìœ 

---

**ì‘ì„±ì**: BíŒ€ (Backend)
**ì‘ì„±ì¼**: 2025-11-16
**ë‹¤ìŒ ê²€í† ì¼**: 2025-11-17 ì˜¤í›„
