# LLM Router Policy Document

> **Version**: 1.1
> **Date**: 2025-11-13 (ëª©ìš”ì¼ ì˜¤í›„ 5:59)
> **Status**: Final
> **Owner**: AI Infrastructure Team

---

## 1. Executive Summary

Smart LLM RouterëŠ” ì‘ì—… íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ ì–¸ì–´ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. **ë¹„ìš©**, **ì†ë„**, **í’ˆì§ˆ**, **í”„ë¼ì´ë²„ì‹œ**ë¥¼ ê· í˜•ìˆê²Œ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

### 1.1 í•µì‹¬ ëª©í‘œ
- **ë¹„ìš© ìµœì í™”**: ì›”ê°„ LLM ë¹„ìš© 40% ì ˆê°
- **ì„±ëŠ¥ ë³´ì¥**: ì‘ë‹µ ì‹œê°„ SLA ì¤€ìˆ˜
- **í’ˆì§ˆ ìœ ì§€**: ì¶œë ¥ í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±
- **í™•ì¥ì„±**: ìƒˆ ëª¨ë¸ ì‰½ê²Œ ì¶”ê°€

### 1.2 ë¼ìš°íŒ… ì›ì¹™
```
ìµœì  ëª¨ë¸ = f(Task, Cost, Latency, Quality, Resource, Privacy)
```

---

## 2. Multi-Node Infrastructure

Sparklio.aiëŠ” **3-Node Hybrid í™˜ê²½**ì—ì„œ ë¡œì»¬/í´ë¼ìš°ë“œ ëª¨ë¸ì„ ìœ ì—°í•˜ê²Œ í™œìš©í•©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ–¥ Desktop (ì£¼ë§)      â”‚â”€â”€â”€â”€â”‚  ğŸ’» Laptop (í‰ì¼)       â”‚â”€â”€â”€â”€â”‚  ğŸ Mac mini M2 (24/7)  â”‚
â”‚  RTX 4070 SUPER        â”‚     â”‚  RTX 4060 Laptop       â”‚     â”‚  M2 + Neural Engine     â”‚
â”‚  â€¢ ì´ë¯¸ì§€/ì˜ìƒ ì¶”ë¡      â”‚     â”‚  â€¢ ê°œë°œÂ·ì‹œì—°Â·í”„ë¡ íŠ¸     â”‚     â”‚  â€¢ API Server          â”‚
â”‚  â€¢ ë¡œì»¬ LLM 7B~13B    â”‚     â”‚  â€¢ í…ŒìŠ¤íŠ¸ í™˜ê²½          â”‚     â”‚  â€¢ DB/Redis            â”‚
â”‚  â€¢ Stable Diffusion    â”‚     â”‚  â€¢ í”„ë ˆì  í…Œì´ì…˜        â”‚     â”‚  â€¢ Worker/Scheduler    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     Tailscale VPN + MinIO (Media) + Git (Code)
```

### ë…¸ë“œë³„ ì—­í• 

- **Desktop (RTX 4070 SUPER)**: ê³ ì„±ëŠ¥ ì¶”ë¡  ì „ë‹´ (SDXL, Llama 70B, Qwen 14B)
- **Laptop (RTX 4060)**: ê°œë°œ ë° ê²½ëŸ‰ ì¶”ë¡  (Llama 8B, Mistral 7B)
- **Mac mini M2**: API ì„œë²„ + ë°ì´í„°ë² ì´ìŠ¤ + ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì‹œ ìš´ì˜

### ë¼ìš°íŒ… ì „ëµ

```python
class NodeAwareRouter:
    """
    ë…¸ë“œ ê°€ìš©ì„± ê¸°ë°˜ ë¼ìš°íŒ…
    """

    async def select_inference_node(
        self,
        model: str,
        task_priority: str
    ) -> str:
        """ì¶”ë¡  ë…¸ë“œ ì„ íƒ"""

        # Desktop GPU ì˜¨ë¼ì¸ ì²´í¬
        desktop_available = await self.check_node_health('desktop')

        # ê³ ì„±ëŠ¥ ëª¨ë¸ â†’ Desktop ìš°ì„ 
        if model in ['llama-70b', 'sdxl', 'qwen-14b']:
            if desktop_available:
                return 'desktop'
            else:
                # Fallback to cloud
                return 'cloud'

        # ê²½ëŸ‰ ëª¨ë¸ â†’ Laptop ë˜ëŠ” Desktop
        if model in ['llama-8b', 'mistral-7b']:
            laptop_available = await self.check_node_health('laptop')
            if laptop_available:
                return 'laptop'
            elif desktop_available:
                return 'desktop'
            else:
                return 'cloud'

        # í´ë¼ìš°ë“œ ëª¨ë¸
        return 'cloud'
```

---

## 3. Model Catalog

### 3.1 Text Generation Models

| Model | Provider | Type | Cost/1K | Latency | Quality | Best For |
|-------|----------|------|---------|---------|---------|----------|
| **GPT-5** | OpenAI | Cloud | $0.015 | 3-6s | â­â­â­â­â­ | ìµœê³  ë‚œì´ë„ ì „ëµÂ·ë¶„ì„ |
| **GPT-4.1** | OpenAI | Cloud | $0.012 | 2-5s | â­â­â­â­â­ | Complex reasoning |
| **GPT-4-Turbo** | OpenAI | Cloud | $0.01 | 2-5s | â­â­â­â­â­ | Complex reasoning |
| **GPT-4o** | OpenAI | Cloud | $0.005 | 1-3s | â­â­â­â­â­ | Balanced tasks |
| **GPT-4o-mini** | OpenAI | Cloud | $0.0015 | <1s | â­â­â­â­ | Fast responses |
| **Claude 3.5 Sonnet** | Anthropic | Cloud | $0.003 | 2-4s | â­â­â­â­â­ | í†¤ ì•ˆì •/ê¸´ ë¬¸ì„œ |
| **Claude 3.5 Haiku** | Anthropic | Cloud | $0.0008 | <1s | â­â­â­â­ | Quick tasks |
| **Gemini 2.5 Pro** | Google | Cloud | $0.0025 | 1-2s | â­â­â­â­ | Multimodal |
| **Gemini 2.5 Flash** | Google | Cloud | $0.0003 | <1s | â­â­â­ | ìš”ì•½/SNS/ì‹¤ì‹œê°„ ì±— |
| **Pi** | Inflection | Cloud | $0.0002 | <1s | â­â­â­ | ê°€ë²¼ìš´ ì–´ì‹œìŠ¤íŠ¸ |
| **Llama 3.1 70B** | Meta | Local | $0.0001* | 3-8s | â­â­â­â­ | Private data |
| **Llama 3.1 8B** | Meta | Local | $0.00005* | 1-2s | â­â­â­ | í”„ë ˆì  í…Œì´ì…˜/ìš”ì•½ |
| **Qwen2 14B** | Alibaba | Local | $0.00008* | 2-4s | â­â­â­â­ | í…œí”Œë¦¿/í”„ë ˆì  í…Œì´ì…˜ |
| **Mistral 7B** | Mistral | Local | $0.00005* | <1s | â­â­â­ | íŠ¸ë Œë“œ ë¶„ì„ |

*Local model costs are estimated based on electricity and hardware amortization

### 3.2 Image Generation Models

| Model | Provider | Type | Cost/Image | Time | Quality | Best For |
|-------|----------|------|------------|------|---------|----------|
| **DALL-E 3** | OpenAI | Cloud | $0.04 | 10-20s | â­â­â­â­â­ | High quality |
| **DALL-E 2** | OpenAI | Cloud | $0.02 | 5-10s | â­â­â­â­ | Standard |
| **Midjourney v6** | Midjourney | Cloud | $0.03 | 30-60s | â­â­â­â­â­ | Artistic |
| **NanoBanana** | Custom | Cloud | $0.01 | 5-15s | â­â­â­â­ | ì¸ë„¤ì¼/ì‹œê° ì•„ì´ë””ì–´ |
| **SD XL** | Stability | Local | $0.001* | 10-30s | â­â­â­â­ | ë¸Œëœë“œ íŠ¹í™” LoRA |
| **SD 1.5** | Stability | Local | $0.0005* | 5-15s | â­â­â­ | Fast local |

### 3.3 Video Generation Models

| Model | Provider | Type | Cost/Sec | Time/Sec | Quality | Best For |
|-------|----------|------|----------|----------|---------|----------|
| **Sora2** | OpenAI | Cloud | $0.50 | 60s | â­â­â­â­â­ | ê´‘ê³ /ì‡¼ì¸  í•©ì„± |
| **Runway Gen-3** | Runway | Cloud | $0.30 | 30s | â­â­â­â­ | Standard |
| **Pika Labs** | Pika | Cloud | $0.20 | 20s | â­â­â­ | Quick drafts |

### 3.4 Embedding Models

| Model | Provider | Dimensions | Cost/1M | Best For |
|-------|----------|------------|---------|----------|
| **text-embedding-ada-002** | OpenAI | 1536 | $0.10 | General |
| **text-embedding-3-small** | OpenAI | 1536 | $0.02 | Cost-effective |
| **voyage-2** | Voyage | 1024 | $0.10 | Quality |
| **e5-large-v2** | Local | 1024 | $0.001* | Private |

---

## 4. Agent Integration

Smart LLM RouterëŠ” **AGENTS_SPEC.md**ì— ì •ì˜ëœ 16ê°œ ì—ì´ì „íŠ¸ì™€ ê¸´ë°€í•˜ê²Œ í†µí•©ë©ë‹ˆë‹¤.

### 4.1 ì—ì´ì „íŠ¸ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘

```python
AGENT_MODEL_PREFERENCES = {
    # Creation Agents
    "StrategistAgent": {
        "primary": ["gpt-5", "claude-3.5-sonnet", "gpt-4.1"],
        "fallback": ["gemini-2.5-pro", "llama-3.1-70b"],
        "preset": "high_fidelity"  # ì „ëµì€ í’ˆì§ˆ ìš°ì„ 
    },
    "CopywriterAgent": {
        "primary": ["claude-3.5-sonnet", "gpt-4o"],
        "fallback": ["gemini-2.5-pro", "qwen2-14b"],
        "preset": "balanced"  # í†¤Â·ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ì¤‘ìš”
    },
    "VisionGeneratorAgent": {
        "primary": ["dall-e-3", "nanobanana"],
        "fallback": ["sdxl"],
        "preset": "high_fidelity"  # ë¸Œëœë“œ ì´ë¯¸ì§€ëŠ” í’ˆì§ˆ ìš°ì„ 
    },
    "VideoDirectorAgent": {
        "primary": ["gpt-4o", "gemini-2.5-pro"],
        "fallback": ["claude-3.5-sonnet"],
        "preset": "balanced"
    },

    # Intelligence Agents
    "TrendCollectorAgent": {
        "primary": ["gemini-2.5-flash", "pi"],
        "fallback": ["mistral-7b"],
        "preset": "draft_fast"  # ë¹ ë¥¸ ìš”ì•½ ìš°ì„ 
    },
    "DataCleanerAgent": {
        "primary": ["gemini-2.5-flash", "gpt-4o-mini"],
        "fallback": ["llama-3.1-8b"],
        "preset": "cost_optimized"  # ëŒ€ëŸ‰ ì²˜ë¦¬, ë¹„ìš© ìµœì†Œí™”
    },
    "RAGAgent": {
        "primary": ["gpt-4o", "claude-3.5-sonnet"],
        "fallback": ["gemini-2.5-pro", "llama-3.1-70b"],
        "preset": "balanced"  # ì •í™•ë„ì™€ ì†ë„ ê· í˜•
    },
    "ReviewerAgent": {
        "primary": ["claude-3.5-sonnet", "gpt-4o"],
        "fallback": ["gemini-2.5-pro"],
        "preset": "high_fidelity"  # í’ˆì§ˆ í‰ê°€ëŠ” ì •í™•ë„ ì¤‘ìš”
    },
    "PerformanceAnalyzerAgent": {
        "primary": ["gpt-4o", "gemini-2.5-pro"],
        "fallback": ["llama-3.1-70b"],
        "preset": "balanced"
    },

    # System Agents
    "PMAgent": {
        "primary": ["gpt-4o", "claude-3.5-sonnet"],
        "fallback": ["gemini-2.5-pro"],
        "preset": "balanced"  # ì›Œí¬í”Œë¡œ ì¡°ìœ¨
    },
    "BudgetAgent": {
        "primary": ["gemini-2.5-flash", "gpt-4o-mini"],
        "fallback": ["mistral-7b"],
        "preset": "cost_optimized"  # ë¹„ìš© ì¶”ì ì€ ê²½ëŸ‰ ëª¨ë¸
    },
    "ADAgent": {
        "primary": ["gpt-4o", "gemini-2.5-pro"],
        "fallback": ["claude-3.5-sonnet"],
        "preset": "balanced"  # ê´‘ê³  ìµœì í™”
    }
}
```

### 4.2 ì—ì´ì „íŠ¸ í˜¸ì¶œ íŒ¨í„´

```python
class AgentAwareLLMRouter:
    """
    ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¼ìš°íŒ…
    """

    async def route_for_agent(
        self,
        agent_name: str,
        task: Dict[str, Any],
        override_preset: Optional[str] = None
    ) -> SelectedModel:
        """ì—ì´ì „íŠ¸ë³„ ìµœì  ëª¨ë¸ ì„ íƒ"""

        # ì—ì´ì „íŠ¸ í”„ë¦¬í¼ëŸ°ìŠ¤ ë¡œë“œ
        preferences = AGENT_MODEL_PREFERENCES.get(agent_name, {})
        preset = override_preset or preferences.get("preset", "balanced")

        # ì‘ì—… ë¶„ì„
        task_obj = self.analyze_task(task, agent_name)

        # ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì‹œë„
        for model_name in preferences.get("primary", []):
            if await self.is_available(model_name):
                return await self.select_model(
                    task_obj,
                    mode=f"preset:{preset}",
                    preferred_model=model_name
                )

        # Fallback ëª¨ë¸
        for model_name in preferences.get("fallback", []):
            if await self.is_available(model_name):
                return await self.select_model(
                    task_obj,
                    mode=f"preset:{preset}",
                    preferred_model=model_name
                )

        # ìµœí›„ ìë™ ì„ íƒ
        return await self.auto_select(task_obj)
```

### 4.3 Brand Learning Engine í†µí•©

**BRAND_LEARNING_ENGINE.md**ì˜ Self-Learning Loopì—ì„œ ìƒì„±ë˜ëŠ” ë¸Œëœë“œ ë²¡í„°ë¥¼ ë¼ìš°íŒ…ì— í™œìš©:

```python
class BrandAwareLLMRouter:
    """
    ë¸Œëœë“œ ì¼ê´€ì„± ê¸°ë°˜ ë¼ìš°íŒ…
    """

    async def select_with_brand_context(
        self,
        task: Task,
        brand_id: str
    ) -> SelectedModel:
        """ë¸Œëœë“œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ"""

        # ë¸Œëœë“œ ë²¡í„° ë¡œë“œ
        brand_vector = await self.load_brand_vector(brand_id)

        # ëª¨ë¸ë³„ ë¸Œëœë“œ ì¼ê´€ì„± ì ìˆ˜
        consistency_scores = {}
        for model in self.get_candidates(task):
            # ì´ì „ ìƒì„±ë¬¼ì˜ ë¸Œëœë“œ ì¼ê´€ì„± íˆìŠ¤í† ë¦¬
            history = await self.get_brand_consistency_history(
                brand_id,
                model.name
            )
            consistency_scores[model.name] = np.mean(history) if history else 0.7

        # ê¸°ì¡´ ì ìˆ˜ì— ë¸Œëœë“œ ì¼ê´€ì„± ê°€ì¤‘ì¹˜ ì¶”ê°€
        adjusted_scores = {}
        for model in self.get_candidates(task):
            base_score = self.calculator.calculate_score(task, model)
            brand_score = consistency_scores[model.name]

            # ë¸Œëœë“œ ì¼ê´€ì„±ì„ 20% ê°€ì¤‘ì¹˜ë¡œ ë°˜ì˜
            adjusted_scores[model.name] = (
                base_score * 0.8 + brand_score * 0.2
            )

        best_model_name = max(adjusted_scores, key=adjusted_scores.get)
        return self.models.get(best_model_name)
```

---

## 5. Routing Algorithm

### 5.1 Score Calculation

```python
class RouterScoreCalculator:
    """
    ë¼ìš°íŒ… ì ìˆ˜ ê³„ì‚°ê¸°
    """

    def __init__(self):
        self.weights = {
            'cost': 0.3,      # ë¹„ìš© ê°€ì¤‘ì¹˜
            'latency': 0.25,  # ì§€ì—°ì‹œê°„ ê°€ì¤‘ì¹˜
            'quality': 0.25,  # í’ˆì§ˆ ê°€ì¤‘ì¹˜
            'resource': 0.1,  # ë¦¬ì†ŒìŠ¤ ê°€ì¤‘ì¹˜
            'privacy': 0.1    # í”„ë¼ì´ë²„ì‹œ ê°€ì¤‘ì¹˜
        }

    def calculate_score(self, task: Task, model: Model) -> float:
        """
        ì¢…í•© ì ìˆ˜ ê³„ì‚°
        Score = Î£(weight_i Ã— normalized_score_i)
        """

        scores = {
            'cost': self.cost_score(task, model),
            'latency': self.latency_score(task, model),
            'quality': self.quality_score(task, model),
            'resource': self.resource_score(task, model),
            'privacy': self.privacy_score(task, model)
        }

        # Normalize scores to [0, 1]
        normalized = self.normalize_scores(scores)

        # Calculate weighted sum
        total = sum(
            self.weights[key] * normalized[key]
            for key in scores
        )

        return total

    def cost_score(self, task: Task, model: Model) -> float:
        """ë¹„ìš© ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        estimated_tokens = task.estimated_tokens
        cost = model.cost_per_1k * (estimated_tokens / 1000)

        # Inverse score: lower cost = higher score
        max_acceptable_cost = 1.0  # $1
        return max(0, 1 - (cost / max_acceptable_cost))

    def latency_score(self, task: Task, model: Model) -> float:
        """ì§€ì—°ì‹œê°„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        estimated_latency = model.base_latency + (
            task.estimated_tokens * model.tokens_per_second
        )

        # SLA based scoring
        if task.priority == 'realtime':
            target_latency = 1.0  # 1 second
        elif task.priority == 'interactive':
            target_latency = 5.0  # 5 seconds
        else:
            target_latency = 30.0  # 30 seconds

        return max(0, 1 - (estimated_latency / target_latency))

    def quality_score(self, task: Task, model: Model) -> float:
        """í’ˆì§ˆ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        # Task-specific quality requirements
        quality_matrix = {
            'reasoning': {
                'gpt-4-turbo': 1.0,
                'claude-3.5-sonnet': 0.95,
                'gemini-2.5-pro': 0.9,
                'llama-3.1-70b': 0.85
            },
            'creative': {
                'claude-3.5-sonnet': 1.0,
                'gpt-4-turbo': 0.95,
                'gemini-2.5-pro': 0.85,
                'llama-3.1-70b': 0.8
            },
            'summarization': {
                'gemini-2.5-flash': 0.9,
                'gpt-4o-mini': 0.85,
                'claude-3.5-haiku': 0.9,
                'llama-3.1-8b': 0.75
            }
        }

        task_type = task.type
        model_name = model.name

        if task_type in quality_matrix:
            return quality_matrix[task_type].get(model_name, 0.5)
        return 0.7  # Default quality score

    def resource_score(self, task: Task, model: Model) -> float:
        """ë¦¬ì†ŒìŠ¤ ê°€ìš©ì„± ì ìˆ˜"""
        if model.type == 'local':
            # Check GPU availability
            gpu_available = self.check_gpu_availability()
            gpu_memory_free = self.get_gpu_memory_free()

            if not gpu_available:
                return 0.0

            # Check if model fits in memory
            if model.memory_requirement > gpu_memory_free:
                return 0.0

            # Score based on utilization
            utilization = self.get_gpu_utilization()
            return max(0, 1 - (utilization / 100))
        else:
            # Cloud models: check rate limits
            rate_limit_usage = self.get_rate_limit_usage(model)
            return max(0, 1 - rate_limit_usage)

    def privacy_score(self, task: Task, model: Model) -> float:
        """í”„ë¼ì´ë²„ì‹œ ì ìˆ˜"""
        if task.contains_pii or task.confidential:
            # Prefer local models for sensitive data
            return 1.0 if model.type == 'local' else 0.2
        return 0.7  # Neutral for non-sensitive data
```

### 5.2 Model Selection Logic

```python
class SmartRouter:
    """
    ìŠ¤ë§ˆíŠ¸ LLM ë¼ìš°í„°
    """

    def __init__(self):
        self.calculator = RouterScoreCalculator()
        self.models = ModelRegistry()
        self.cache = ModelSelectionCache()

    async def select_model(
        self,
        task: Task,
        mode: str = 'auto'
    ) -> SelectedModel:
        """
        ìµœì  ëª¨ë¸ ì„ íƒ
        """

        # Check cache
        cache_key = self.get_cache_key(task)
        if cached := self.cache.get(cache_key):
            return cached

        if mode == 'auto':
            model = await self.auto_select(task)
        elif mode == 'manual':
            model = await self.manual_select(task)
        elif mode == 'preset':
            model = await self.preset_select(task)
        else:
            raise ValueError(f"Unknown mode: {mode}")

        # Cache the selection
        self.cache.set(cache_key, model, ttl=300)

        return model

    async def auto_select(self, task: Task) -> SelectedModel:
        """ìë™ ëª¨ë¸ ì„ íƒ"""

        # Get eligible models
        candidates = await self.get_candidates(task)

        # Calculate scores
        scores = {}
        for model in candidates:
            score = self.calculator.calculate_score(task, model)
            scores[model.name] = score

        # Select best model
        best_model_name = max(scores, key=scores.get)
        best_model = self.models.get(best_model_name)

        # Check if cost warning needed
        if self.needs_cost_warning(task, best_model):
            await self.send_cost_warning(task, best_model)

        return SelectedModel(
            model=best_model,
            score=scores[best_model_name],
            reasoning=self.explain_selection(task, best_model, scores)
        )

    async def get_candidates(self, task: Task) -> List[Model]:
        """ì‘ì—…ì— ì í•©í•œ í›„ë³´ ëª¨ë¸ í•„í„°ë§"""
        all_models = self.models.get_all()
        candidates = []

        for model in all_models:
            # Check capability match
            if not self.check_capability(task, model):
                continue

            # Check availability
            if not await self.check_availability(model):
                continue

            # Check budget constraint
            if not self.check_budget(task, model):
                continue

            candidates.append(model)

        if not candidates:
            raise NoSuitableModelError(task)

        return candidates
```

### 5.3 Preset Configurations

README.mdì— ì •ì˜ëœ 3ê°€ì§€ í”„ë¦¬ì…‹ ëª¨ë“œë¥¼ í¬í•¨í•œ ì´ 5ê°€ì§€ í”„ë¦¬ì…‹:

#### 5.3.1 Draft Fast (ë¹ ë¥¸ ì´ˆì•ˆ)

**ìš©ë„**: ë¹ ë¥¸ ì´ˆì•ˆ ìƒì„± (ì†ë„ ìš°ì„ )
- **ê°€ì¤‘ì¹˜**: ì†ë„ 50%, ë¹„ìš© 20%, í’ˆì§ˆ 20%
- **ìµœì  ëª¨ë¸**: Gemini Flash, GPT-4o-mini, Pi, Mistral
- **ì‚¬ìš© ì¼€ì´ìŠ¤**: SNS ì¹´í”¼ ì´ˆì•ˆ, íŠ¸ë Œë“œ ìš”ì•½, ê°„ë‹¨í•œ ì§ˆë¬¸ ì‘ë‹µ

#### 5.3.2 Balanced (ê· í˜•)

**ìš©ë„**: ê· í˜•ì¡íŒ í’ˆì§ˆê³¼ ì†ë„
- **ê°€ì¤‘ì¹˜**: í’ˆì§ˆ 25%, ì†ë„ 25%, ë¹„ìš© 25%, ë¦¬ì†ŒìŠ¤ 15%, í”„ë¼ì´ë²„ì‹œ 10%
- **ìµœì  ëª¨ë¸**: GPT-4o, Claude Sonnet, Gemini Pro, Llama 70B
- **ì‚¬ìš© ì¼€ì´ìŠ¤**: ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ ì‘ì—… (ê¸°ë³¸ê°’)

#### 5.3.3 High-Fidelity (ìµœê³  í’ˆì§ˆ)

**ìš©ë„**: ìµœê³  í’ˆì§ˆ (í’ˆì§ˆ ìš°ì„ )
- **ê°€ì¤‘ì¹˜**: í’ˆì§ˆ 60%, ë¹„ìš© 10%, ì†ë„ 10%
- **ìµœì  ëª¨ë¸**: GPT-5, GPT-4.1, Claude Sonnet
- **ì‚¬ìš© ì¼€ì´ìŠ¤**: ì „ëµ ë¸Œë¦¬í”„, ì¤‘ìš” ì¹´í”¼, ë¸Œëœë“œ ì´ë¯¸ì§€, í”„ë ˆì  í…Œì´ì…˜

#### 5.3.4 Privacy First (í”„ë¼ì´ë²„ì‹œ ìš°ì„ )

**ìš©ë„**: ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©
- **ê°€ì¤‘ì¹˜**: í”„ë¼ì´ë²„ì‹œ 50%, í’ˆì§ˆ 20%, ë¹„ìš© 10%
- **ìµœì  ëª¨ë¸**: Llama 70B/8B, Qwen 14B, Mistral 7B
- **ì‚¬ìš© ì¼€ì´ìŠ¤**: ë¯¼ê° ë°ì´í„° ì²˜ë¦¬, ë‚´ë¶€ ë¬¸ì„œ, ê°œì¸ì •ë³´ í¬í•¨

#### 5.3.5 Cost Optimized (ë¹„ìš© ìµœì í™”)

**ìš©ë„**: ë¹„ìš© ìµœì†Œí™”
- **ê°€ì¤‘ì¹˜**: ë¹„ìš© 60%, ì†ë„ 15%, í’ˆì§ˆ 15%
- **ìµœì  ëª¨ë¸**: ë¡œì»¬ ëª¨ë¸ ìš°ì„  â†’ Gemini Flash
- **ì‚¬ìš© ì¼€ì´ìŠ¤**: ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬, í¬ë¡¤ë§ í›„ ìš”ì•½, ë°°ì¹˜ ì‘ì—…

```python
ROUTER_PRESETS = {
    "draft_fast": {
        "description": "ë¹ ë¥¸ ì´ˆì•ˆ ìƒì„± (ì†ë„ ìš°ì„ )",
        "korean_name": "Draft Fast",
        "weights": {
            "cost": 0.2,
            "latency": 0.5,  # ì†ë„ ìš°ì„ 
            "quality": 0.2,
            "resource": 0.05,
            "privacy": 0.05
        },
        "preferred_models": [
            "gemini-2.5-flash",
            "gpt-4o-mini",
            "pi",
            "claude-3.5-haiku",
            "mistral-7b"
        ]
    },

    "balanced": {
        "description": "ê· í˜•ì¡íŒ í’ˆì§ˆê³¼ ì†ë„",
        "korean_name": "Balanced",
        "weights": {
            "cost": 0.25,
            "latency": 0.25,
            "quality": 0.25,
            "resource": 0.15,
            "privacy": 0.1
        },
        "preferred_models": [
            "gpt-4o",
            "claude-3.5-sonnet",
            "gemini-2.5-pro",
            "llama-3.1-70b"
        ]
    },

    "high_fidelity": {
        "description": "ìµœê³  í’ˆì§ˆ (í’ˆì§ˆ ìš°ì„ )",
        "korean_name": "High-Fidelity",
        "weights": {
            "cost": 0.1,
            "latency": 0.1,
            "quality": 0.6,  # í’ˆì§ˆ ìš°ì„ 
            "resource": 0.1,
            "privacy": 0.1
        },
        "preferred_models": [
            "gpt-5",
            "gpt-4.1",
            "gpt-4-turbo",
            "claude-3.5-sonnet"
        ]
    },

    "privacy_first": {
        "description": "ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš© (í”„ë¼ì´ë²„ì‹œ ìš°ì„ )",
        "korean_name": "Privacy First",
        "weights": {
            "cost": 0.1,
            "latency": 0.1,
            "quality": 0.2,
            "resource": 0.1,
            "privacy": 0.5  # í”„ë¼ì´ë²„ì‹œ ìš°ì„ 
        },
        "preferred_models": [
            "llama-3.1-70b",
            "llama-3.1-8b",
            "qwen2-14b",
            "mistral-7b"
        ]
    },

    "cost_optimized": {
        "description": "ë¹„ìš© ìµœì†Œí™”",
        "korean_name": "Cost Optimized",
        "weights": {
            "cost": 0.6,  # ë¹„ìš© ìš°ì„ 
            "latency": 0.15,
            "quality": 0.15,
            "resource": 0.05,
            "privacy": 0.05
        },
        "preferred_models": [
            "mistral-7b",
            "llama-3.1-8b",
            "gemini-2.5-flash",
            "claude-3.5-haiku"
        ]
    }
}
```

---

## 6. Task Classification

### 6.1 Task Types

```python
class TaskClassifier:
    """
    ì‘ì—… ìœ í˜• ë¶„ë¥˜ê¸°
    """

    TASK_TYPES = {
        "reasoning": {
            "keywords": ["analyze", "solve", "explain", "understand"],
            "complexity": "high",
            "token_estimate": "high",
            "quality_requirement": "critical"
        },
        "creative": {
            "keywords": ["create", "generate", "imagine", "design"],
            "complexity": "medium",
            "token_estimate": "medium",
            "quality_requirement": "high"
        },
        "summarization": {
            "keywords": ["summarize", "brief", "outline", "highlight"],
            "complexity": "low",
            "token_estimate": "low",
            "quality_requirement": "medium"
        },
        "translation": {
            "keywords": ["translate", "convert", "localize"],
            "complexity": "medium",
            "token_estimate": "medium",
            "quality_requirement": "high"
        },
        "extraction": {
            "keywords": ["extract", "find", "identify", "parse"],
            "complexity": "low",
            "token_estimate": "low",
            "quality_requirement": "medium"
        },
        "conversation": {
            "keywords": ["chat", "discuss", "talk", "respond"],
            "complexity": "low",
            "token_estimate": "low",
            "quality_requirement": "medium"
        },
        "coding": {
            "keywords": ["code", "program", "implement", "debug"],
            "complexity": "high",
            "token_estimate": "high",
            "quality_requirement": "critical"
        }
    }

    def classify(self, task_description: str) -> TaskType:
        """ì‘ì—… ìœ í˜• ìë™ ë¶„ë¥˜"""
        lower_desc = task_description.lower()

        scores = {}
        for task_type, config in self.TASK_TYPES.items():
            score = sum(
                1 for keyword in config["keywords"]
                if keyword in lower_desc
            )
            scores[task_type] = score

        # Return task with highest score
        best_type = max(scores, key=scores.get)

        return TaskType(
            name=best_type,
            config=self.TASK_TYPES[best_type]
        )
```

### 6.2 Context Requirements

```python
CONTEXT_REQUIREMENTS = {
    "brand_analysis": {
        "required_context": ["brand_kit", "industry", "target_audience"],
        "optimal_models": ["gpt-4o", "claude-3.5-sonnet"],
        "fallback_models": ["llama-3.1-70b"]
    },
    "marketing_brief": {
        "required_context": ["campaign_goal", "budget", "timeline"],
        "optimal_models": ["claude-3.5-sonnet", "gpt-4-turbo"],
        "fallback_models": ["gemini-2.5-pro"]
    },
    "product_description": {
        "required_context": ["product_info", "target_market", "competitors"],
        "optimal_models": ["gpt-4o", "claude-3.5-sonnet"],
        "fallback_models": ["qwen2-14b"]
    },
    "social_media": {
        "required_context": ["platform", "audience", "tone"],
        "optimal_models": ["gemini-2.5-flash", "gpt-4o-mini"],
        "fallback_models": ["llama-3.1-8b"]
    },
    "presentation": {
        "required_context": ["topic", "audience_level", "duration"],
        "optimal_models": ["gpt-4-turbo", "claude-3.5-sonnet"],
        "fallback_models": ["llama-3.1-70b"]
    }
}
```

---

## 7. Cost Management

README.mdì— ì •ì˜ëœ ë¹„ìš© ê²½ë³´ ì‹œìŠ¤í…œì„ í¬í•¨í•œ ì¢…í•© ë¹„ìš© ê´€ë¦¬:

### 7.1 ë¹„ìš© ê²½ë³´ ì‹œìŠ¤í…œ (Cost Alert System)

ì˜ìƒÂ·ëŒ€ìš©ëŸ‰ ì‘ì—… ì‹œ ì˜ˆìƒ ë¹„ìš©/ì‹œê°„ íŒì—… ê³ ì§€ ë° ì‚¬ìš©ì ìŠ¹ì¸:

```python
class CostAlertSystem:
    """
    ë¹„ìš© ê²½ë³´ ì‹œìŠ¤í…œ
    """

    def __init__(self):
        self.thresholds = {
            'warning': 1.0,   # $1 ì´ìƒ
            'approval': 5.0,  # $5 ì´ìƒ ìŠ¹ì¸ í•„ìš”
            'critical': 20.0  # $20 ì´ìƒ critical ê²½ê³ 
        }

    async def check_cost_alert(
        self,
        task: Task,
        selected_model: Model
    ) -> CostAlertResult:
        """ë¹„ìš© ê²½ë³´ í™•ì¸"""

        # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
        estimated_cost = self.estimate_cost(task, selected_model)
        estimated_time = self.estimate_time(task, selected_model)

        # ê²½ë³´ ìˆ˜ì¤€ ê²°ì •
        if estimated_cost >= self.thresholds['critical']:
            return CostAlertResult(
                level='critical',
                cost=estimated_cost,
                time=estimated_time,
                requires_approval=True,
                message=f"âš ï¸ ë†’ì€ ë¹„ìš© ì˜ˆìƒ: ${estimated_cost:.2f}\n"
                        f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time}ì´ˆ\n"
                        f"ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
            )
        elif estimated_cost >= self.thresholds['approval']:
            return CostAlertResult(
                level='approval',
                cost=estimated_cost,
                time=estimated_time,
                requires_approval=True,
                message=f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f}\n"
                        f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time}ì´ˆ\n"
                        f"ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
            )
        elif estimated_cost >= self.thresholds['warning']:
            return CostAlertResult(
                level='warning',
                cost=estimated_cost,
                time=estimated_time,
                requires_approval=False,
                message=f"â„¹ï¸ ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f}"
            )

        return CostAlertResult(level='none', cost=estimated_cost)

    def estimate_cost(self, task: Task, model: Model) -> float:
        """ë¹„ìš© ì˜ˆì¸¡"""

        if task.type == 'video_generation':
            # ì˜ìƒ ìƒì„±: ì´ˆë‹¹ ë¹„ìš©
            duration_seconds = task.params.get('duration', 30)
            return model.cost_per_second * duration_seconds

        elif task.type == 'image_generation':
            # ì´ë¯¸ì§€ ìƒì„±: ì´ë¯¸ì§€ë‹¹ ë¹„ìš©
            num_images = task.params.get('num_images', 1)
            return model.cost_per_image * num_images

        else:
            # í…ìŠ¤íŠ¸ ìƒì„±: í† í°ë‹¹ ë¹„ìš©
            estimated_tokens = task.estimated_tokens
            return model.cost_per_1k * (estimated_tokens / 1000)

    def estimate_time(self, task: Task, model: Model) -> int:
        """ì‹œê°„ ì˜ˆì¸¡ (ì´ˆ)"""

        if task.type == 'video_generation':
            duration = task.params.get('duration', 30)
            return int(duration * model.time_per_second)

        elif task.type == 'image_generation':
            num_images = task.params.get('num_images', 1)
            return int(num_images * model.time_per_image)

        else:
            return int(task.estimated_tokens / model.tokens_per_second)
```

### 7.2 ì‹¤ì‹œê°„ ë¹„ìš© ëŒ€ì‹œë³´ë“œ

```python
class CostDashboard:
    """
    ì‹¤ì‹œê°„ ë¹„ìš© ëŒ€ì‹œë³´ë“œ
    """

    async def get_realtime_stats(
        self,
        user_id: str
    ) -> DashboardStats:
        """ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ"""

        now = datetime.utcnow()
        today_start = now.replace(hour=0, minute=0, second=0)

        return DashboardStats(
            # ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰
            today={
                'total_cost': await self.get_cost(user_id, today_start, now),
                'total_requests': await self.get_request_count(user_id, today_start, now),
                'by_model': await self.get_cost_by_model(user_id, today_start, now)
            },

            # ì´ë²ˆ ë‹¬ ì‚¬ìš©ëŸ‰
            this_month={
                'total_cost': await self.get_monthly_cost(user_id),
                'budget_remaining': await self.get_budget_remaining(user_id),
                'trend': await self.get_cost_trend(user_id)
            },

            # í•œë„ ì •ë³´
            limits={
                'daily_limit': 100.0,
                'daily_used': await self.get_daily_usage(user_id),
                'daily_remaining': await self.get_daily_remaining(user_id),
                'warning_level': await self.get_warning_level(user_id)
            },

            # ìµœê·¼ ì‘ì—…
            recent_tasks=await self.get_recent_tasks(user_id, limit=10)
        )

    async def get_warning_level(self, user_id: str) -> str:
        """ê²½ê³  ìˆ˜ì¤€ ì¡°íšŒ"""
        usage = await self.get_daily_usage(user_id)
        limit = 100.0

        usage_percentage = (usage / limit) * 100

        if usage_percentage >= 90:
            return 'critical'
        elif usage_percentage >= 70:
            return 'warning'
        elif usage_percentage >= 50:
            return 'caution'
        else:
            return 'normal'
```

### 7.3 Budget Controls

```python
class BudgetManager:
    """
    ì˜ˆì‚° ê´€ë¦¬ì
    """

    def __init__(self):
        self.daily_limit = 100.0  # $100 per day
        self.hourly_limit = 10.0  # $10 per hour
        self.per_user_limit = 5.0  # $5 per user per day

    async def check_budget(
        self,
        user_id: str,
        estimated_cost: float
    ) -> BudgetCheckResult:
        """ì˜ˆì‚° í™•ì¸"""

        # Get current usage
        daily_usage = await self.get_daily_usage()
        hourly_usage = await self.get_hourly_usage()
        user_usage = await self.get_user_usage(user_id)

        # Check limits
        checks = {
            "daily": daily_usage + estimated_cost <= self.daily_limit,
            "hourly": hourly_usage + estimated_cost <= self.hourly_limit,
            "user": user_usage + estimated_cost <= self.per_user_limit
        }

        if all(checks.values()):
            return BudgetCheckResult(
                approved=True,
                remaining_budget=min(
                    self.daily_limit - daily_usage,
                    self.hourly_limit - hourly_usage,
                    self.per_user_limit - user_usage
                )
            )

        return BudgetCheckResult(
            approved=False,
            reason=self.get_rejection_reason(checks),
            alternatives=await self.suggest_alternatives(estimated_cost)
        )

    async def suggest_alternatives(
        self,
        original_cost: float
    ) -> List[Alternative]:
        """ì €ë ´í•œ ëŒ€ì•ˆ ì œì‹œ"""
        alternatives = []

        # Suggest cheaper models
        if original_cost > 0.1:
            alternatives.append(Alternative(
                suggestion="Use Gemini Flash instead",
                estimated_cost=original_cost * 0.1,
                quality_impact="Minor reduction in quality"
            ))

        # Suggest local models
        if self.check_local_availability():
            alternatives.append(Alternative(
                suggestion="Use local Llama model",
                estimated_cost=original_cost * 0.01,
                quality_impact="Moderate reduction, longer processing"
            ))

        # Suggest batching
        alternatives.append(Alternative(
            suggestion="Batch with other requests",
            estimated_cost=original_cost * 0.7,
            quality_impact="Increased latency"
        ))

        return alternatives
```

### 7.4 Cost Tracking

```python
class CostTracker:
    """
    ë¹„ìš© ì¶”ì ê¸°
    """

    def __init__(self):
        self.db = CostDatabase()
        self.alerts = AlertSystem()

    async def track_usage(
        self,
        request_id: str,
        user_id: str,
        model: str,
        tokens: int,
        cost: float
    ):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""

        # Record in database
        await self.db.insert({
            "request_id": request_id,
            "user_id": user_id,
            "model": model,
            "tokens": tokens,
            "cost": cost,
            "timestamp": datetime.utcnow()
        })

        # Check for anomalies
        if await self.is_anomaly(user_id, cost):
            await self.alerts.send(
                level="warning",
                message=f"Unusual cost spike for user {user_id}: ${cost}"
            )

        # Update aggregates
        await self.update_aggregates(user_id, model, cost)

    async def get_usage_report(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> UsageReport:
        """ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ìƒì„±"""

        data = await self.db.query_range(start_date, end_date)

        return UsageReport(
            total_cost=sum(d["cost"] for d in data),
            total_tokens=sum(d["tokens"] for d in data),
            by_model=self.aggregate_by_model(data),
            by_user=self.aggregate_by_user(data),
            by_day=self.aggregate_by_day(data),
            top_users=self.get_top_users(data, limit=10),
            cost_trend=self.calculate_trend(data)
        )
```

---

## 8. Performance Optimization

### 8.1 Caching Strategy

```python
class ModelResponseCache:
    """
    ëª¨ë¸ ì‘ë‹µ ìºì‹±
    """

    def __init__(self):
        self.redis = Redis()
        self.ttl = {
            "reasoning": 3600,      # 1 hour
            "creative": 1800,       # 30 minutes
            "summarization": 7200,  # 2 hours
            "translation": 86400,   # 24 hours
            "extraction": 7200,     # 2 hours
        }

    async def get(
        self,
        prompt_hash: str,
        model: str
    ) -> Optional[CachedResponse]:
        """ìºì‹œ ì¡°íšŒ"""
        key = f"llm:cache:{model}:{prompt_hash}"
        cached = await self.redis.get(key)

        if cached:
            return CachedResponse(
                content=cached["content"],
                cached_at=cached["timestamp"],
                model=model,
                hit=True
            )

        return None

    async def set(
        self,
        prompt_hash: str,
        model: str,
        response: str,
        task_type: str
    ):
        """ìºì‹œ ì €ì¥"""
        key = f"llm:cache:{model}:{prompt_hash}"
        ttl = self.ttl.get(task_type, 1800)

        await self.redis.set(
            key,
            {
                "content": response,
                "timestamp": datetime.utcnow().isoformat(),
                "task_type": task_type
            },
            ex=ttl
        )

    def should_cache(self, task: Task) -> bool:
        """ìºì‹± ì—¬ë¶€ ê²°ì •"""
        # Don't cache personalized content
        if task.is_personalized:
            return False

        # Don't cache time-sensitive content
        if task.is_time_sensitive:
            return False

        # Don't cache if contains PII
        if task.contains_pii:
            return False

        return True
```

### 8.2 Batching Strategy

```python
class RequestBatcher:
    """
    ìš”ì²­ ë°°ì¹­
    """

    def __init__(self):
        self.batch_window = 100  # milliseconds
        self.max_batch_size = 10
        self.pending = defaultdict(list)

    async def add_request(
        self,
        request: ModelRequest
    ) -> ModelResponse:
        """ë°°ì¹˜ì— ìš”ì²­ ì¶”ê°€"""

        # Check if batching is beneficial
        if not self.should_batch(request):
            return await self.process_single(request)

        # Add to pending batch
        batch_key = self.get_batch_key(request)
        future = asyncio.Future()

        self.pending[batch_key].append({
            "request": request,
            "future": future
        })

        # Start batch timer if first in batch
        if len(self.pending[batch_key]) == 1:
            asyncio.create_task(
                self.process_batch_after_delay(batch_key)
            )

        # Process immediately if batch is full
        if len(self.pending[batch_key]) >= self.max_batch_size:
            await self.process_batch(batch_key)

        return await future

    async def process_batch(self, batch_key: str):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        batch = self.pending.pop(batch_key, [])
        if not batch:
            return

        # Combine requests
        combined_request = self.combine_requests(
            [item["request"] for item in batch]
        )

        # Process combined request
        try:
            response = await self.send_to_model(combined_request)
            responses = self.split_response(response, len(batch))

            # Resolve futures
            for item, resp in zip(batch, responses):
                item["future"].set_result(resp)

        except Exception as e:
            # Reject all futures
            for item in batch:
                item["future"].set_exception(e)
```

---

## 9. Fallback & Error Handling

### 9.1 Fallback Chain

```python
class FallbackChain:
    """
    í´ë°± ì²´ì¸ ê´€ë¦¬
    """

    FALLBACK_CHAINS = {
        "gpt-4-turbo": ["gpt-4o", "claude-3.5-sonnet", "llama-3.1-70b"],
        "claude-3.5-sonnet": ["gpt-4o", "gemini-2.5-pro", "llama-3.1-70b"],
        "gemini-2.5-flash": ["gpt-4o-mini", "claude-3.5-haiku", "mistral-7b"],
        "dall-e-3": ["dall-e-2", "sdxl", "sd-1.5"],
        "sora2": ["runway-gen3", "pika-labs"]
    }

    async def execute_with_fallback(
        self,
        primary_model: str,
        request: ModelRequest
    ) -> ModelResponse:
        """í´ë°± ì²´ì¸ ì‹¤í–‰"""

        chain = [primary_model] + self.FALLBACK_CHAINS.get(primary_model, [])

        for model in chain:
            try:
                # Check availability
                if not await self.check_availability(model):
                    continue

                # Try model
                response = await self.execute_model(model, request)

                # Log fallback usage
                if model != primary_model:
                    await self.log_fallback(primary_model, model, "success")

                return response

            except ModelError as e:
                # Log error
                await self.log_error(model, e)

                # Check if retryable
                if not self.is_retryable(e):
                    raise

                # Continue to next model
                continue

        # All models failed
        raise AllModelsFailed(chain)

    def is_retryable(self, error: ModelError) -> bool:
        """ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        non_retryable = [
            "invalid_api_key",
            "insufficient_quota",
            "content_policy_violation"
        ]
        return error.code not in non_retryable
```

### 9.2 Circuit Breaker

```python
class CircuitBreaker:
    """
    ì„œí‚· ë¸Œë ˆì´ì»¤
    """

    def __init__(self):
        self.failure_threshold = 5
        self.success_threshold = 2
        self.timeout = 60  # seconds
        self.states = {}  # model -> state

    async def call(
        self,
        model: str,
        func: Callable,
        *args,
        **kwargs
    ):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í˜¸ì¶œ"""

        state = self.get_state(model)

        if state == "open":
            # Check if timeout passed
            if self.should_attempt_reset(model):
                state = "half_open"
            else:
                raise CircuitOpenError(model)

        try:
            result = await func(*args, **kwargs)
            self.on_success(model)
            return result

        except Exception as e:
            self.on_failure(model)
            raise

    def get_state(self, model: str) -> str:
        """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        if model not in self.states:
            self.states[model] = {
                "state": "closed",
                "failures": 0,
                "successes": 0,
                "last_failure": None
            }
        return self.states[model]["state"]

    def on_success(self, model: str):
        """ì„±ê³µ ì²˜ë¦¬"""
        state = self.states[model]
        state["failures"] = 0
        state["successes"] += 1

        if state["state"] == "half_open":
            if state["successes"] >= self.success_threshold:
                state["state"] = "closed"
                state["successes"] = 0

    def on_failure(self, model: str):
        """ì‹¤íŒ¨ ì²˜ë¦¬"""
        state = self.states[model]
        state["failures"] += 1
        state["successes"] = 0
        state["last_failure"] = time.time()

        if state["failures"] >= self.failure_threshold:
            state["state"] = "open"
```

---

## 10. Monitoring & Analytics

### 10.1 Metrics Collection

```python
class RouterMetrics:
    """
    ë¼ìš°í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """

    def __init__(self):
        self.prometheus = PrometheusClient()

    # Counter metrics
    model_selections = Counter(
        'router_model_selections_total',
        'Total model selections',
        ['model', 'task_type', 'preset']
    )

    # Histogram metrics
    selection_latency = Histogram(
        'router_selection_latency_seconds',
        'Model selection latency',
        ['task_type']
    )

    model_latency = Histogram(
        'model_response_latency_seconds',
        'Model response latency',
        ['model', 'task_type']
    )

    # Gauge metrics
    model_cost_rate = Gauge(
        'model_cost_dollars_per_hour',
        'Current cost rate',
        ['model']
    )

    cache_hit_rate = Gauge(
        'router_cache_hit_rate',
        'Cache hit rate',
        ['task_type']
    )

    # Summary metrics
    quality_scores = Summary(
        'model_quality_scores',
        'Quality scores by model',
        ['model', 'task_type']
    )

    async def record_selection(
        self,
        model: str,
        task_type: str,
        preset: str,
        latency: float
    ):
        """ëª¨ë¸ ì„ íƒ ê¸°ë¡"""
        self.model_selections.labels(
            model=model,
            task_type=task_type,
            preset=preset
        ).inc()

        self.selection_latency.labels(
            task_type=task_type
        ).observe(latency)

    async def record_response(
        self,
        model: str,
        task_type: str,
        latency: float,
        tokens: int,
        cost: float
    ):
        """ì‘ë‹µ ê¸°ë¡"""
        self.model_latency.labels(
            model=model,
            task_type=task_type
        ).observe(latency)

        # Update cost rate
        current_rate = await self.calculate_cost_rate(model)
        self.model_cost_rate.labels(model=model).set(current_rate)
```

### 10.2 Analytics Dashboard

```python
class RouterAnalytics:
    """
    ë¼ìš°í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ
    """

    async def get_dashboard_data(
        self,
        time_range: str = "24h"
    ) -> DashboardData:
        """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""

        return DashboardData(
            # Model usage distribution
            model_usage=await self.get_model_usage(time_range),

            # Cost breakdown
            cost_breakdown=await self.get_cost_breakdown(time_range),

            # Performance metrics
            performance={
                "avg_latency": await self.get_avg_latency(time_range),
                "p95_latency": await self.get_p95_latency(time_range),
                "error_rate": await self.get_error_rate(time_range)
            },

            # Quality metrics
            quality={
                "avg_quality_score": await self.get_avg_quality(time_range),
                "user_satisfaction": await self.get_satisfaction(time_range)
            },

            # Efficiency metrics
            efficiency={
                "cache_hit_rate": await self.get_cache_hit_rate(time_range),
                "fallback_rate": await self.get_fallback_rate(time_range),
                "batch_efficiency": await self.get_batch_efficiency(time_range)
            },

            # Top users
            top_users=await self.get_top_users(time_range, limit=10),

            # Anomalies
            anomalies=await self.detect_anomalies(time_range)
        )

    async def generate_optimization_report(self) -> OptimizationReport:
        """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""

        return OptimizationReport(
            recommendations=[
                {
                    "title": "Increase cache TTL for translations",
                    "impact": "Reduce costs by 15%",
                    "implementation": "Update cache TTL from 1h to 24h"
                },
                {
                    "title": "Use local models for simple tasks",
                    "impact": "Reduce costs by 30%",
                    "implementation": "Route summarization tasks to Llama-8B"
                }
            ],
            potential_savings=await self.calculate_potential_savings(),
            performance_improvements=await self.identify_bottlenecks()
        )
```

---

## 11. Configuration Management

### 11.1 Router Configuration

```yaml
# router_config.yaml
router:
  version: "1.0"
  mode: "auto"  # auto | manual | preset

  # Weight configuration
  weights:
    cost: 0.3
    latency: 0.25
    quality: 0.25
    resource: 0.1
    privacy: 0.1

  # Model registry
  models:
    - name: "gpt-4-turbo"
      enabled: true
      endpoint: "${OPENAI_API_ENDPOINT}"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 128000
      timeout: 30

    - name: "llama-3.1-70b"
      enabled: true
      type: "local"
      model_path: "/models/llama-70b.gguf"
      gpu_layers: 40
      context_size: 8192

  # Budget limits
  budget:
    daily_limit: 100.0
    hourly_limit: 10.0
    per_user_daily: 5.0
    warning_threshold: 0.8

  # Cache configuration
  cache:
    enabled: true
    ttl:
      default: 1800
      reasoning: 3600
      creative: 1800
      summarization: 7200
      translation: 86400

  # Fallback configuration
  fallback:
    enabled: true
    max_attempts: 3
    timeout: 5

  # Monitoring
  monitoring:
    metrics_enabled: true
    metrics_port: 9090
    log_level: "INFO"
    trace_sampling: 0.1
```

### 11.2 Dynamic Configuration

```python
class DynamicConfig:
    """
    ë™ì  ì„¤ì • ê´€ë¦¬
    """

    def __init__(self):
        self.config_source = ConfigSource()
        self.update_interval = 60  # seconds
        self.callbacks = []

    async def start(self):
        """ì„¤ì • ì—…ë°ì´íŠ¸ ì‹œì‘"""
        while True:
            try:
                new_config = await self.config_source.fetch()
                if self.has_changed(new_config):
                    await self.apply_config(new_config)
                    await self.notify_callbacks(new_config)
            except Exception as e:
                logger.error(f"Config update failed: {e}")

            await asyncio.sleep(self.update_interval)

    async def apply_config(self, config: dict):
        """ì„¤ì • ì ìš©"""
        # Update weights
        if "weights" in config:
            RouterScoreCalculator.weights = config["weights"]

        # Update limits
        if "budget" in config:
            BudgetManager.daily_limit = config["budget"]["daily_limit"]

        # Update model states
        if "models" in config:
            for model_config in config["models"]:
                await self.update_model_state(model_config)

    def register_callback(self, callback: Callable):
        """ì„¤ì • ë³€ê²½ ì½œë°± ë“±ë¡"""
        self.callbacks.append(callback)

    async def notify_callbacks(self, new_config: dict):
        """ì½œë°± ì•Œë¦¼"""
        for callback in self.callbacks:
            try:
                await callback(new_config)
            except Exception as e:
                logger.error(f"Callback failed: {e}")
```

---

## 12. Testing & Validation

### 12.1 Router Testing

```python
import pytest
from unittest.mock import Mock, patch

class TestSmartRouter:

    @pytest.fixture
    def router(self):
        return SmartRouter()

    @pytest.mark.asyncio
    async def test_cost_optimized_selection(self, router):
        """ë¹„ìš© ìµœì í™” ì„ íƒ í…ŒìŠ¤íŠ¸"""
        task = Task(
            type="summarization",
            estimated_tokens=1000,
            priority="low",
            contains_pii=False
        )

        # Mock model availability
        with patch.object(router, 'check_availability', return_value=True):
            selected = await router.select_model(task, mode='preset:cost_optimized')

            assert selected.model.name in ["gemini-2.5-flash", "mistral-7b"]
            assert selected.score > 0.7

    @pytest.mark.asyncio
    async def test_privacy_constraint(self, router):
        """í”„ë¼ì´ë²„ì‹œ ì œì•½ í…ŒìŠ¤íŠ¸"""
        task = Task(
            type="analysis",
            estimated_tokens=2000,
            contains_pii=True,
            confidential=True
        )

        selected = await router.select_model(task)

        # Should select local model
        assert selected.model.type == "local"
        assert selected.model.name in ["llama-3.1-70b", "llama-3.1-8b", "qwen2-14b"]

    @pytest.mark.asyncio
    async def test_fallback_chain(self, router):
        """í´ë°± ì²´ì¸ í…ŒìŠ¤íŠ¸"""
        task = Task(type="reasoning", estimated_tokens=5000)

        # Mock primary model failure
        with patch.object(router, 'execute_model') as mock_execute:
            mock_execute.side_effect = [
                ModelError("rate_limit"),  # Primary fails
                ModelResponse(content="Success")  # Fallback succeeds
            ]

            response = await router.execute_with_fallback("gpt-4-turbo", task)

            assert response.content == "Success"
            assert mock_execute.call_count == 2

    @pytest.mark.asyncio
    async def test_budget_enforcement(self, router):
        """ì˜ˆì‚° ì œí•œ í…ŒìŠ¤íŠ¸"""
        task = Task(
            type="creative",
            estimated_tokens=100000  # Very large
        )

        # Mock budget exceeded
        with patch.object(BudgetManager, 'check_budget') as mock_budget:
            mock_budget.return_value = BudgetCheckResult(
                approved=False,
                reason="Daily limit exceeded"
            )

            with pytest.raises(BudgetExceededError):
                await router.select_model(task)
```

### 12.2 Performance Testing

```python
class PerformanceTest:
    """
    ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    """

    async def test_selection_latency(self):
        """ì„ íƒ ì§€ì—°ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        router = SmartRouter()
        latencies = []

        for _ in range(100):
            task = self.generate_random_task()
            start = time.time()
            await router.select_model(task)
            latencies.append(time.time() - start)

        assert np.percentile(latencies, 50) < 0.1  # P50 < 100ms
        assert np.percentile(latencies, 95) < 0.5  # P95 < 500ms

    async def test_concurrent_routing(self):
        """ë™ì‹œ ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸"""
        router = SmartRouter()
        tasks = [self.generate_random_task() for _ in range(100)]

        start = time.time()
        results = await asyncio.gather(*[
            router.select_model(task) for task in tasks
        ])
        duration = time.time() - start

        assert len(results) == 100
        assert duration < 5.0  # Should handle 100 requests in < 5s
```

---

## 13. Migration & Rollout

### 13.1 Rollout Strategy

```python
class RouterRollout:
    """
    ë¼ìš°í„° ë°°í¬ ì „ëµ
    """

    def __init__(self):
        self.phases = [
            {
                "name": "Phase 1: Shadow Mode",
                "duration": "1 week",
                "traffic": "0%",
                "description": "Log decisions without routing"
            },
            {
                "name": "Phase 2: Canary",
                "duration": "1 week",
                "traffic": "10%",
                "description": "Route 10% of traffic"
            },
            {
                "name": "Phase 3: Progressive",
                "duration": "2 weeks",
                "traffic": "10% -> 50%",
                "description": "Gradual increase"
            },
            {
                "name": "Phase 4: Full Rollout",
                "duration": "Ongoing",
                "traffic": "100%",
                "description": "Complete migration"
            }
        ]

    async def should_use_new_router(
        self,
        user_id: str,
        phase: int
    ) -> bool:
        """ìƒˆ ë¼ìš°í„° ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""

        if phase == 1:
            # Shadow mode: always use old, log new
            await self.log_shadow_decision(user_id)
            return False

        traffic_percentage = self.get_traffic_percentage(phase)
        user_hash = hash(user_id) % 100

        return user_hash < traffic_percentage
```

### 13.2 Rollback Plan

```python
class RollbackManager:
    """
    ë¡¤ë°± ê´€ë¦¬
    """

    async def check_health_metrics(self) -> HealthStatus:
        """í—¬ìŠ¤ ë©”íŠ¸ë¦­ í™•ì¸"""
        metrics = await self.collect_metrics()

        if metrics.error_rate > 0.05:  # > 5% errors
            return HealthStatus.UNHEALTHY

        if metrics.p95_latency > 5.0:  # > 5 seconds
            return HealthStatus.DEGRADED

        if metrics.cost_spike > 2.0:  # 2x cost increase
            return HealthStatus.WARNING

        return HealthStatus.HEALTHY

    async def automatic_rollback(self):
        """ìë™ ë¡¤ë°±"""
        health = await self.check_health_metrics()

        if health == HealthStatus.UNHEALTHY:
            logger.error("Unhealthy metrics detected, rolling back")
            await self.rollback()
            await self.alert_team("Automatic rollback triggered")

    async def rollback(self):
        """ë¡¤ë°± ì‹¤í–‰"""
        # Switch to previous router version
        await self.switch_router_version("previous")

        # Clear caches
        await self.clear_caches()

        # Reset configurations
        await self.reset_configs()

        logger.info("Rollback completed")
```

---

## 14. Appendix

### 14.1 Model Comparison Matrix

| Capability | GPT-4 | Claude 3.5 | Gemini 2.5 | Llama 3.1 |
|------------|-------|------------|------------|-----------|
| Reasoning | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| Creativity | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| Speed | â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­ |
| Cost | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| Context | 128K | 200K | 1M | 128K |
| Multi-modal | âœ… | âŒ | âœ… | âŒ |
| Code | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |

### 14.2 Cost Calculation Examples

```python
# Example: Blog post generation (2000 words)
# Estimated tokens: 3000 input + 2500 output = 5500 total

costs = {
    "gpt-4-turbo": 5.5 * 0.01 = 0.055,     # $0.055
    "gpt-4o": 5.5 * 0.005 = 0.0275,        # $0.0275
    "claude-3.5-sonnet": 5.5 * 0.003 = 0.0165,  # $0.0165
    "gemini-2.5-flash": 5.5 * 0.0003 = 0.00165, # $0.00165
    "llama-3.1-70b": 5.5 * 0.0001 = 0.00055,    # $0.00055 (local)
}
```

### 14.3 References

- [OpenAI Pricing](https://openai.com/pricing)
- [Anthropic Claude Pricing](https://anthropic.com/pricing)
- [Google AI Pricing](https://ai.google.dev/pricing)
- [Local LLM Benchmarks](https://github.com/eugeneyan/open-llms)

### 14.4 Change Log

| Date | Version | Changes | Author |
|------|---------|---------|--------|
| 2025-01-13 | 1.0 | Initial policy | AI Team |
| 2025-11-13 (ëª©) | 1.1 | Multi-Node Infrastructure, Agent Integration, Cost Alert System ì¶”ê°€ | AI Team |
