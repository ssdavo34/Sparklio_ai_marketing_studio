# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê³„íšì„œ

> **ë²„ì „**: 2.0
> **ë‚ ì§œ**: 2025-01-14
> **ìƒíƒœ**: ìµœì¢… (TrendPipeline í†µí•©)
> **ë‹´ë‹¹**: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒ€

---

## 1. ê°œìš”

Sparklio.aiì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì€ **TrendPipeline** ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ë§ˆì¼€íŒ… ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ì„ë² ë”©í•˜ì—¬ RAG(Retrieval Augmented Generation) ì‹œìŠ¤í…œê³¼ ìê°€í•™ìŠµ ì—”ì§„ì— í™œìš©í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ë¸Œëœë“œ ë¶„ì„, ë§ˆì¼€íŒ… ë¸Œë¦¬í”„, ê´‘ê³ Â·SNS ìƒì„±, íŠ¸ë Œë“œ ë¶„ì„ ë“± ëª¨ë“  ê¸°ëŠ¥ì˜ ë°ì´í„° ì¸í”„ë¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 1.1 í•µì‹¬ ëª©í‘œ
- **ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›**: ì›¹ í¬ë¡¤ë§, SNS, íŒŒì¼ ì—…ë¡œë“œ, API í†µí•©
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ì—…ë¡œë“œ ì¦‰ì‹œ ì²˜ë¦¬ (< 10ì´ˆ)
- **ìë™ í•™ìŠµ**: ë¸Œëœë“œ/íŠ¸ë Œë“œ ë°ì´í„° ìë™ ìˆ˜ì§‘ ë° ì—…ë°ì´íŠ¸
- **ë†’ì€ ì •í™•ë„**: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì •í™•ë„ > 95%
- **í™•ì¥ì„±**: ì¼ì¼ 10,000ê°œ ë¬¸ì„œ + 100,000ê°œ ì›¹í˜ì´ì§€ ì²˜ë¦¬

### 1.2 TrendPipeline êµ¬ì¡°
```
ìˆ˜ì§‘(Collector) â†’ ì •ì œ(Cleaner) â†’ íŒŒì‹±(Parser) â†’ ì²­í‚¹(Chunker) â†’ ì„ë² ë”©(Embedder) â†’ ì €ì¥(Ingestor) â†’ ê²€ì¦(Reviewer)
                                                                                              â†“
                                                                           RAG Engine â† ìê°€í•™ìŠµ ì—”ì§„
```

---

## 2. TrendPipeline ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ í”Œë¡œìš°

```mermaid
graph TB
    subgraph "Data Sources"
        WEB[ì›¹ í¬ë¡¤ë§]
        SNS[SNS API]
        FILE[íŒŒì¼ ì—…ë¡œë“œ]
        COMP[ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§]
    end

    subgraph "TrendPipeline"
        COLL[Collector<br/>ë°ì´í„° ìˆ˜ì§‘]
        CLEAN[Cleaner<br/>ì •ì œ/ì „ì²˜ë¦¬]
        PARSE[Parser<br/>êµ¬ì¡° ë¶„ì„]
        CHUNK[Chunker<br/>ë¬¸ì„œ ë¶„í• ]
        EMBED[Embedder<br/>ì„ë² ë”© ìƒì„±]
        INGEST[Ingestor<br/>DB ì €ì¥]
        REVIEW[Reviewer<br/>í’ˆì§ˆ ê²€ì¦]
    end

    subgraph "Storage"
        PG[(PostgreSQL<br/>+pgvector)]
        REDIS[(Redis<br/>ìºì‹œ)]
        S3[(S3/MinIO<br/>ë¯¸ë””ì–´)]
    end

    subgraph "Intelligence"
        RAG[RAG Engine]
        LEARN[Self-Learning<br/>Engine]
        BRAND[Brand Model]
    end

    WEB --> COLL
    SNS --> COLL
    FILE --> COLL
    COMP --> COLL

    COLL --> CLEAN
    CLEAN --> PARSE
    PARSE --> CHUNK
    CHUNK --> EMBED
    EMBED --> INGEST
    INGEST --> REVIEW

    INGEST --> PG
    INGEST --> REDIS
    INGEST --> S3

    PG --> RAG
    RAG --> LEARN
    LEARN --> BRAND
    BRAND -.-> EMBED
```

### 2.2 ì»´í¬ë„ŒíŠ¸ ìƒì„¸ êµ¬ì¡°

```python
class TrendPipeline:
    """
    TrendPipeline ë©”ì¸ í´ë˜ìŠ¤ - ë°ì´í„° ìˆ˜ì§‘ë¶€í„° RAGê¹Œì§€ ì „ì²´ í”Œë¡œìš° ê´€ë¦¬
    """

    def __init__(self):
        # íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.collector = DataCollector()      # ë©€í‹°ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        self.cleaner = DataCleaner()         # ë…¸ì´ì¦ˆ ì œê±° ë° ì •ì œ
        self.parser = UniversalParser()      # ë‹¤ì–‘í•œ í˜•ì‹ íŒŒì‹±
        self.chunker = SmartChunker()        # ì§€ëŠ¥í˜• ì²­í‚¹
        self.embedder = EmbeddingGenerator() # ë²¡í„° ì„ë² ë”© ìƒì„±
        self.ingestor = VectorIngestor()     # DB ì €ì¥ ë° ì¸ë±ì‹±
        self.reviewer = QualityReviewer()    # í’ˆì§ˆ ê²€ì¦
        self.rag = RAGEngine()              # ê²€ìƒ‰ ì¦ê°• ìƒì„±
        self.learner = SelfLearningEngine() # ìê°€í•™ìŠµ ì—”ì§„

    async def process_batch(self, sources: List[DataSource]) -> PipelineResult:
        """
        ë°°ì¹˜ ì²˜ë¦¬ - ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        """
        results = []

        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for source in sources:
            task = asyncio.create_task(self.process_single(source))
            tasks.append(task)

        # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ìê°€í•™ìŠµ ì—”ì§„ ì—…ë°ì´íŠ¸
        await self.learner.update_from_batch(results)

        return PipelineResult(
            success_count=len([r for r in results if r.success]),
            total_count=len(results),
            results=results
        )
```

---

## 3. Collector (ë°ì´í„° ìˆ˜ì§‘ê¸°) ìƒì„¸ ìŠ¤í™

### 3.1 ìˆ˜ì§‘ ëŒ€ìƒ ë°ì´í„°

#### ğŸ“Š **ë§ˆì¼€íŒ… ë°ì´í„° 12ì¢…**

| ì¹´í…Œê³ ë¦¬ | ë°ì´í„° ìœ í˜• | ìˆ˜ì§‘ ì†ŒìŠ¤ | ìš°ì„ ìˆœìœ„ |
|---------|------------|----------|----------|
| **ë¸Œëœë“œ** | ë¡œê³ , ì»¬ëŸ¬, í°íŠ¸, í‚¤ì›Œë“œ | ì›¹ì‚¬ì´íŠ¸, SNS | P0 |
| **ìƒí’ˆ/ì„œë¹„ìŠ¤** | ìƒì„¸í˜ì´ì§€, ìŠ¤í™, FAQ | ì´ì»¤ë¨¸ìŠ¤, ê³µì‹ ì‚¬ì´íŠ¸ | P0 |
| **ì‹œê° ìŠ¤íƒ€ì¼** | ì´ë¯¸ì§€, ë°°ë„ˆ, ì˜ìƒ | SNS, ê´‘ê³  í”Œë«í¼ | P0 |
| **ê´‘ê³  ì¹´í”¼** | í—¤ë“œë¼ì¸, CTA, ë³¸ë¬¸ | Google Ads, ë„¤ì´ë²„ | P1 |
| **ì„±ê³¼ ë°ì´í„°** | CTR, CPC, ROAS | ê´‘ê³  API, Analytics | P1 |
| **SNS ì½˜í…ì¸ ** | í¬ìŠ¤íŠ¸, ë¦´ìŠ¤, í•´ì‹œíƒœê·¸ | Instagram, TikTok | P0 |
| **ê²½ìŸì‚¬ ë¶„ì„** | ê²½ìŸì‚¬ ê´‘ê³ , í†¤ì•¤ë§¤ë„ˆ | ì›¹ í¬ë¡¤ë§, API | P1 |
| **íŠ¸ë Œë“œ** | ê²€ìƒ‰ í‚¤ì›Œë“œ, ì¸ê¸° ì½˜í…ì¸  | Google Trends, ë„¤ì´ë²„ | P1 |
| **ê³ ê° í”¼ë“œë°±** | ë¦¬ë·°, ëŒ“ê¸€, Q&A | ì‡¼í•‘ëª°, SNS | P2 |
| **ê°€ê²© ì •ë³´** | ì‹œì¥ ê°€ê²©, í• ì¸ ì •ë³´ | ê°€ê²©ë¹„êµ ì‚¬ì´íŠ¸ | P2 |
| **ê³„ì ˆì„± ë°ì´í„°** | ì‹œì¦Œ íŠ¸ë Œë“œ, ì´ë²¤íŠ¸ | ìº˜ë¦°ë”, ê²€ìƒ‰ ë°ì´í„° | P2 |
| **ì—…ì¢… íŠ¹í™”** | ì—…ì¢…ë³„ ì„ í˜¸ íŒ¨í„´ | ì—…ì¢… ë¦¬í¬íŠ¸, í†µê³„ | P2 |

### 3.2 ìˆ˜ì§‘ ê¸°ìˆ  ìŠ¤íƒ

```python
class DataCollector:
    """
    ë©€í‹°ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°
    """

    def __init__(self):
        # ì›¹ í¬ë¡¤ë§ ë„êµ¬
        self.playwright = PlaywrightCrawler()   # ë™ì  í˜ì´ì§€ í¬ë¡¤ë§
        self.scrapy = ScrapyEngine()           # ëŒ€ê·œëª¨ ì •ì  í¬ë¡¤ë§
        self.selenium = SeleniumDriver()       # ë³µì¡í•œ ì¸í„°ë™ì…˜ í•„ìš”ì‹œ

        # API í´ë¼ì´ì–¸íŠ¸
        self.apis = {
            'google': GoogleSearchAPI(),        # ê²€ìƒ‰ ê²°ê³¼
            'naver': NaverAPI(),                # ì‡¼í•‘/ë¸”ë¡œê·¸
            'instagram': InstagramGraphAPI(),   # ì¸ìŠ¤íƒ€ê·¸ë¨ ë°ì´í„°
            'tiktok': TikTokAPI(),              # í‹±í†¡ íŠ¸ë Œë“œ
            'youtube': YouTubeDataAPI(),        # ìœ íŠœë¸Œ íŠ¸ë Œë“œ
            'twitter': TwitterAPI(),            # X(íŠ¸ìœ„í„°) íŠ¸ë Œë“œ
            'serp': SerpAPI(),                 # SERP ë°ì´í„°
            'ads': {
                'google': GoogleAdsAPI(),
                'meta': MetaAdsAPI(),
                'naver': NaverAdsAPI()
            }
        }

        # íŒŒì¼ ì²˜ë¦¬
        self.file_processors = {
            'pdf': PDFProcessor(),
            'ppt': PPTProcessor(),
            'excel': ExcelProcessor(),
            'image': ImageProcessor(),
            'video': VideoProcessor()
        }

    async def collect_web_data(self, url: str, depth: int = 2) -> WebData:
        """
        ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ - ë¸Œëœë“œ ì‚¬ì´íŠ¸, ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§
        """
        # robots.txt í™•ì¸
        if not await self.check_robots_txt(url):
            return WebData(error="Robots.txt ì°¨ë‹¨")

        # Playwrightë¡œ ë™ì  ì½˜í…ì¸  ìˆ˜ì§‘
        pages = []
        async with self.playwright.new_context() as context:
            page = await context.new_page()

            # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            await page.goto(url)
            content = await page.content()

            # ìŠ¤í¬ë¦°ìƒ· (ì‹œê° ìŠ¤íƒ€ì¼ ë¶„ì„ìš©)
            screenshot = await page.screenshot(full_page=True)

            # ë§í¬ ì¶”ì¶œ ë° depth í¬ë¡¤ë§
            if depth > 0:
                links = await self.extract_links(page)
                for link in links[:10]:  # ìµœëŒ€ 10ê°œ ì„œë¸Œí˜ì´ì§€
                    subpage_data = await self.collect_web_data(link, depth-1)
                    pages.append(subpage_data)

        return WebData(
            url=url,
            content=content,
            screenshot=screenshot,
            subpages=pages,
            timestamp=datetime.utcnow()
        )

    async def collect_sns_trends(self, platform: str, keywords: List[str]) -> SNSData:
        """
        SNS íŠ¸ë Œë“œ ìˆ˜ì§‘ - ì¸ê¸° ì½˜í…ì¸ , í•´ì‹œíƒœê·¸, ë°˜ì‘
        """
        api_client = self.apis.get(platform)
        if not api_client:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {platform}")

        trends = []
        for keyword in keywords:
            # í‚¤ì›Œë“œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            posts = await api_client.search_posts(
                query=keyword,
                limit=100,
                sort='engagement'  # ë°˜ì‘ ë†’ì€ ìˆœ
            )

            # ê° í¬ìŠ¤íŠ¸ ë¶„ì„
            for post in posts:
                trend_data = {
                    'platform': platform,
                    'keyword': keyword,
                    'content': post.text,
                    'media': post.media_urls,
                    'hashtags': post.hashtags,
                    'engagement': {
                        'likes': post.likes,
                        'comments': post.comments,
                        'shares': post.shares,
                        'views': post.views
                    },
                    'author': post.author,
                    'created_at': post.created_at
                }
                trends.append(trend_data)

        return SNSData(
            platform=platform,
            trends=trends,
            top_hashtags=self.extract_top_hashtags(trends),
            engagement_patterns=self.analyze_engagement(trends)
        )

    async def collect_competitor_data(self, competitors: List[str]) -> CompetitorData:
        """
        ê²½ìŸì‚¬ ë°ì´í„° ìˆ˜ì§‘ - ê´‘ê³ , ì½˜í…ì¸ , ì „ëµ ë¶„ì„
        """
        competitor_insights = []

        for competitor in competitors:
            # ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
            website_data = await self.collect_web_data(competitor['website'])

            # SNS í”„ë¡œí•„ ìˆ˜ì§‘
            sns_profiles = {}
            for platform in ['instagram', 'facebook', 'tiktok']:
                if competitor.get(f'{platform}_handle'):
                    profile = await self.apis[platform].get_profile(
                        competitor[f'{platform}_handle']
                    )
                    sns_profiles[platform] = profile

            # ê´‘ê³  ë°ì´í„° ìˆ˜ì§‘ (Google Ads Transparency ë“±)
            ads_data = await self.collect_ads_data(competitor['domain'])

            competitor_insights.append({
                'name': competitor['name'],
                'website': website_data,
                'sns': sns_profiles,
                'ads': ads_data,
                'brand_style': await self.analyze_brand_style(website_data, sns_profiles)
            })

        return CompetitorData(insights=competitor_insights)
```

### 3.3 í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§

```python
class CrawlingScheduler:
    """
    ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
    """

    def __init__(self):
        self.scheduler = APScheduler()
        self.tasks = []

    def setup_schedules(self):
        """
        í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ ì„¤ì •
        """
        # ë§¤ì¼: íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘
        self.scheduler.add_job(
            func=self.collect_daily_trends,
            trigger='cron',
            hour=6,  # ë§¤ì¼ ì˜¤ì „ 6ì‹œ
            id='daily_trends'
        )

        # ë§¤ì£¼: ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§
        self.scheduler.add_job(
            func=self.monitor_competitors,
            trigger='cron',
            day_of_week='mon',  # ë§¤ì£¼ ì›”ìš”ì¼
            hour=9,
            id='weekly_competitors'
        )

        # ì‹¤ì‹œê°„: SNS ëª¨ë‹ˆí„°ë§ (15ë¶„ë§ˆë‹¤)
        self.scheduler.add_job(
            func=self.monitor_sns_realtime,
            trigger='interval',
            minutes=15,
            id='realtime_sns'
        )

        # ì›”ê°„: ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸
        self.scheduler.add_job(
            func=self.analyze_market_trends,
            trigger='cron',
            day=1,  # ë§¤ì›” 1ì¼
            hour=0,
            id='monthly_market'
        )
```

---

## 4. Cleaner (ë°ì´í„° ì •ì œ) ìƒì„¸ ìŠ¤í™

### 4.1 ì •ì œ í”„ë¡œì„¸ìŠ¤

```python
class DataCleaner:
    """
    ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬
    """

    def __init__(self):
        self.rules = {
            'remove_html': True,              # HTML íƒœê·¸ ì œê±°
            'remove_ads': True,               # ê´‘ê³ ì„± ì½˜í…ì¸  ì œê±°
            'normalize_text': True,           # í…ìŠ¤íŠ¸ ì •ê·œí™”
            'remove_duplicates': True,        # ì¤‘ë³µ ì œê±°
            'fix_encoding': True,             # ì¸ì½”ë”© ìˆ˜ì •
            'mask_pii': True,                # ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
            'remove_noise': True,            # ë…¸ì´ì¦ˆ ì œê±°
            'extract_key_sentences': True    # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        }

        # ê´‘ê³ ì„± í‚¤ì›Œë“œ í•„í„°
        self.ad_keywords = [
            'í´ë¦­', 'êµ¬ë§¤í•˜ê¸°', 'í• ì¸', 'ë¬´ë£Œë°°ì†¡', 'í•œì •íŒë§¤',
            'ì§€ê¸ˆ ë°”ë¡œ', 'ë†“ì¹˜ì§€ ë§ˆì„¸ìš”', 'ë‹¨ë… íŠ¹ê°€'
        ]

        # ë…¸ì´ì¦ˆ íŒ¨í„´
        self.noise_patterns = [
            r'[^\w\sê°€-í£]',  # íŠ¹ìˆ˜ë¬¸ì ê³¼ë‹¤
            r'(.)\1{5,}',     # ë°˜ë³µ ë¬¸ì
            r'\s{3,}',        # ê³¼ë„í•œ ê³µë°±
        ]

    async def clean(self, data: RawData) -> CleanedData:
        """
        ë°ì´í„° ì •ì œ ë©”ì¸ í”„ë¡œì„¸ìŠ¤
        """
        # 1. HTML ë° ë§ˆí¬ì—… ì œê±°
        if self.rules['remove_html']:
            data.text = self.remove_html_tags(data.text)

        # 2. ê´‘ê³ ì„± ì½˜í…ì¸  í•„í„°ë§
        if self.rules['remove_ads']:
            data.text = self.filter_ad_content(data.text)

        # 3. í…ìŠ¤íŠ¸ ì •ê·œí™”
        if self.rules['normalize_text']:
            data.text = self.normalize_text(data.text)

        # 4. ì¤‘ë³µ ì œê±° (ë¬¸ì¥ ë‹¨ìœ„)
        if self.rules['remove_duplicates']:
            data.text = self.remove_duplicate_sentences(data.text)

        # 5. ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
        if self.rules['mask_pii']:
            data.text = await self.mask_personal_info(data.text)

        # 6. ë…¸ì´ì¦ˆ ì œê±°
        if self.rules['remove_noise']:
            data.text = self.remove_noise(data.text)

        # 7. í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ìš”ì•½)
        if self.rules['extract_key_sentences']:
            data.key_sentences = await self.extract_key_sentences(data.text)

        # 8. í’ˆì§ˆ ê²€ì¦
        quality_score = self.assess_quality(data.text)

        return CleanedData(
            text=data.text,
            key_sentences=data.key_sentences,
            metadata=data.metadata,
            quality_score=quality_score,
            cleaned_at=datetime.utcnow()
        )

    def filter_ad_content(self, text: str) -> str:
        """
        ê´‘ê³ ì„± ì½˜í…ì¸  í•„í„°ë§
        """
        lines = text.split('\n')
        filtered_lines = []

        for line in lines:
            # ê´‘ê³  í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°
            ad_score = sum(1 for keyword in self.ad_keywords if keyword in line)

            # ì„ê³„ê°’ ì´í•˜ë§Œ ìœ ì§€
            if ad_score < 3:
                filtered_lines.append(line)

        return '\n'.join(filtered_lines)

    async def extract_key_sentences(self, text: str) -> List[str]:
        """
        í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (TextRank ì•Œê³ ë¦¬ì¦˜)
        """
        sentences = self.split_sentences(text)

        if len(sentences) < 3:
            return sentences

        # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
        embeddings = await self.generate_sentence_embeddings(sentences)

        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)

        # TextRank ì ìˆ˜ ê³„ì‚°
        scores = self.textrank(similarity_matrix)

        # ìƒìœ„ 30% ë¬¸ì¥ ì„ íƒ
        top_k = max(3, len(sentences) // 3)
        top_indices = np.argsort(scores)[-top_k:]

        return [sentences[i] for i in sorted(top_indices)]
```

---

## 5. Parser (êµ¬ì¡° ë¶„ì„) ìƒì„¸ ìŠ¤í™

### 5.1 í†µí•© íŒŒì„œ

```python
class UniversalParser:
    """
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„° íŒŒì‹±
    """

    def __init__(self):
        self.parsers = {
            'html': HTMLParser(),
            'pdf': PDFParser(),
            'pptx': PPTParser(),
            'xlsx': ExcelParser(),
            'docx': WordParser(),
            'image': ImageParser(),
            'video': VideoParser(),
            'json': JSONParser(),
            'csv': CSVParser()
        }

    async def parse(self, data: CleanedData) -> ParsedData:
        """
        ë°ì´í„° íƒ€ì…ë³„ íŒŒì‹±
        """
        # íŒŒì¼ íƒ€ì… ê°ì§€
        file_type = self.detect_type(data)

        # ì ì ˆí•œ íŒŒì„œ ì„ íƒ
        parser = self.parsers.get(file_type)
        if not parser:
            raise UnsupportedFormatError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {file_type}")

        # íŒŒì‹± ì‹¤í–‰
        parsed = await parser.parse(data)

        # êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
        structure = self.extract_structure(parsed)

        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        metadata = self.enrich_metadata(parsed, structure)

        return ParsedData(
            content=parsed.content,
            structure=structure,
            metadata=metadata,
            tables=parsed.tables,
            images=parsed.images,
            links=parsed.links
        )

    def extract_structure(self, parsed: dict) -> DocumentStructure:
        """
        ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        """
        return DocumentStructure(
            sections=self.identify_sections(parsed),
            hierarchy=self.build_hierarchy(parsed),
            toc=self.generate_toc(parsed),
            summary=self.generate_summary(parsed)
        )
```

### 5.2 íŠ¹ìˆ˜ íŒŒì„œ: ë§ˆì¼€íŒ… ë°ì´í„°

```python
class MarketingDataParser:
    """
    ë§ˆì¼€íŒ… íŠ¹í™” ë°ì´í„° íŒŒì‹±
    """

    async def parse_brand_data(self, website_content: str) -> BrandData:
        """
        ë¸Œëœë“œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ
        """
        soup = BeautifulSoup(website_content, 'html.parser')

        # ë¡œê³  ì¶”ì¶œ
        logo = self.extract_logo(soup)

        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì¶”ì¶œ
        colors = self.extract_brand_colors(soup)

        # í°íŠ¸ ì •ë³´ ì¶”ì¶œ
        fonts = self.extract_fonts(soup)

        # í‚¤ ë©”ì‹œì§€ ì¶”ì¶œ
        messages = self.extract_key_messages(soup)

        # í†¤ì•¤ë§¤ë„ˆ ë¶„ì„
        tone = await self.analyze_tone(soup.get_text())

        return BrandData(
            logo=logo,
            colors=colors,
            fonts=fonts,
            key_messages=messages,
            tone_and_manner=tone
        )

    async def parse_ad_performance(self, ad_data: dict) -> AdPerformance:
        """
        ê´‘ê³  ì„±ê³¼ ë°ì´í„° íŒŒì‹±
        """
        return AdPerformance(
            impressions=ad_data.get('impressions', 0),
            clicks=ad_data.get('clicks', 0),
            ctr=ad_data.get('ctr', 0.0),
            cpc=ad_data.get('cpc', 0.0),
            conversions=ad_data.get('conversions', 0),
            conversion_rate=ad_data.get('conversion_rate', 0.0),
            cost=ad_data.get('cost', 0.0),
            roas=ad_data.get('roas', 0.0),
            campaign_info={
                'name': ad_data.get('campaign_name'),
                'type': ad_data.get('campaign_type'),
                'objective': ad_data.get('objective'),
                'targeting': ad_data.get('targeting'),
                'creatives': ad_data.get('creatives', [])
            }
        )

    def extract_brand_colors(self, soup: BeautifulSoup) -> List[str]:
        """
        ì›¹í˜ì´ì§€ì—ì„œ ë¸Œëœë“œ ì»¬ëŸ¬ ì¶”ì¶œ
        """
        colors = set()

        # CSSì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ
        styles = soup.find_all('style')
        for style in styles:
            # HEX ìƒ‰ìƒ íŒ¨í„´
            hex_colors = re.findall(r'#[0-9a-fA-F]{3,6}', style.string or '')
            colors.update(hex_colors)

            # RGB ìƒ‰ìƒ íŒ¨í„´
            rgb_colors = re.findall(r'rgb\([^)]+\)', style.string or '')
            colors.update(rgb_colors)

        # ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ
        elements = soup.find_all(style=True)
        for elem in elements:
            style_str = elem.get('style', '')
            hex_colors = re.findall(r'#[0-9a-fA-F]{3,6}', style_str)
            colors.update(hex_colors)

        # ë¹ˆë„ ê¸°ë°˜ ì£¼ìš” ìƒ‰ìƒ ì„ íƒ
        return self.select_primary_colors(list(colors))
```

---

## 6. Chunker (ì²­í‚¹ ì „ëµ) ìƒì„¸ ìŠ¤í™

### 6.1 ìŠ¤ë§ˆíŠ¸ ì²­í‚¹

```python
class SmartChunker:
    """
    ì§€ëŠ¥í˜• ë¬¸ì„œ ì²­í‚¹
    """

    def __init__(self):
        self.strategies = {
            'fixed_size': FixedSizeChunker(),      # ê³ ì • í¬ê¸°
            'sentence': SentenceChunker(),         # ë¬¸ì¥ ë‹¨ìœ„
            'paragraph': ParagraphChunker(),       # ë‹¨ë½ ë‹¨ìœ„
            'semantic': SemanticChunker(),         # ì˜ë¯¸ ë‹¨ìœ„
            'recursive': RecursiveChunker(),       # ì¬ê·€ì  ë¶„í• 
            'document': DocumentChunker(),         # ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜
            'marketing': MarketingChunker()        # ë§ˆì¼€íŒ… íŠ¹í™”
        }

        self.config = {
            'chunk_size': 512,          # í† í° ìˆ˜
            'chunk_overlap': 128,       # ì˜¤ë²„ë© í† í° ìˆ˜
            'min_chunk_size': 100,      # ìµœì†Œ ì²­í¬ í¬ê¸°
            'max_chunk_size': 1024,     # ìµœëŒ€ ì²­í¬ í¬ê¸°
        }

    async def chunk(self, data: ParsedData, strategy: str = 'auto') -> List[Chunk]:
        """
        ë°ì´í„° ì²­í‚¹ ì‹¤í–‰
        """
        # ì „ëµ ìë™ ì„ íƒ
        if strategy == 'auto':
            strategy = self.select_strategy(data)

        # ì„ íƒëœ ì „ëµìœ¼ë¡œ ì²­í‚¹
        chunker = self.strategies[strategy]
        chunks = await chunker.chunk(data, self.config)

        # ì²­í¬ í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”
        optimized_chunks = await self.optimize_chunks(chunks)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        final_chunks = []
        for i, chunk_data in enumerate(optimized_chunks):
            chunk = Chunk(
                id=self.generate_chunk_id(),
                text=chunk_data['text'],
                position=i,
                token_count=self.count_tokens(chunk_data['text']),
                metadata={
                    'strategy': strategy,
                    'source_type': data.metadata.get('type'),
                    'brand_id': data.metadata.get('brand_id'),
                    'chunk_index': i,
                    'total_chunks': len(optimized_chunks),
                    'semantic_tags': chunk_data.get('tags', [])
                }
            )
            final_chunks.append(chunk)

        return final_chunks

    def select_strategy(self, data: ParsedData) -> str:
        """
        ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ ì„ íƒ
        """
        # ë§ˆì¼€íŒ… ë°ì´í„°ì¸ ê²½ìš°
        if data.metadata.get('type') in ['ad', 'sns', 'brand']:
            return 'marketing'

        # êµ¬ì¡°í™”ëœ ë¬¸ì„œì¸ ê²½ìš°
        if data.structure and data.structure.sections:
            return 'document'

        # ëŒ€í™”í˜• í…ìŠ¤íŠ¸ì¸ ê²½ìš°
        if self.is_conversational(data.content):
            return 'sentence'

        # ê¸´ ë¬¸ì„œì¸ ê²½ìš°
        if len(data.content) > 10000:
            return 'semantic'

        # ê¸°ë³¸
        return 'paragraph'
```

### 6.2 ë§ˆì¼€íŒ… íŠ¹í™” ì²­í‚¹

```python
class MarketingChunker:
    """
    ë§ˆì¼€íŒ… ì½˜í…ì¸  íŠ¹í™” ì²­í‚¹ ì „ëµ
    """

    async def chunk(self, data: ParsedData, config: dict) -> List[dict]:
        """
        ë§ˆì¼€íŒ… ë°ì´í„°ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
        """
        chunks = []

        # ê´‘ê³  ì¹´í”¼ëŠ” ìº í˜ì¸ ë‹¨ìœ„ë¡œ
        if data.metadata.get('type') == 'ad':
            chunks = self.chunk_by_campaign(data)

        # SNS ì½˜í…ì¸ ëŠ” í¬ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ
        elif data.metadata.get('type') == 'sns':
            chunks = self.chunk_by_post(data)

        # ë¸Œëœë“œ ë°ì´í„°ëŠ” ì¹´í…Œê³ ë¦¬ë³„ë¡œ
        elif data.metadata.get('type') == 'brand':
            chunks = self.chunk_by_brand_element(data)

        # ìƒí’ˆ ì •ë³´ëŠ” ì„¹ì…˜ë³„ë¡œ
        elif data.metadata.get('type') == 'product':
            chunks = self.chunk_by_product_section(data)

        else:
            # ê¸°ë³¸ ì²­í‚¹
            chunks = self.default_chunk(data, config)

        return chunks

    def chunk_by_campaign(self, data: ParsedData) -> List[dict]:
        """
        ê´‘ê³  ìº í˜ì¸ ë‹¨ìœ„ ì²­í‚¹
        """
        chunks = []

        for campaign in data.campaigns:
            chunk = {
                'text': f"""
                ìº í˜ì¸: {campaign['name']}
                ëª©í‘œ: {campaign['objective']}
                í—¤ë“œë¼ì¸: {campaign['headline']}
                ë³¸ë¬¸: {campaign['body']}
                CTA: {campaign['cta']}
                """,
                'tags': ['campaign', campaign['objective']],
                'metadata': {
                    'campaign_id': campaign['id'],
                    'performance': campaign.get('performance', {})
                }
            }
            chunks.append(chunk)

        return chunks
```

---

## 7. Embedder (ì„ë² ë”© ìƒì„±) ìƒì„¸ ìŠ¤í™

### 7.1 ë©€í‹°ëª¨ë¸ ì„ë² ë”©

```python
class EmbeddingGenerator:
    """
    ì„ë² ë”© ìƒì„± ê´€ë¦¬
    """

    def __init__(self):
        # ì„ë² ë”© ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
        self.models = {
            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            'text-embedding-3-large': OpenAIEmbeddings(
                model='text-embedding-3-large',
                dimensions=3072
            ),
            'text-embedding-ada-002': OpenAIEmbeddings(
                model='text-embedding-ada-002',
                dimensions=1536
            ),
            'bge-large': HuggingFaceEmbeddings(
                model_name='BAAI/bge-large-en-v1.5',
                dimensions=1024
            ),
            'e5-large': HuggingFaceEmbeddings(
                model_name='intfloat/e5-large-v2',
                dimensions=1024
            ),
            'llama-embed': LocalLlamaEmbeddings(
                model_path='/models/llama3.1-8b',
                dimensions=4096
            ),

            # ì´ë¯¸ì§€ ì„ë² ë”©
            'clip': CLIPEmbeddings(
                model='ViT-L/14',
                dimensions=768
            ),
            'blip': BLIPEmbeddings(
                model='blip-large',
                dimensions=768
            )
        }

        # ê¸°ë³¸ ì„¤ì •
        self.default_text_model = 'text-embedding-3-large'
        self.default_image_model = 'clip'
        self.batch_size = 100

        # ìºì‹œ
        self.cache = EmbeddingCache()

    async def generate(
        self,
        chunks: List[Chunk],
        model: str = None,
        use_cache: bool = True
    ) -> List[EmbeddedChunk]:
        """
        ì²­í¬ ì„ë² ë”© ìƒì„±
        """
        model = model or self.default_text_model
        embedder = self.models[model]

        embedded_chunks = []

        # ë°°ì¹˜ ì²˜ë¦¬
        for batch_start in range(0, len(chunks), self.batch_size):
            batch = chunks[batch_start:batch_start + self.batch_size]

            # ìºì‹œ í™•ì¸
            if use_cache:
                cached_embeddings = await self.get_cached_embeddings(batch, model)
                uncached_batch = [
                    chunk for chunk, emb in zip(batch, cached_embeddings)
                    if emb is None
                ]
            else:
                uncached_batch = batch
                cached_embeddings = [None] * len(batch)

            # ìºì‹œë˜ì§€ ì•Šì€ ì²­í¬ë§Œ ì„ë² ë”© ìƒì„±
            if uncached_batch:
                new_embeddings = await self.generate_batch(uncached_batch, embedder)

                # ìºì‹œ ì €ì¥
                if use_cache:
                    await self.cache_embeddings(uncached_batch, new_embeddings, model)
            else:
                new_embeddings = []

            # ê²°ê³¼ ë³‘í•©
            new_emb_iter = iter(new_embeddings)
            for chunk, cached_emb in zip(batch, cached_embeddings):
                if cached_emb is not None:
                    embedding = cached_emb
                else:
                    embedding = next(new_emb_iter)

                embedded_chunk = EmbeddedChunk(
                    chunk_id=chunk.id,
                    text=chunk.text,
                    embedding=embedding,
                    model=model,
                    metadata={
                        **chunk.metadata,
                        'embedded_at': datetime.utcnow().isoformat(),
                        'embedding_model': model,
                        'embedding_dim': len(embedding)
                    }
                )
                embedded_chunks.append(embedded_chunk)

        # í’ˆì§ˆ ê²€ì¦
        await self.validate_embeddings(embedded_chunks)

        return embedded_chunks

    async def generate_hybrid_embedding(
        self,
        text: str,
        metadata: dict,
        image: Optional[bytes] = None
    ) -> np.ndarray:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© (í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° + ì´ë¯¸ì§€)
        """
        embeddings = []

        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        text_emb = await self.generate_text_embedding(text)
        embeddings.append(text_emb)

        # ë©”íƒ€ë°ì´í„° ì„ë² ë”©
        if metadata:
            meta_emb = await self.generate_metadata_embedding(metadata)
            embeddings.append(meta_emb * 0.3)  # ê°€ì¤‘ì¹˜ ì ìš©

        # ì´ë¯¸ì§€ ì„ë² ë”©
        if image:
            image_emb = await self.generate_image_embedding(image)
            embeddings.append(image_emb * 0.5)  # ê°€ì¤‘ì¹˜ ì ìš©

        # ê²°í•© ë° ì •ê·œí™”
        combined = np.concatenate(embeddings)
        normalized = combined / np.linalg.norm(combined)

        return normalized

    async def generate_metadata_embedding(self, metadata: dict) -> np.ndarray:
        """
        ë©”íƒ€ë°ì´í„°ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        """
        # ë©”íƒ€ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        meta_text = self.metadata_to_text(metadata)

        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        embedding = await self.generate_text_embedding(meta_text)

        return embedding

    def metadata_to_text(self, metadata: dict) -> str:
        """
        ë©”íƒ€ë°ì´í„°ë¥¼ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        parts = []

        if 'brand' in metadata:
            parts.append(f"ë¸Œëœë“œ: {metadata['brand']}")

        if 'category' in metadata:
            parts.append(f"ì¹´í…Œê³ ë¦¬: {metadata['category']}")

        if 'tags' in metadata:
            parts.append(f"íƒœê·¸: {', '.join(metadata['tags'])}")

        if 'performance' in metadata:
            perf = metadata['performance']
            if 'ctr' in perf:
                parts.append(f"CTR: {perf['ctr']:.2%}")
            if 'roas' in perf:
                parts.append(f"ROAS: {perf['roas']:.2f}")

        return ' '.join(parts)
```

---

## 8. Ingestor (ì €ì¥ ë° ì¸ë±ì‹±) ìƒì„¸ ìŠ¤í™

### 8.1 ë²¡í„° DB ì €ì¥

```python
class VectorIngestor:
    """
    ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê´€ë¦¬
    """

    def __init__(self):
        # PostgreSQL + pgvector
        self.pg_pool = asyncpg.create_pool(
            host='localhost',
            database='sparklio',
            user='sparklio',
            password=os.getenv('DB_PASSWORD')
        )

        # Redis ìºì‹œ
        self.redis = aioredis.create_redis_pool(
            'redis://localhost:6379'
        )

        # S3/MinIO ê°ì²´ ìŠ¤í† ë¦¬ì§€
        self.s3 = aioboto3.Session().client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id=os.getenv('MINIO_ACCESS_KEY'),
            aws_secret_access_key=os.getenv('MINIO_SECRET_KEY')
        )

    async def ingest(
        self,
        embedded_chunks: List[EmbeddedChunk],
        source_metadata: dict
    ) -> IngestResult:
        """
        ì„ë² ë”© ë°ì´í„° ì €ì¥
        """
        async with self.pg_pool.acquire() as conn:
            async with conn.transaction():
                # 1. ì†ŒìŠ¤ ë¬¸ì„œ ì €ì¥
                source_id = await self.save_source(conn, source_metadata)

                # 2. ë²¡í„° ì €ì¥
                chunk_ids = []
                for chunk in embedded_chunks:
                    chunk_id = await self.save_vector(
                        conn,
                        chunk,
                        source_id
                    )
                    chunk_ids.append(chunk_id)

                # 3. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                await self.update_indices(conn, chunk_ids)

                # 4. ë¸Œëœë“œ í”„ë¡œí•„ ì—…ë°ì´íŠ¸
                if source_metadata.get('brand_id'):
                    await self.update_brand_profile(
                        conn,
                        source_metadata['brand_id'],
                        embedded_chunks
                    )

                # 5. íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸
                await self.update_trend_insights(
                    conn,
                    embedded_chunks,
                    source_metadata
                )

        # 6. ìºì‹œ ë¬´íš¨í™”
        await self.invalidate_cache(source_metadata.get('brand_id'))

        # 7. ë¯¸ë””ì–´ íŒŒì¼ ì €ì¥ (S3/MinIO)
        if source_metadata.get('media_files'):
            await self.save_media_files(
                source_metadata['media_files'],
                source_id
            )

        return IngestResult(
            success=True,
            source_id=source_id,
            chunk_count=len(chunk_ids),
            chunk_ids=chunk_ids,
            ingested_at=datetime.utcnow()
        )

    async def save_vector(
        self,
        conn,
        chunk: EmbeddedChunk,
        source_id: str
    ) -> str:
        """
        ë²¡í„° ë ˆì½”ë“œ ì €ì¥
        """
        query = """
        INSERT INTO vectors (
            id,
            source_id,
            chunk_text,
            embedding,
            model,
            metadata,
            created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7)
        RETURNING id
        """

        chunk_id = await conn.fetchval(
            query,
            chunk.chunk_id,
            source_id,
            chunk.text,
            chunk.embedding.tolist(),  # pgvector í˜•ì‹
            chunk.model,
            json.dumps(chunk.metadata),
            datetime.utcnow()
        )

        return chunk_id

    async def update_brand_profile(
        self,
        conn,
        brand_id: str,
        chunks: List[EmbeddedChunk]
    ):
        """
        ë¸Œëœë“œ í”„ë¡œí•„ ì—…ë°ì´íŠ¸
        """
        # ë¸Œëœë“œ ê´€ë ¨ ì²­í¬ ë¶„ì„
        brand_data = await self.analyze_brand_chunks(chunks)

        # í”„ë¡œí•„ ì—…ë°ì´íŠ¸
        query = """
        UPDATE brand_profiles
        SET
            keywords = array_cat(keywords, $2),
            tone_vectors = array_cat(tone_vectors, $3),
            style_vectors = array_cat(style_vectors, $4),
            updated_at = $5
        WHERE brand_id = $1
        """

        await conn.execute(
            query,
            brand_id,
            brand_data['keywords'],
            brand_data['tone_vectors'],
            brand_data['style_vectors'],
            datetime.utcnow()
        )
```

### 8.2 ì¸ë±ìŠ¤ ê´€ë¦¬

```python
class IndexManager:
    """
    ë²¡í„° ì¸ë±ìŠ¤ ê´€ë¦¬
    """

    async def create_indices(self, conn):
        """
        í•„ìš”í•œ ì¸ë±ìŠ¤ ìƒì„±
        """
        # IVFFlat ì¸ë±ìŠ¤ (pgvector)
        await conn.execute("""
            CREATE INDEX IF NOT EXISTS vectors_embedding_idx
            ON vectors
            USING ivfflat (embedding vector_cosine_ops)
            WITH (lists = 100);
        """)

        # HNSW ì¸ë±ìŠ¤ (ê³ ì •ë°€ ê²€ìƒ‰ìš©)
        await conn.execute("""
            CREATE INDEX IF NOT EXISTS vectors_embedding_hnsw_idx
            ON vectors
            USING hnsw (embedding vector_l2_ops)
            WITH (m = 16, ef_construction = 64);
        """)

        # ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤
        await conn.execute("""
            CREATE INDEX IF NOT EXISTS vectors_metadata_idx
            ON vectors
            USING gin (metadata);
        """)

        # ë¸Œëœë“œ ì¸ë±ìŠ¤
        await conn.execute("""
            CREATE INDEX IF NOT EXISTS vectors_brand_idx
            ON vectors ((metadata->>'brand_id'));
        """)

    async def optimize_indices(self, conn):
        """
        ì¸ë±ìŠ¤ ìµœì í™”
        """
        # í…Œì´ë¸” í¬ê¸° í™•ì¸
        row_count = await conn.fetchval(
            "SELECT COUNT(*) FROM vectors"
        )

        # IVFFlat ë¦¬ìŠ¤íŠ¸ ìˆ˜ ì¬ì¡°ì •
        optimal_lists = int(math.sqrt(row_count))

        if optimal_lists > 100:  # í˜„ì¬ ì„¤ì •ë³´ë‹¤ í¬ë©´
            await conn.execute(f"""
                DROP INDEX vectors_embedding_idx;
                CREATE INDEX vectors_embedding_idx
                ON vectors
                USING ivfflat (embedding vector_cosine_ops)
                WITH (lists = {optimal_lists});
            """)
```

---

## 9. Reviewer (í’ˆì§ˆ ê²€ì¦) ìƒì„¸ ìŠ¤í™

### 9.1 í’ˆì§ˆ ê²€ì¦ ì—”ì§„

```python
class QualityReviewer:
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    """

    def __init__(self):
        self.validators = {
            'duplicate': DuplicateValidator(),
            'quality': QualityValidator(),
            'relevance': RelevanceValidator(),
            'compliance': ComplianceValidator(),
            'brand': BrandConsistencyValidator()
        }

        self.thresholds = {
            'min_quality_score': 0.6,
            'max_duplicate_ratio': 0.3,
            'min_relevance_score': 0.7,
            'min_brand_consistency': 0.8
        }

    async def review(
        self,
        data: IngestedData
    ) -> ReviewResult:
        """
        ì¢…í•© í’ˆì§ˆ ê²€í† 
        """
        issues = []
        scores = {}

        # ê° ê²€ì¦ê¸° ì‹¤í–‰
        for name, validator in self.validators.items():
            result = await validator.validate(data)
            scores[name] = result.score

            if result.issues:
                issues.extend(result.issues)

        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = np.mean(list(scores.values()))

        # ì„ê³„ê°’ ì²´í¬
        passed = all(
            scores.get(metric, 0) >= threshold
            for metric, threshold in self.thresholds.items()
            if metric.replace('min_', '').replace('max_', '') in scores
        )

        # ë¬¸ì œê°€ ìˆìœ¼ë©´ ìë™ ìˆ˜ì • ì‹œë„
        if not passed:
            data = await self.auto_fix(data, issues)

            # ì¬ê²€ì¦
            re_review = await self.review(data)
            if re_review.passed:
                issues.append("ìë™ ìˆ˜ì • ì™„ë£Œ")

        return ReviewResult(
            passed=passed,
            overall_score=overall_score,
            scores=scores,
            issues=issues,
            reviewed_at=datetime.utcnow()
        )

    async def auto_fix(
        self,
        data: IngestedData,
        issues: List[str]
    ) -> IngestedData:
        """
        í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì •
        """
        for issue in issues:
            if "duplicate" in issue.lower():
                data = await self.remove_duplicates(data)

            elif "quality" in issue.lower():
                data = await self.improve_quality(data)

            elif "relevance" in issue.lower():
                data = await self.filter_irrelevant(data)

            elif "brand" in issue.lower():
                data = await self.fix_brand_consistency(data)

        return data
```

### 9.2 íŠ¹ìˆ˜ ê²€ì¦ê¸°

```python
class BrandConsistencyValidator:
    """
    ë¸Œëœë“œ ì¼ê´€ì„± ê²€ì¦
    """

    async def validate(self, data: IngestedData) -> ValidationResult:
        """
        ë¸Œëœë“œ í†¤ì•¤ë§¤ë„ˆ ì¼ê´€ì„± ê²€ì¦
        """
        if not data.metadata.get('brand_id'):
            return ValidationResult(score=1.0, issues=[])

        # ë¸Œëœë“œ í”„ë¡œí•„ ë¡œë“œ
        brand_profile = await self.load_brand_profile(
            data.metadata['brand_id']
        )

        issues = []
        scores = []

        # í†¤ ì¼ê´€ì„± ì²´í¬
        tone_score = await self.check_tone_consistency(
            data.text,
            brand_profile.tone
        )
        scores.append(tone_score)

        if tone_score < 0.8:
            issues.append(f"í†¤ ì¼ê´€ì„± ë¶€ì¡±: {tone_score:.2f}")

        # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì²´í¬
        keyword_score = self.check_keyword_usage(
            data.text,
            brand_profile.keywords
        )
        scores.append(keyword_score)

        # ê¸ˆì§€ì–´ ì²´í¬
        if brand_profile.forbidden_words:
            forbidden = self.check_forbidden_words(
                data.text,
                brand_profile.forbidden_words
            )
            if forbidden:
                issues.append(f"ê¸ˆì§€ì–´ ë°œê²¬: {', '.join(forbidden)}")
                scores.append(0.0)

        return ValidationResult(
            score=np.mean(scores),
            issues=issues
        )

class ComplianceValidator:
    """
    ê·œì • ì¤€ìˆ˜ ê²€ì¦
    """

    async def validate(self, data: IngestedData) -> ValidationResult:
        """
        ë²•ì /ìœ¤ë¦¬ì  ê·œì • ì¤€ìˆ˜ ê²€ì¦
        """
        issues = []

        # ê°œì¸ì •ë³´ í¬í•¨ ì—¬ë¶€
        if self.contains_pii(data.text):
            issues.append("ê°œì¸ì •ë³´ í¬í•¨")

        # ì €ì‘ê¶Œ ì¹¨í•´ ê°€ëŠ¥ì„±
        if await self.check_copyright(data.text):
            issues.append("ì €ì‘ê¶Œ ì¹¨í•´ ê°€ëŠ¥ì„±")

        # ë¶€ì ì ˆí•œ ì½˜í…ì¸ 
        if self.contains_inappropriate(data.text):
            issues.append("ë¶€ì ì ˆí•œ ì½˜í…ì¸ ")

        # ê´‘ê³  ê·œì • ìœ„ë°˜
        if data.metadata.get('type') == 'ad':
            ad_violations = self.check_ad_compliance(data.text)
            if ad_violations:
                issues.extend(ad_violations)

        score = 1.0 - (len(issues) * 0.2)  # ì´ìŠˆë‹¹ 20% ê°ì 

        return ValidationResult(
            score=max(0, score),
            issues=issues
        )
```

---

## 10. RAG Engine ìƒì„¸ ìŠ¤í™

### 10.1 ê²€ìƒ‰ ì¦ê°• ìƒì„±

```python
class RAGEngine:
    """
    ê²€ìƒ‰ ì¦ê°• ìƒì„± ì—”ì§„
    """

    def __init__(self):
        self.retriever = VectorRetriever()
        self.reranker = CrossEncoderReranker()
        self.generator = ResponseGenerator()
        self.cache = RAGCache()

    async def query(
        self,
        query: str,
        brand_id: Optional[str] = None,
        filters: Optional[dict] = None,
        top_k: int = 10
    ) -> RAGResponse:
        """
        RAG ì¿¼ë¦¬ ì‹¤í–‰
        """
        # 1. ìºì‹œ í™•ì¸
        cache_key = self.generate_cache_key(query, brand_id, filters)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached

        # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = await self.generate_query_embedding(query)

        # 3. ë²¡í„° ê²€ìƒ‰
        candidates = await self.retriever.search(
            embedding=query_embedding,
            filters=self.build_filters(brand_id, filters),
            top_k=top_k * 3  # Rerankingì„ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
        )

        # 4. ë¦¬ë­í‚¹
        reranked = await self.reranker.rerank(
            query=query,
            candidates=candidates,
            top_k=top_k
        )

        # 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(reranked)

        # 6. ë¸Œëœë“œ ì •ë³´ ì¶”ê°€
        if brand_id:
            brand_context = await self.get_brand_context(brand_id)
            context = f"{brand_context}\n\n{context}"

        # 7. ì‘ë‹µ ìƒì„±
        response = await self.generator.generate(
            query=query,
            context=context
        )

        # 8. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result = RAGResponse(
            query=query,
            response=response,
            sources=reranked,
            context=context,
            metadata={
                'brand_id': brand_id,
                'filters': filters,
                'retrieval_count': len(reranked)
            }
        )

        # 9. ìºì‹œ ì €ì¥
        await self.cache.set(cache_key, result, ttl=300)

        return result

    async def hybrid_search(
        self,
        query: str,
        image: Optional[bytes] = None,
        brand_id: Optional[str] = None
    ) -> RAGResponse:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        """
        embeddings = []

        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        if query:
            text_emb = await self.generate_query_embedding(query)
            embeddings.append(('text', text_emb, 0.7))  # ê°€ì¤‘ì¹˜

        # ì´ë¯¸ì§€ ì„ë² ë”©
        if image:
            image_emb = await self.generate_image_embedding(image)
            embeddings.append(('image', image_emb, 0.3))  # ê°€ì¤‘ì¹˜

        # ê°€ì¤‘ í‰ê·  ì„ë² ë”©
        combined_embedding = self.weighted_average_embeddings(embeddings)

        # ê²€ìƒ‰ ì‹¤í–‰
        return await self.query_with_embedding(
            embedding=combined_embedding,
            brand_id=brand_id
        )
```

### 10.2 ë¸Œëœë“œ ì¸ì‹ RAG

```python
class BrandAwareRAG:
    """
    ë¸Œëœë“œ íŠ¹í™” RAG ì‹œìŠ¤í…œ
    """

    def __init__(self):
        self.rag = RAGEngine()
        self.brand_manager = BrandManager()
        self.style_adapter = StyleAdapter()

    async def generate_brand_content(
        self,
        prompt: str,
        brand_id: str,
        content_type: str
    ) -> BrandContent:
        """
        ë¸Œëœë“œ ë§ì¶¤ ì½˜í…ì¸  ìƒì„±
        """
        # 1. ë¸Œëœë“œ í”„ë¡œí•„ ë¡œë“œ
        brand = await self.brand_manager.get_profile(brand_id)

        # 2. ê´€ë ¨ ë¸Œëœë“œ ë°ì´í„° ê²€ìƒ‰
        brand_context = await self.rag.query(
            query=prompt,
            brand_id=brand_id,
            filters={'type': 'brand_data'}
        )

        # 3. ì„±ê³µ ì‚¬ë¡€ ê²€ìƒ‰
        success_examples = await self.rag.query(
            query=f"{content_type} ê³ ì„±ê³¼ ì‚¬ë¡€",
            brand_id=brand_id,
            filters={'performance_score': {'gte': 0.8}}
        )

        # 4. ê²½ìŸì‚¬ ë¶„ì„ ë°ì´í„° ê²€ìƒ‰
        competitor_insights = await self.rag.query(
            query=f"{brand.industry} ê²½ìŸì‚¬ {content_type}",
            filters={'type': 'competitor'}
        )

        # 5. íŠ¸ë Œë“œ ë°ì´í„° ê²€ìƒ‰
        trends = await self.rag.query(
            query=f"{brand.industry} ìµœì‹  íŠ¸ë Œë“œ",
            filters={
                'type': 'trend',
                'recency': {'gte': datetime.now() - timedelta(days=30)}
            }
        )

        # 6. ì»¨í…ìŠ¤íŠ¸ í†µí•©
        integrated_context = self.integrate_contexts(
            brand=brand,
            brand_context=brand_context,
            examples=success_examples,
            competitors=competitor_insights,
            trends=trends
        )

        # 7. ë¸Œëœë“œ ìŠ¤íƒ€ì¼ ì ìš© ìƒì„±
        content = await self.style_adapter.generate(
            prompt=prompt,
            context=integrated_context,
            brand_style=brand.style_guide,
            content_type=content_type
        )

        return BrandContent(
            content=content,
            brand_id=brand_id,
            content_type=content_type,
            sources={
                'brand_data': brand_context.sources,
                'success_examples': success_examples.sources,
                'competitors': competitor_insights.sources,
                'trends': trends.sources
            }
        )
```

---

## 11. ìê°€í•™ìŠµ ì—°ë™ ì¸í„°í˜ì´ìŠ¤

### 11.1 í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘

```python
class LearningDataCollector:
    """
    ìê°€í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘
    """

    def __init__(self):
        self.pipeline = TrendPipeline()
        self.performance_tracker = PerformanceTracker()

    async def collect_feedback_data(
        self,
        content_id: str
    ) -> FeedbackData:
        """
        ìƒì„±ëœ ì½˜í…ì¸ ì˜ í”¼ë“œë°± ë°ì´í„° ìˆ˜ì§‘
        """
        # SNS ë°˜ì‘ ìˆ˜ì§‘
        sns_feedback = await self.collect_sns_metrics(content_id)

        # ê´‘ê³  ì„±ê³¼ ìˆ˜ì§‘
        ad_performance = await self.collect_ad_metrics(content_id)

        # ì‚¬ìš©ì ìˆ˜ì • ë‚´ì—­ ìˆ˜ì§‘
        user_edits = await self.collect_user_edits(content_id)

        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘
        ab_results = await self.collect_ab_test_results(content_id)

        return FeedbackData(
            content_id=content_id,
            sns=sns_feedback,
            ads=ad_performance,
            edits=user_edits,
            ab_test=ab_results,
            collected_at=datetime.utcnow()
        )

    async def update_learning_pipeline(
        self,
        feedback: FeedbackData
    ):
        """
        í”¼ë“œë°±ì„ íŒŒì´í”„ë¼ì¸ì— ë°˜ì˜
        """
        # ì„±ê³µ íŒ¨í„´ ì¶”ì¶œ
        success_patterns = await self.extract_success_patterns(feedback)

        # ì‹¤íŒ¨ íŒ¨í„´ ì¶”ì¶œ
        failure_patterns = await self.extract_failure_patterns(feedback)

        # íŒŒì´í”„ë¼ì¸ ì—…ë°ì´íŠ¸
        await self.pipeline.update_patterns(
            success=success_patterns,
            failure=failure_patterns
        )

        # ì„ë² ë”© ì¬ìƒì„± íŠ¸ë¦¬ê±°
        if self.needs_reembedding(feedback):
            await self.trigger_reembedding(feedback.content_id)
```

---

## 12. ìŠ¤ì¼€ì¤„ë§ ë° ìë™í™”

### 12.1 íŒŒì´í”„ë¼ì¸ ìŠ¤ì¼€ì¤„ëŸ¬

```python
class PipelineScheduler:
    """
    ìë™ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
    """

    def __init__(self):
        self.scheduler = APScheduler()
        self.pipeline = TrendPipeline()
        self.config = SchedulerConfig()

    def setup_schedules(self):
        """
        ìë™ ì‹¤í–‰ ìŠ¤ì¼€ì¤„ ì„¤ì •
        """
        # ì‹¤ì‹œê°„: SNS íŠ¸ë Œë“œ (15ë¶„)
        self.scheduler.add_job(
            func=self.collect_sns_trends,
            trigger='interval',
            minutes=15,
            id='sns_trends_realtime',
            args=['instagram', 'tiktok', 'twitter']
        )

        # ì‹œê°„ë³„: ê²€ìƒ‰ íŠ¸ë Œë“œ (1ì‹œê°„)
        self.scheduler.add_job(
            func=self.collect_search_trends,
            trigger='interval',
            hours=1,
            id='search_trends_hourly'
        )

        # ì¼ë³„: ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ (ë§¤ì¼ ì˜¤ì „ 6ì‹œ)
        self.scheduler.add_job(
            func=self.monitor_competitors,
            trigger='cron',
            hour=6,
            id='competitor_daily'
        )

        # ì£¼ë³„: ì‹œì¥ ë¶„ì„ (ë§¤ì£¼ ì›”ìš”ì¼)
        self.scheduler.add_job(
            func=self.analyze_market,
            trigger='cron',
            day_of_week='mon',
            hour=9,
            id='market_weekly'
        )

        # ì›”ë³„: íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ (ë§¤ì›” 1ì¼)
        self.scheduler.add_job(
            func=self.generate_trend_report,
            trigger='cron',
            day=1,
            hour=0,
            id='trend_report_monthly'
        )

    async def collect_sns_trends(self, *platforms):
        """
        SNS íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‘ì—…
        """
        for platform in platforms:
            try:
                # íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìˆ˜ì§‘
                hashtags = await self.get_trending_hashtags(platform)

                # ê° í•´ì‹œíƒœê·¸ë³„ ì½˜í…ì¸  ìˆ˜ì§‘
                for hashtag in hashtags[:20]:  # ìƒìœ„ 20ê°œ
                    data = await self.pipeline.collector.collect_sns_trends(
                        platform=platform,
                        keywords=[hashtag]
                    )

                    # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
                    await self.pipeline.process_single(data)

                # ë¡œê¹…
                logger.info(f"{platform} íŠ¸ë Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {len(hashtags)} í•´ì‹œíƒœê·¸")

            except Exception as e:
                logger.error(f"{platform} íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                await self.alert_admin(f"SNS íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {platform}")
```

---

## 13. ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤

### 13.1 ë°ì´í„° ë³´ì•ˆ

```python
class DataSecurity:
    """
    ë°ì´í„° ë³´ì•ˆ ê´€ë¦¬
    """

    def __init__(self):
        self.encryptor = DataEncryptor()
        self.masker = PIIMasker()
        self.validator = ComplianceValidator()

    async def secure_data(self, data: RawData) -> SecuredData:
        """
        ë°ì´í„° ë³´ì•ˆ ì²˜ë¦¬
        """
        # PII ë§ˆìŠ¤í‚¹
        masked_data = await self.masker.mask(data)

        # ë¯¼ê° ì •ë³´ ì•”í˜¸í™”
        encrypted = await self.encryptor.encrypt_sensitive(masked_data)

        # ê·œì • ì¤€ìˆ˜ ê²€ì¦
        compliance = await self.validator.validate(encrypted)

        if not compliance.passed:
            raise ComplianceError(compliance.issues)

        return SecuredData(
            data=encrypted,
            security_level='high',
            compliance=compliance
        )

class PIIMasker:
    """
    ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
    """

    patterns = {
        'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
        'phone': r'010-?\d{4}-?\d{4}',
        'rrn': r'\d{6}-[1-4]\d{6}',  # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
        'card': r'\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}',
        'account': r'\d{3,6}-\d{2,6}-\d{6,}',  # ê³„ì¢Œë²ˆí˜¸
    }

    async def mask(self, text: str) -> str:
        """
        ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬
        """
        masked = text

        for pii_type, pattern in self.patterns.items():
            masked = re.sub(pattern, f'[{pii_type.upper()}_MASKED]', masked)

        return masked
```

---

## 14. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 14.1 íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­

```python
class PipelineMetrics:
    """
    íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­
    """

    def __init__(self):
        # Prometheus ë©”íŠ¸ë¦­
        self.metrics = {
            'documents_processed': Counter(
                'pipeline_documents_processed_total',
                'Total documents processed',
                ['source', 'status']
            ),
            'processing_time': Histogram(
                'pipeline_processing_seconds',
                'Processing time',
                ['stage']
            ),
            'embedding_quality': Gauge(
                'pipeline_embedding_quality',
                'Embedding quality score'
            ),
            'rag_accuracy': Gauge(
                'pipeline_rag_accuracy',
                'RAG retrieval accuracy'
            ),
            'storage_usage': Gauge(
                'pipeline_storage_bytes',
                'Storage usage in bytes',
                ['type']
            )
        }

    async def record_processing(
        self,
        source: str,
        stage: str,
        duration: float,
        status: str
    ):
        """
        ì²˜ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡
        """
        self.metrics['documents_processed'].labels(
            source=source,
            status=status
        ).inc()

        self.metrics['processing_time'].labels(
            stage=stage
        ).observe(duration)
```

---

## 15. KPI ë° ì„±ê³µ ì§€í‘œ

### 15.1 í•µì‹¬ ì„±ê³¼ ì§€í‘œ

| KPI | ëª©í‘œ | ì¸¡ì • ë°©ë²• | í˜„ì¬ |
|-----|------|-----------|------|
| **ë°ì´í„° ìˆ˜ì§‘ëŸ‰** | 100K ë¬¸ì„œ/ì¼ | ì¼ë³„ ì²˜ë¦¬ ë¬¸ì„œ ìˆ˜ | - |
| **ì²˜ë¦¬ ì†ë„** | < 10ì´ˆ/ë¬¸ì„œ | P90 ì²˜ë¦¬ ì‹œê°„ | - |
| **ì„ë² ë”© í’ˆì§ˆ** | > 0.85 | Cosine similarity | - |
| **RAG ì •í™•ë„** | > 90% | Relevance score | - |
| **íŠ¸ë Œë“œ ì ì‹œì„±** | < 1ì‹œê°„ | íŠ¸ë Œë“œ ë°œê²¬-ë°˜ì˜ ì‹œê°„ | - |
| **ë¸Œëœë“œ ì¼ê´€ì„±** | > 95% | Brand consistency score | - |
| **ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨** | < $0.01/GB | ì›”ê°„ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© | - |
| **API ì‘ë‹µ ì‹œê°„** | < 500ms | P95 latency | - |

### 15.2 í’ˆì§ˆ ì§€í‘œ

| ì§€í‘œ | ê¸°ì¤€ | ì¸¡ì • ì£¼ê¸° |
|------|------|-----------|
| **í…ìŠ¤íŠ¸ ì¶”ì¶œ ì •í™•ë„** | > 95% | ì¼ë³„ |
| **ì¤‘ë³µ ì œê±°ìœ¨** | > 99% | ì‹¤ì‹œê°„ |
| **PII ë§ˆìŠ¤í‚¹ ì •í™•ë„** | 100% | ì‹¤ì‹œê°„ |
| **ì²­í‚¹ í’ˆì§ˆ** | > 0.8 | ì£¼ë³„ |
| **ê²€ìƒ‰ ê´€ë ¨ì„±** | > 0.85 | ì‹¤ì‹œê°„ |

---

## 16. Next Steps

### 16.1 Phase 1 ê°œë°œ ìš°ì„ ìˆœìœ„

1. **ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•** (Week 1-2)
   - Collector ê¸°ë³¸ êµ¬í˜„
   - Parser í†µí•©
   - ë²¡í„° DB ì„¤ì •

2. **í¬ë¡¤ë§ ì‹œìŠ¤í…œ** (Week 3-4)
   - ì›¹ í¬ë¡¤ëŸ¬ êµ¬í˜„
   - SNS API ì—°ë™
   - ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

3. **RAG ì—”ì§„** (Week 5-6)
   - ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„
   - ë¦¬ë­í‚¹ ë¡œì§
   - ìºì‹± ì „ëµ

4. **ìê°€í•™ìŠµ ì—°ë™** (Week 7-8)
   - í”¼ë“œë°± ìˆ˜ì§‘
   - íŒ¨í„´ ë¶„ì„
   - ëª¨ë¸ ì—…ë°ì´íŠ¸

### 16.2 ì—°ê´€ ë¬¸ì„œ

- [BRAND_LEARNING_ENGINE.md](./BRAND_LEARNING_ENGINE.md) - ìê°€í•™ìŠµ ìƒì„¸
- [AGENTS_SPEC.md](./AGENTS_SPEC.md) - ì—ì´ì „íŠ¸ ëª…ì„¸
- [LLM_ROUTER_POLICY.md](./LLM_ROUTER_POLICY.md) - ë¼ìš°íŒ… ì •ì±…

---

## 17. ë¶€ë¡

### 17.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

```sql
-- ì†ŒìŠ¤ ë¬¸ì„œ í…Œì´ë¸”
CREATE TABLE sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT,
    type VARCHAR(50),
    brand_id UUID,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- ë²¡í„° í…Œì´ë¸”
CREATE TABLE vectors (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID REFERENCES sources(id),
    chunk_text TEXT,
    embedding vector(3072),  -- OpenAI text-embedding-3-large
    model VARCHAR(50),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- ë¸Œëœë“œ í”„ë¡œí•„ í…Œì´ë¸”
CREATE TABLE brand_profiles (
    brand_id UUID PRIMARY KEY,
    name VARCHAR(255),
    keywords TEXT[],
    tone_vectors FLOAT[][],
    style_vectors FLOAT[][],
    forbidden_words TEXT[],
    metadata JSONB,
    updated_at TIMESTAMP
);

-- íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ í…Œì´ë¸”
CREATE TABLE trend_insights (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    keyword VARCHAR(255),
    platform VARCHAR(50),
    trend_score FLOAT,
    engagement_data JSONB,
    discovered_at TIMESTAMP,
    expires_at TIMESTAMP
);

-- ì¸ë±ìŠ¤
CREATE INDEX vectors_embedding_idx ON vectors
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX vectors_metadata_idx ON vectors USING gin (metadata);
CREATE INDEX vectors_brand_idx ON vectors ((metadata->>'brand_id'));
CREATE INDEX sources_brand_idx ON sources (brand_id);
CREATE INDEX trend_insights_keyword_idx ON trend_insights (keyword);
```

### 17.2 API ì—”ë“œí¬ì¸íŠ¸

```python
# FastAPI ë¼ìš°íŠ¸ ì˜ˆì‹œ
@app.post("/pipeline/ingest")
async def ingest_data(
    file: UploadFile = File(...),
    brand_id: str = Form(...),
    metadata: str = Form("{}")
):
    """ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬"""
    pass

@app.get("/pipeline/search")
async def search(
    query: str,
    brand_id: Optional[str] = None,
    top_k: int = 10
):
    """RAG ê²€ìƒ‰"""
    pass

@app.post("/pipeline/crawl")
async def trigger_crawl(
    url: str,
    depth: int = 2,
    brand_id: Optional[str] = None
):
    """ì›¹ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°"""
    pass

@app.get("/pipeline/trends")
async def get_trends(
    platform: str,
    limit: int = 20
):
    """íŠ¸ë Œë“œ ì¡°íšŒ"""
    pass
```

### 17.3 í™˜ê²½ ë³€ìˆ˜

```bash
# .env íŒŒì¼
DATABASE_URL=postgresql://sparklio:password@localhost/sparklio
REDIS_URL=redis://localhost:6379
MINIO_ENDPOINT=http://localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin

# API Keys
OPENAI_API_KEY=sk-...
SERPAPI_KEY=...
INSTAGRAM_ACCESS_TOKEN=...
TIKTOK_API_KEY=...

# ì„¤ì •
PIPELINE_BATCH_SIZE=100
EMBEDDING_MODEL=text-embedding-3-large
CHUNK_SIZE=512
CHUNK_OVERLAP=128
```

---

## ë³€ê²½ ì´ë ¥

| ë‚ ì§œ | ë²„ì „ | ë³€ê²½ ë‚´ìš© | ì‘ì„±ì |
|------|------|-----------|--------|
| 2025-01-13 | 1.0 | ì´ˆê¸° ì‘ì„± | ë°ì´í„°íŒ€ |
| 2025-01-14 | 2.0 | TrendPipeline í†µí•©, í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì¶”ê°€, RAG ê³ ë„í™” | ë°ì´í„°íŒ€ |