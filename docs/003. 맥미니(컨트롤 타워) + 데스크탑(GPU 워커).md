좋아요, 이제 **맥미니(컨트롤 타워) + 데스크탑(GPU 워커)** 기준으로  
전체 서버 셋팅 & 체크 플로우를 한 번에 정리해볼게요.  
(나중에 그대로 `SERVER_SETUP_GUIDE.md`로 써도 되는 수준으로 정리하겠습니다.)

---

## 0. 목표 아키텍처 다시 정리

- **맥미니 (Control Tower)**
    
    - 역할: Backend API(FastAPI), DB(Postgres), Redis, MinIO, Admin/Stats
        
    - 실행 방식: **무조건 Docker Compose (docker-compose.mini.yml)**  
        → 로컬 uvicorn 직접 실행은 “개발 디버깅 용”일 때만.
        
- **데스크탑 (GPU Worker)**
    
    - 역할: Ollama(Qwen/Llama 등), 기타 LLM, ComfyUI(이미지·비디오), Nanobanana 연동 브리지 등
        
    - 실행 방식: LLM 스택은 Docker(ollama, qwen 등), ComfyUI는 로컬 또는 Docker
        
- **노트북 (개발용)**
    
    - 역할: VSCode, 프론트엔드 dev 서버(Next.js)
        
    - API 호출 대상: **맥미니 IP:PORT** (예: [http://100.123.51.5:8000](http://100.123.51.5:8000/))
        

---

## 1. 맥미니 서버 셋팅 & 체크

### 1-1. 필수 파일/경로 확인

맥미니에 SSH 또는 VNC로 접속 후:

```bash
cd ~/sparklio_ai_marketing_studio   # 실제 클론된 경로
ls
# backend/
# frontend/
# docker-compose.mini.yml
# .env.mini (또는 .env)
```

확인할 것:

- `docker-compose.mini.yml` 존재
    
- `backend/.env.mini` 또는 `backend/.env` 에 아래와 같은 설정이 들어있어야 함 (예시):
    

```env
# 백엔드 공통
GENERATOR_MODE=live          # 실서비스는 live, 테스트는 mock
API_PORT=8000
ADMIN_PORT=8001

# DB
POSTGRES_HOST=db
POSTGRES_PORT=5432

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# MinIO
MINIO_ENDPOINT=minio:9000

# GPU 워커(데스크탑)와의 연결
OLLAMA_BASE_URL=http://100.120.180.42:11434
COMFYUI_BASE_URL=http://100.120.180.42:8188
NANOBANANA_BASE_URL=...      # 사용 시
```

👉 중요한 점:  
**맥미니의 백엔드는 127.0.0.1이 아니라 Docker 네트워크 기준 호스트명을 사용**하고,  
GPU 쪽 리소스는 **데스크탑의 Tailscale IP(예: 100.120.180.42)** 로 지정.

---

### 1-2. Docker Compose로 서버 기동

맥미니에서:

```bash
cd ~/sparklio_ai_marketing_studio

# (1) 백엔드 스택 기동
docker compose -f docker-compose.mini.yml up -d

# (2) 컨테이너 상태 확인
docker ps
```

여기에서 대략 이런 이름들이 떠야 정상:

- `sparklio-backend`
    
- `sparklio-db`
    
- `sparklio-redis`
    
- `sparklio-minio`
    
- (있다면) `sparklio-admin` 등
    

---

### 1-3. 맥미니 내부에서 헬스체크

```bash
# 맥미니 터미널에서
curl http://localhost:8000/health
curl http://localhost:8000/api/v1/admin/stats
```

- `{"status":"ok"}` 비슷한 응답이 나오면 정상
    
- 안 나오면 `docker logs sparklio-backend`로 에러 확인
    

---

### 1-4. 노트북/데스크탑에서 맥미니 접근 체크

노트북 PowerShell에서:

```powershell
# (1) 맥미니 Tailscale IP 핑
ping 100.123.51.5

# (2) 백엔드 헬스체크
curl http://100.123.51.5:8000/health

# (3) 브라우저로 접속
# 주소창에 입력:
# http://100.123.51.5:8000/docs
# http://100.123.51.5:8001/api/v1/admin/stats  (있다면)
```

여기까지 통과되면,  
**"맥미니 백엔드 서버는 정상적으로 외부에서 접근 가능"** 상태입니다.

---

## 2. 데스크탑(GPU 워커) 셋팅 & 체크

### 2-1. Ollama / LLM 스택

데스크탑(윈도우)에서 PowerShell:

```powershell
cd D:\sparklio-worker   # 예시 경로

# Docker 기반 LLM 스택 실행 (예시)
docker compose -f docker-compose.4070.yml up -d

docker ps
```

확인:

- `ollama` 컨테이너
    
- 필요하면 `qwen-server`, `llama-server` 등
    

**모델 Pull** (한번만):

```powershell
# 로컬에서
ollama pull llama3.1
ollama pull qwen2:14b
```

---
## ✅ 데스크탑(GPU 워커) – ComfyUI 설정 (로컬 전용 버전)

### 2-2. ComfyUI (이미지·비디오 생성) – **항상 로컬 실행**

> ⚠️ 이 프로젝트에서 ComfyUI는 **항상 데스크탑 로컬 앱으로 실행**합니다.  
> ComfyUI용 Docker 컨테이너는 **사용하지 않습니다.**  
> Claude에게 주는 문서에서도 **ComfyUI + Docker 관련 내용은 전부 제거**하세요.

#### 1) ComfyUI 실행

데스크탑(Windows)에서:

```powershell
cd D:\AI\ComfyUI
.\run_nvidia_gpu.bat
```

- 이 배치 파일이 ComfyUI 서버를 띄우는 **유일한 방식**입니다.
    
- 실행 후 기본 포트: **[http://localhost:8188](http://localhost:8188/)**
    

#### 2) 데스크탑에서 헬스 체크

```powershell
curl http://localhost:8188
```

- HTML 또는 JSON 비슷한 응답이 오면 정상.
    

#### 3) 맥미니에서 데스크탑 ComfyUI 접속 확인

맥미니 터미널에서:

```bash
curl http://100.120.180.42:8188
```

- 여기서 응답이 오면  
    **“맥미니 → 데스크탑 ComfyUI 호출 가능”** 상태입니다.
    

#### 4) 환경변수 / 설정 예시

맥미니 `backend/.env.mini` (또는 `.env`) 에:

```env
COMFYUI_BASE_URL=http://100.120.180.42:8188
```

백엔드 LLM/Media Gateway에서는:

- 이미지/비디오 생성시 `COMFYUI_BASE_URL`을 사용해  
    **데스크탑 로컬 ComfyUI 서버**로 요청을 보냄.
    

---


### 2-3. 데스크탑 Tailscale / 방화벽 체크

- 데스크탑 Tailscale가 **로그인 & 온라인 상태**인지 확인
    
- Windows 방화벽에서 11434(ollama), 8188(ComfyUI) 포트가 허용되어 있는지 확인  
    (필요시 “인바운드 규칙”에 추가)
    

---

## 3. 노트북(개발용) 셋팅 & 체크

### 3-1. 프론트엔드 API 타깃을 “맥미니”로 고정

노트북에서:

```powershell
cd K:\sparklio_ai_marketing_studio\frontend
```

`next.config.js` 또는 `.env.local` 에 다음이 들어가야 합니다.

```env
# .env.local 예시
NEXT_PUBLIC_API_BASE_URL=http://100.123.51.5:8000
NEXT_PUBLIC_ADMIN_API_BASE_URL=http://100.123.51.5:8001
```

> 지금 로그를 보면 `/api/v1/admin/stats`를 `127.0.0.1:8001` 로 프록시하려다  
> `ECONNREFUSED`가 나는 상황이었어요.  
> → 이 부분을 **맥미니 IP로 교체**하거나, 개발 중에는 admin 프록시를 꺼도 됩니다.

예를 들어 `next.config.js` 안에서:

```js
const API_BASE = process.env.NEXT_PUBLIC_API_BASE_URL;
// const ADMIN_BASE = "http://127.0.0.1:8001";  // ❌
// 이렇게 변경:
const ADMIN_BASE = process.env.NEXT_PUBLIC_ADMIN_API_BASE_URL;
```

---

### 3-2. 프론트 dev 서버 실행

```powershell
cd K:\sparklio_ai_marketing_studio\frontend
npm run dev
# → http://localhost:3000
```

프론트에서 보내는 요청이  
**반드시 `http://100.123.51.5:8000/...` 으로 나가는지**  
브라우저 DevTools Network 탭에서 한 번 확인해 주세요.

---

### 3-3. “백엔드가 어디서 도는지” 헷갈리지 않는 법

지금처럼 노트북에서 `uvicorn app.main:app`을 실행하면  
**노트북 로컬 백엔드(개발용)**가 떠버립니다.

최종 구조에서는:

- **실서비스 / 통합 테스트**:  
    👉 백엔드는 **맥미니 Docker**만 사용  
    (노트북 uvicorn 실행 X)
    
- 노트북 uvicorn은  
    👉 “완전 로컬 실험”이나 “빠른 디버깅”용으로만 사용
    

헷갈릴 때는 노트북에서:

```powershell
# 이걸 쳐서 확인
curl http://localhost:8000/health
curl http://100.123.51.5:8000/health
```

- 둘 다 응답이 오면 **두 군데에서 백엔드가 동시에 돌고 있는 상황**이라서  
    프론트 설정을 다시 확인해야 합니다.
    

---

## 4. 맥미니 접속이 안 될 때 체크 리스트

맥미니에 접속이 안 될 때는 아래 순서로 보는 것이 좋습니다.

1. **맥미니 전원 / 잠자기 모드**
    
    - 화면/키보드 연결해서 깨어 있는지 확인
        
2. **Tailscale 상태**
    
    - 맥미니에서 Tailscale 클라이언트 열어:
        
        - 로그인 상태인지
            
        - “온라인”인지
            
    - 노트북 Tailscale에서 맥미니 이름이 보이는지
        
3. **IP 확인**
    
    - 맥미니 Tailscale IP가 `100.123.51.5`가 맞는지 다시 확인  
        (변경되었을 수 있음)
        
4. **도커 스택 상태**
    
    - `docker ps` 에서 백엔드 컨테이너가 뜨는지
        
5. **포트 열림 확인**
    
    - 맥미니에서 `curl http://localhost:8000/health`
        
    - 노트북에서 `curl http://<맥미니-IP>:8000/health`
        

여기까지 모두 확인했는데도 안 되면,  
그때는 **맥미니에서 `docker logs sparklio-backend` 출력 전체**를 한 번 보여주면  
구체적으로 어디서 막히는지 같이 볼 수 있습니다.

---

## 5. LLM 선택과 서버 구조의 연결

앞에서 설계한 **유저 LLM 선택 기능**과 이 서버 구조는 이렇게 이어집니다:

- 맥미니의 `.env.mini` 에
    
    - `OLLAMA_BASE_URL=http://100.120.180.42:11434`
        
    - `COMFYUI_BASE_URL=http://100.120.180.42:8188`
        
- 백엔드 Gateway에서
    
    - `provider_from_name("ollama")` → 저 URL을 호출
        
    - `provider_from_name("comfyui_image")` → ComfyUI 이미지 워크플로 호출
        
- 프론트에서
    
    - 유저가 Text LLM을 “Qwen(ollama)”로, 이미지 엔진을 “ComfyUI”로 선택
        
    - → `llm_selection`이 맥미니 백엔드로 전달
        
    - → 맥미니가 데스크탑 GPU 워커로 요청을 넘기는 구조
        

---

원하시면 이 내용을 바로 붙여넣을 수 있게  
`docs/INFRA_SERVER_SETUP.md` 같은 파일 형식으로 정리해서  
섹션/파일명까지 포함한 버전도 만들어 드릴게요.