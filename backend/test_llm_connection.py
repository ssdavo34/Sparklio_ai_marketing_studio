"""
LLM Provider ì‹¤ì œ ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸
ê° Providerê°€ ì‹¤ì œë¡œ APIì™€ í†µì‹ í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸

ì‘ì„±ì¼: 2025-11-20
ì‘ì„±ì: BíŒ€ (Backend)
"""

import asyncio
import sys
import os
from typing import Dict, Any

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.config import settings


async def test_openai_connection():
    """OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from app.services.llm.providers.openai_provider import OpenAIProvider

        provider = OpenAIProvider()

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        response = await provider.chat(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Say 'OpenAI Connected' if you can read this."}],
            temperature=0.1
        )

        if response and response.content:
            print(f"âœ… OpenAI ì—°ê²° ì„±ê³µ!")
            print(f"   ì‘ë‹µ: {response.content[:100]}...")
            print(f"   ëª¨ë¸: {response.model}")
            return True
        else:
            print("âŒ OpenAI ì‘ë‹µ ì—†ìŒ")
            return False

    except Exception as e:
        print(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False


async def test_anthropic_connection():
    """Anthropic (Claude) API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” Anthropic (Claude) API ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from app.services.llm.providers.anthropic_provider import AnthropicProvider

        provider = AnthropicProvider()

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        response = await provider.chat(
            model="claude-3-5-haiku-20241022",
            messages=[{"role": "user", "content": "Say 'Claude Connected' if you can read this."}],
            temperature=0.1
        )

        if response and response.content:
            print(f"âœ… Anthropic ì—°ê²° ì„±ê³µ!")
            print(f"   ì‘ë‹µ: {response.content[:100]}...")
            print(f"   ëª¨ë¸: {response.model}")
            return True
        else:
            print("âŒ Anthropic ì‘ë‹µ ì—†ìŒ")
            return False

    except Exception as e:
        print(f"âŒ Anthropic ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False


async def test_gemini_connection():
    """Google Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” Google Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from app.services.llm.providers.gemini_provider import GeminiProvider

        provider = GeminiProvider()

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        response = await provider.chat(
            model="gemini-2.0-flash-exp",
            messages=[{"role": "user", "content": "Say 'Gemini Connected' if you can read this."}],
            temperature=0.1
        )

        if response and response.content:
            print(f"âœ… Gemini ì—°ê²° ì„±ê³µ!")
            print(f"   ì‘ë‹µ: {response.content[:100]}...")
            print(f"   ëª¨ë¸: {response.model}")
            return True
        else:
            print("âŒ Gemini ì‘ë‹µ ì—†ìŒ")
            return False

    except Exception as e:
        print(f"âŒ Gemini ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False


async def test_ollama_connection():
    """Ollama API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” Ollama API ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from app.services.llm.providers.ollama import OllamaProvider

        provider = OllamaProvider()

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        response = await provider.chat(
            model="qwen2.5:7b",
            messages=[{"role": "user", "content": "Say 'Ollama Connected' if you can read this."}],
            temperature=0.1
        )

        if response and response.content:
            print(f"âœ… Ollama ì—°ê²° ì„±ê³µ!")
            print(f"   ì‘ë‹µ: {response.content[:100]}...")
            print(f"   ëª¨ë¸: {response.model}")
            return True
        else:
            print("âŒ Ollama ì‘ë‹µ ì—†ìŒ")
            return False

    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        print(f"   (Ollama ì„œë²„ê°€ {settings.OLLAMA_BASE_URL}ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”)")
        return False


async def test_llm_gateway():
    """LLM Gateway í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” LLM Gateway í†µí•© í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from app.services.llm import LLMGateway

        gateway = LLMGateway()

        # ê° Providerë¡œ í…ŒìŠ¤íŠ¸
        test_cases = [
            ("gpt-4o-mini", "openai"),
            ("claude-3-5-haiku-20241022", "anthropic"),
            ("gemini-2.0-flash-exp", "gemini"),
            # ("qwen2.5:7b", "ollama")  # OllamaëŠ” ë¡œì»¬ ì„œë²„ í•„ìš”
        ]

        results = []
        for model, expected_provider in test_cases:
            try:
                response = await gateway.chat(
                    model=model,
                    messages=[{"role": "user", "content": f"Say 'Hello from {model}'"}],
                    temperature=0.1
                )

                if response and response.content:
                    print(f"âœ… {model:30} â†’ ì„±ê³µ")
                    results.append(True)
                else:
                    print(f"âŒ {model:30} â†’ ì‘ë‹µ ì—†ìŒ")
                    results.append(False)

            except Exception as e:
                print(f"âŒ {model:30} â†’ ì‹¤íŒ¨: {str(e)[:50]}...")
                results.append(False)

        return all(results) if results else False

    except Exception as e:
        print(f"âŒ LLM Gateway í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False


async def check_api_keys():
    """API í‚¤ ì„¤ì • í™•ì¸"""
    print("\nğŸ“‹ API í‚¤ ì„¤ì • í™•ì¸")
    print("-" * 40)

    keys_status = {
        "OpenAI": bool(getattr(settings, 'OPENAI_API_KEY', None)),
        "Anthropic": bool(getattr(settings, 'ANTHROPIC_API_KEY', None)),
        "Google (Gemini)": bool(getattr(settings, 'GOOGLE_API_KEY', None)),
        "Ollama URL": bool(getattr(settings, 'OLLAMA_BASE_URL', None))
    }

    for provider, has_key in keys_status.items():
        status = "âœ… ì„¤ì •ë¨" if has_key else "âŒ ë¯¸ì„¤ì •"
        print(f"{provider:15} : {status}")

    return all(keys_status.values())


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 50)
    print("ğŸš€ LLM Provider ì—°ê²° ìƒíƒœ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # API í‚¤ í™•ì¸
    keys_ok = await check_api_keys()

    if not keys_ok:
        print("\nâš ï¸ ì¼ë¶€ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # ê° Provider í…ŒìŠ¤íŠ¸
    results = {
        "OpenAI": await test_openai_connection(),
        "Anthropic": await test_anthropic_connection(),
        "Gemini": await test_gemini_connection(),
        "Ollama": await test_ollama_connection()
    }

    # LLM Gateway í†µí•© í…ŒìŠ¤íŠ¸
    gateway_ok = await test_llm_gateway()

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)

    for provider, success in results.items():
        status = "âœ… ì •ìƒ" if success else "âŒ ì‹¤íŒ¨"
        print(f"{provider:15} : {status}")

    print(f"{'LLM Gateway':15} : {'âœ… ì •ìƒ' if gateway_ok else 'âŒ ì‹¤íŒ¨'}")

    # ì „ì²´ ìƒíƒœ
    all_success = all(results.values()) and gateway_ok

    print("\n" + "=" * 50)
    if all_success:
        print("ğŸ‰ ëª¨ë“  LLM Providerê°€ ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì¼ë¶€ Provider ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("\nğŸ“ í™•ì¸ ì‚¬í•­:")
        if not results["OpenAI"]:
            print("  - OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸")
        if not results["Anthropic"]:
            print("  - Anthropic API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸")
        if not results["Gemini"]:
            print("  - Google API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸")
        if not results["Ollama"]:
            print(f"  - Ollama ì„œë²„ê°€ {settings.OLLAMA_BASE_URL}ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")

    return all_success


if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    success = asyncio.run(main())

    # ì¢…ë£Œ ì½”ë“œ
    exit(0 if success else 1)