# BíŒ€ Meeting AI Transcriber êµ¬í˜„ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-11-24 (ì¼ìš”ì¼)
**ì‘ì„±ì**: AíŒ€ (QA & Testing)
**ëŒ€ìƒ**: BíŒ€ (Backend)
**ì°¸ì¡° ë¬¸ì„œ**:
- [backend/docs/MEETING_AI_TRANSCRIBER_SPEC.md](./MEETING_AI_TRANSCRIBER_SPEC.md)
- [docs/MEETING_AI_ARCHITECTURE.md](../../docs/MEETING_AI_ARCHITECTURE.md)

---

## ğŸ“‹ ëª©ì°¨

1. [êµ¬í˜„ í˜„í™© ì •ë¦¬](#1-êµ¬í˜„-í˜„í™©-ì •ë¦¬)
2. [Step 1: FasterWhisperClient êµ¬í˜„](#2-step-1-fasterwhisperclient-êµ¬í˜„)
3. [Step 2: TranscriberService 4-Mode êµ¬í˜„](#3-step-2-transcriberservice-4-mode-êµ¬í˜„)
4. [Step 3: API ì—”ë“œí¬ì¸íŠ¸ ì—…ë°ì´íŠ¸](#4-step-3-api-ì—”ë“œí¬ì¸íŠ¸-ì—…ë°ì´íŠ¸)
5. [A/CíŒ€ í˜‘ì—… ê°€ì´ë“œ](#5-acíŒ€-í˜‘ì—…-ê°€ì´ë“œ)
6. [ë‹¤ìŒ ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸](#6-ë‹¤ìŒ-ë‹¨ê³„-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## 1. êµ¬í˜„ í˜„í™© ì •ë¦¬

### 1-1. í˜„ì¬ ìƒíƒœ (2025-11-24 ê¸°ì¤€)

âœ… **ì™„ë£Œëœ ì‘ì—…**:
- ì•„í‚¤í…ì²˜ ë¬¸ì„œ: `MEETING_AI_ARCHITECTURE.md` ì‘ì„± ì™„ë£Œ
- ìƒì„¸ ìŠ¤í™: `MEETING_AI_TRANSCRIBER_SPEC.md` ì‘ì„± ì™„ë£Œ
- DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„: `meeting_transcripts` í…Œì´ë¸” ì •ì˜ ì™„ë£Œ
- í™˜ê²½ë³€ìˆ˜ ì •ì˜: `WHISPER_MODE`, `WHISPER_LOCAL_BACKEND` ë“± ì •ì˜ ì™„ë£Œ

ğŸ”§ **êµ¬í˜„ í•„ìš” ì»´í¬ë„ŒíŠ¸**:
1. `backend/app/services/transcriber_clients.py` - Whisper í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
2. `backend/app/services/transcriber.py` - TranscriberService 4-Mode êµ¬í˜„
3. `backend/app/api/routes/meetings.py` - POST `/meetings/{id}/transcribe` ì—”ë“œí¬ì¸íŠ¸
4. `backend/app/schemas/transcriber.py` - Request/Response ìŠ¤í‚¤ë§ˆ (ì¼ë¶€ ì¡´ì¬ ì‹œ ì—…ë°ì´íŠ¸)
5. `backend/app/services/meeting_audio.py` - ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì²˜ë¦¬ (ì¡´ì¬ ì‹œ ê²€í† )
6. `backend/app/services/meeting_agent.py` - MeetingAgent íŠ¸ë¦¬ê±° (ì¡´ì¬ ì‹œ ê²€í† )

### 1-2. êµ¬í˜„ ìš°ì„ ìˆœìœ„

```
ìš°ì„ ìˆœìœ„ 1: FasterWhisperClient (RTX Desktop ì—°ë™)
ìš°ì„ ìˆœìœ„ 2: TranscriberService 4-Mode ë¡œì§
ìš°ì„ ìˆœìœ„ 3: API ì—”ë“œí¬ì¸íŠ¸ `/meetings/{id}/transcribe`
ìš°ì„ ìˆœìœ„ 4: A/CíŒ€ ì—°ë™ í…ŒìŠ¤íŠ¸
```

---

## 2. Step 1: FasterWhisperClient êµ¬í˜„

### 2-1. íŒŒì¼ ìœ„ì¹˜
**ê²½ë¡œ**: `backend/app/services/transcriber_clients.py`

### 2-2. êµ¬í˜„ ë‚´ìš©

#### BaseWhisperClient (ì¶”ìƒ í´ë˜ìŠ¤)

```python
# app/services/transcriber_clients.py
"""
Whisper í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„

ë‹¤ì–‘í•œ Whisper ë°±ì—”ë“œë¥¼ ì¶”ìƒí™”í•˜ì—¬ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ ì œê³µ:
- OpenAI Whisper API
- whisper.cpp ì„œë²„
- faster-whisper ì„œë²„ (RTX Desktop)

ì‘ì„±ì¼: 2025-11-24
ì‘ì„±ì: BíŒ€ (Backend)
"""

import httpx
import logging
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

from app.core.settings import Settings
from app.schemas.transcriber import TranscriptionResult, TranscriptSegment

logger = logging.getLogger(__name__)


class BaseWhisperClient(ABC):
    """
    Whisper í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤

    ëª¨ë“  Whisper ë°±ì—”ë“œëŠ” ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼ í•¨
    """

    @abstractmethod
    async def transcribe(self, audio_path: str, **kwargs) -> TranscriptionResult:
        """
        ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ)
            **kwargs: ë°±ì—”ë“œë³„ ì¶”ê°€ ì˜µì…˜
                - model_profile: str (ì˜ˆ: "small", "medium", "large-v3")
                - language: str (ì˜ˆ: "ko", "en", "auto")
                - task: str (ì˜ˆ: "transcribe", "translate")

        Returns:
            TranscriptionResult: ë³€í™˜ëœ í…ìŠ¤íŠ¸ + ì„¸ê·¸ë¨¼íŠ¸ + ë©”íƒ€ë°ì´í„°
        """
        pass


class OpenAIWhisperClient(BaseWhisperClient):
    """
    OpenAI Whisper API í´ë¼ì´ì–¸íŠ¸

    - API í˜¸ì¶œ: openai.Audio.transcribe()
    - ë¹„ìš©: $0.006/ë¶„ (2025ë…„ ê¸°ì¤€)
    - ëª¨ë¸: whisper-1 (large-v2 ê¸°ë°˜)
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        self.api_key = settings.OPENAI_API_KEY

        # TODO: OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # import openai
        # openai.api_key = self.api_key

    async def transcribe(self, audio_path: str, **kwargs) -> TranscriptionResult:
        """
        OpenAI Whisper APIë¡œ ìŒì„± ë³€í™˜

        TODO: ì‹¤ì œ OpenAI API í˜¸ì¶œ êµ¬í˜„
        - openai.Audio.transcribe() ì‚¬ìš©
        - response_format="verbose_json" ì„¤ì • (ì„¸ê·¸ë¨¼íŠ¸ í¬í•¨)
        - language ì§€ì • (auto ê°ì§€ëŠ” ë¯¸ì§€ì •)
        """
        logger.info(f"[OpenAI] Transcribing: {audio_path}")

        # TODO: êµ¬í˜„ í•„ìš”
        raise NotImplementedError("OpenAI Whisper API ì—°ë™ êµ¬í˜„ í•„ìš”")


class WhisperCppClient(BaseWhisperClient):
    """
    whisper.cpp ì„œë²„ í´ë¼ì´ì–¸íŠ¸

    - ì„œë²„: HTTP API (C++ ê¸°ë°˜ whisper.cpp)
    - ìš©ë„: CPU í™˜ê²½ì—ì„œì˜ ë¡œì»¬ STT
    - ì—”ë“œí¬ì¸íŠ¸: settings.WHISPER_CPP_ENDPOINT
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        self.endpoint = settings.WHISPER_CPP_ENDPOINT  # ì˜ˆ: http://localhost:8080/inference
        self.timeout = settings.WHISPER_TIMEOUT_SECONDS

    async def transcribe(self, audio_path: str, **kwargs) -> TranscriptionResult:
        """
        whisper.cpp ì„œë²„ë¡œ ìŒì„± ë³€í™˜

        TODO: whisper.cpp ì„œë²„ ìŠ¤í™ì— ë§ì¶° êµ¬í˜„
        - HTTP POST multipart/form-data
        - ì‘ë‹µ í˜•ì‹ì„ TranscriptionResultë¡œ ë§¤í•‘
        """
        logger.info(f"[whisper.cpp] Transcribing: {audio_path}")

        # TODO: êµ¬í˜„ í•„ìš”
        raise NotImplementedError("whisper.cpp ì„œë²„ ì—°ë™ êµ¬í˜„ í•„ìš”")


class FasterWhisperClient(BaseWhisperClient):
    """
    faster-whisper ì„œë²„ í´ë¼ì´ì–¸íŠ¸ (RTX Desktop)

    - ì„œë²„: RTX Desktop (IP: 100.120.180.42, Port: 9000)
    - GPU: NVIDIA RTX 4060 Ti (VRAM 16GB)
    - ì—”ì§„: faster-whisper (CTranslate2 ê¸°ë°˜)
    - ì†ë„: ~15x realtime (large-v3 ëª¨ë¸ ê¸°ì¤€)

    ìš”ì²­ í˜•ì‹:
        POST http://100.120.180.42:9000/transcribe
        Content-Type: multipart/form-data

        - audio_file: binary (audio/wav, audio/mp3 ë“±)
        - model: str (ì˜ˆ: "large-v3", "medium", "small")
        - language: str (ì˜ˆ: "ko", "en", "auto")
        - task: str ("transcribe" ë˜ëŠ” "translate")
        - temperature: float (ê¸°ë³¸: 0.0)

    ì‘ë‹µ í˜•ì‹:
        {
          "text": "ì „ì²´ ë³€í™˜ í…ìŠ¤íŠ¸",
          "segments": [
            {
              "start": 0.0,
              "end": 2.5,
              "text": "ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸"
            },
            ...
          ],
          "language": "ko",
          "duration": 120.5,
          "backend": "faster_whisper",
          "model": "large-v3",
          "latency_ms": 8234,
          "confidence": 0.92
        }
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        self.endpoint = settings.WHISPER_FAST_ENDPOINT  # http://100.120.180.42:9000/transcribe
        self.timeout = settings.WHISPER_TIMEOUT_SECONDS  # ê¸°ë³¸: 300ì´ˆ (5ë¶„)

    async def transcribe(self, audio_path: str, **kwargs) -> TranscriptionResult:
        """
        faster-whisper ì„œë²„ë¡œ ìŒì„± ë³€í™˜

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            **kwargs:
                - model_profile: str (ê¸°ë³¸: "large-v3")
                - language: str (ê¸°ë³¸: "auto")
                - task: str (ê¸°ë³¸: "transcribe")

        Returns:
            TranscriptionResult: ë³€í™˜ ê²°ê³¼

        Raises:
            httpx.HTTPError: ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” HTTP ì—ëŸ¬
            TimeoutException: íƒ€ì„ì•„ì›ƒ ì´ˆê³¼
        """
        model_profile: str = kwargs.get("model_profile") or "large-v3"
        language: str = kwargs.get("language") or "auto"
        task: str = kwargs.get("task") or "transcribe"

        logger.info(
            f"[faster-whisper] Transcribing: {audio_path} "
            f"(model={model_profile}, language={language})"
        )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # multipart/form-data ìš”ì²­ êµ¬ì„±
                with open(audio_path, "rb") as f:
                    files = {
                        "audio_file": ("audio.wav", f, "audio/wav"),
                    }
                    data = {
                        "model": model_profile,
                        "language": language,
                        "task": task,
                        "temperature": "0.0",
                    }

                    # HTTP POST ìš”ì²­
                    resp = await client.post(self.endpoint, files=files, data=data)
                    resp.raise_for_status()
                    payload = resp.json()

            # ì‘ë‹µ íŒŒì‹±: segments ë³€í™˜
            segments = [
                TranscriptSegment(
                    start=float(s["start"]),
                    end=float(s["end"]),
                    text=s["text"],
                )
                for s in payload.get("segments", [])
            ]

            # TranscriptionResult ìƒì„±
            return TranscriptionResult(
                text=payload["text"],
                segments=segments,
                language=payload.get("language", language),
                duration_seconds=float(payload.get("duration", 0.0)),
                backend=payload.get("backend", "faster_whisper"),
                model=payload.get("model", model_profile),
                latency_ms=int(payload.get("latency_ms", 0)),
                confidence=payload.get("confidence"),
            )

        except httpx.HTTPStatusError as e:
            logger.error(f"[faster-whisper] HTTP error: {e.response.status_code} - {e.response.text}")
            raise
        except httpx.TimeoutException as e:
            logger.error(f"[faster-whisper] Timeout after {self.timeout}s")
            raise
        except Exception as e:
            logger.exception(f"[faster-whisper] Unexpected error: {e}")
            raise
```

### 2-3. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `BaseWhisperClient` ì¶”ìƒ í´ë˜ìŠ¤ ì‘ì„±
- [ ] `OpenAIWhisperClient` êµ¬í˜„ (TODO ì±„ìš°ê¸°)
- [ ] `WhisperCppClient` êµ¬í˜„ (TODO ì±„ìš°ê¸°)
- [ ] `FasterWhisperClient` ì™„ì„± (RTX Desktop ì—°ë™)
- [ ] `app/schemas/transcriber.py`ì— `TranscriptionResult`, `TranscriptSegment` ì •ì˜ í™•ì¸
- [ ] RTX Desktop ì„œë²„ Health Check í…ŒìŠ¤íŠ¸ (`curl http://100.120.180.42:9000/health`)
- [ ] ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ `FasterWhisperClient.transcribe()` ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

---

## 3. Step 2: TranscriberService 4-Mode êµ¬í˜„

### 3-1. íŒŒì¼ ìœ„ì¹˜
**ê²½ë¡œ**: `backend/app/services/transcriber.py`

### 3-2. êµ¬í˜„ ë‚´ìš©

```python
# app/services/transcriber.py
"""
TranscriberService: Meeting AI Transcriber í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

4ê°€ì§€ Whisper ëª¨ë“œë¥¼ ì§€ì›í•˜ë©°, ê¸¸ì´/ì¤‘ìš”ë„ì— ë”°ë¼ ìµœì ì˜ STT ì—”ì§„ ì„ íƒ:
1. openai: OpenAI Whisper API ì „ìš©
2. local: ë¡œì»¬ ì„œë²„ ì „ìš© (faster-whisper ë˜ëŠ” whisper.cpp)
3. hybrid_cost: ë¹„ìš©/ì†ë„ ê· í˜• (ì§§ì€ íšŒì˜ëŠ” OpenAI, ê¸´ íšŒì˜ëŠ” ë¡œì»¬)
4. hybrid_quality: í’ˆì§ˆ ìš°ì„  (ë¡œì»¬ large-v3 ëª¨ë¸ ìš°ì„ , ì‹¤íŒ¨ ì‹œ OpenAI)

ì‘ì„±ì¼: 2025-11-24
ì‘ì„±ì: BíŒ€ (Backend)
ì°¸ì¡°: MEETING_AI_TRANSCRIBER_SPEC.md
"""

import logging
from typing import Optional

from app.core.settings import Settings, WhisperMode, WhisperLocalBackend
from app.schemas.transcriber import TranscriptionResult
from app.services.transcriber_clients import (
    BaseWhisperClient,
    OpenAIWhisperClient,
    WhisperCppClient,
    FasterWhisperClient,
)

logger = logging.getLogger(__name__)


class TranscriberService:
    """
    Meeting ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì„œë¹„ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - 4ê°€ì§€ Whisper ëª¨ë“œ ì§€ì›
    - Graceful Degradation (ë¡œì»¬ ì‹¤íŒ¨ ì‹œ OpenAI fallback)
    - íšŒì˜ ê¸¸ì´ ê¸°ë°˜ ëª¨ë¸ í”„ë¡œí•„ ìë™ ì„ íƒ
    - Retry ë¡œì§ (ìµœëŒ€ 3íšŒ)

    í™˜ê²½ë³€ìˆ˜:
    - WHISPER_MODE: openai | local | hybrid_cost | hybrid_quality
    - WHISPER_LOCAL_BACKEND: faster_whisper | whisper_cpp
    - WHISPER_OPENAI_MAX_MINUTES: hybrid_cost ëª¨ë“œì—ì„œ OpenAI ì‚¬ìš© ì„ê³„ê°’ (ë¶„)
    - WHISPER_MAX_RETRIES: ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 3)
    - WHISPER_PROFILE_FAST: ì§§ì€ íšŒì˜ìš© ëª¨ë¸ (ê¸°ë³¸: "small")
    - WHISPER_PROFILE_BALANCED: ì¤‘ê°„ ê¸¸ì´ íšŒì˜ìš© ëª¨ë¸ (ê¸°ë³¸: "medium")
    - WHISPER_PROFILE_ACCURATE: ê¸´ íšŒì˜ìš© ëª¨ë¸ (ê¸°ë³¸: "large-v3")
    """

    def __init__(self, settings: Optional[Settings] = None):
        """
        TranscriberService ì´ˆê¸°í™”

        Args:
            settings: Settings ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        """
        self.settings = settings or Settings()
        self.mode: WhisperMode = self.settings.WHISPER_MODE

        # OpenAI í´ë¼ì´ì–¸íŠ¸ (í•­ìƒ ì´ˆê¸°í™” - fallbackìš©)
        self.openai_client = OpenAIWhisperClient(self.settings)

        # ë¡œì»¬ í´ë¼ì´ì–¸íŠ¸ ì„ íƒ (faster-whisper ë˜ëŠ” whisper.cpp)
        if self.settings.WHISPER_LOCAL_BACKEND == WhisperLocalBackend.whisper_cpp:
            self.local_client: Optional[BaseWhisperClient] = WhisperCppClient(self.settings)
            logger.info("[Transcriber] Local backend: whisper.cpp")
        elif self.settings.WHISPER_LOCAL_BACKEND == WhisperLocalBackend.faster_whisper:
            self.local_client = FasterWhisperClient(self.settings)
            logger.info("[Transcriber] Local backend: faster-whisper")
        else:
            self.local_client = None
            logger.warning("[Transcriber] No local backend configured")

    async def transcribe(
        self,
        audio_path: str,
        duration_seconds: float,
        importance: str = "normal"
    ) -> TranscriptionResult:
        """
        ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë©”ì¸ ì§„ì…ì )

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ)
            duration_seconds: ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ) - ffprobeë¡œ ë¯¸ë¦¬ ê³„ì‚°
            importance: íšŒì˜ ì¤‘ìš”ë„ ("normal" | "high")
                - "high": hybrid_quality ëª¨ë“œ ê°•ì œ ì ìš© ê°€ëŠ¥

        Returns:
            TranscriptionResult: ë³€í™˜ ê²°ê³¼

        Raises:
            Exception: ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ
        """
        logger.info(
            f"[Transcriber] Start - mode={self.mode}, duration={duration_seconds:.1f}s, importance={importance}"
        )

        # ëª¨ë“œë³„ ë¶„ê¸°
        if self.mode == WhisperMode.openai:
            return await self._openai_only(audio_path)

        if self.mode == WhisperMode.local:
            return await self._local_only(audio_path)

        if self.mode == WhisperMode.hybrid_cost:
            return await self._hybrid_cost(audio_path, duration_seconds)

        if self.mode == WhisperMode.hybrid_quality:
            return await self._hybrid_quality(audio_path, duration_seconds)

        # fallback (ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ)
        logger.warning(f"[Transcriber] Unknown WHISPER_MODE={self.mode}, using openai")
        return await self._openai_only(audio_path)

    # ============================================================================
    # ë‚´ë¶€ ì „ëµ êµ¬í˜„ (4ê°€ì§€ ëª¨ë“œ)
    # ============================================================================

    async def _openai_only(self, audio_path: str) -> TranscriptionResult:
        """
        ëª¨ë“œ 1: OpenAI Whisper APIë§Œ ì‚¬ìš©

        - ìš©ë„: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë¡œì»¬ ì„œë²„ ë¶ˆê°€ í™˜ê²½
        - ë¹„ìš©: $0.006/ë¶„
        """
        logger.info("[Transcriber] Mode: openai_only")
        return await self.openai_client.transcribe(audio_path)

    async def _local_only(self, audio_path: str) -> TranscriptionResult:
        """
        ëª¨ë“œ 2: ë¡œì»¬ ì„œë²„ë§Œ ì‚¬ìš© (faster-whisper ë˜ëŠ” whisper.cpp)

        - ìš©ë„: ë¹„ìš© ì ˆê°, ë°ì´í„° í”„ë¼ì´ë²„ì‹œ
        - Fallback: ë¡œì»¬ ì‹¤íŒ¨ ì‹œ OpenAIë¡œ ì „í™˜ (ì„ íƒì )
        """
        logger.info(
            f"[Transcriber] Mode: local_only (backend={self.settings.WHISPER_LOCAL_BACKEND})"
        )

        if not self.local_client:
            logger.warning("[Transcriber] No local_client, fallback to openai")
            return await self._openai_only(audio_path)

        return await self._with_retries(
            self.local_client,
            audio_path,
            fallback_openai=True,  # ë¡œì»¬ ì‹¤íŒ¨ ì‹œ OpenAI fallback
        )

    async def _hybrid_cost(
        self,
        audio_path: str,
        duration_seconds: float
    ) -> TranscriptionResult:
        """
        ëª¨ë“œ 3: ë¹„ìš©/ì†ë„ ê· í˜• (Hybrid - Cost Optimized)

        ì „ëµ:
        1. ì§§ì€ íšŒì˜ (<= WHISPER_OPENAI_MAX_MINUTES):
           - OpenAI ìš°ì„  (ë¹ ë¥¸ ì‘ë‹µ)
           - ì‹¤íŒ¨ ì‹œ ë¡œì»¬ fallback

        2. ê¸´ íšŒì˜ (> WHISPER_OPENAI_MAX_MINUTES):
           - ë¡œì»¬ ìš°ì„  (ë¹„ìš© ì ˆê°)
           - ì‹¤íŒ¨ ì‹œ OpenAI fallback

        ì˜ˆì‹œ:
        - WHISPER_OPENAI_MAX_MINUTES=20 ì„¤ì • ì‹œ
        - 15ë¶„ íšŒì˜: OpenAI ì‚¬ìš© ($0.09)
        - 60ë¶„ íšŒì˜: ë¡œì»¬ ì‚¬ìš© (ë¬´ë£Œ)
        """
        max_minutes = self.settings.WHISPER_OPENAI_MAX_MINUTES
        duration_minutes = duration_seconds / 60

        # 1) ì§§ì€ íšŒì˜ â†’ OpenAI ìš°ì„ 
        if duration_seconds <= max_minutes * 60:
            logger.info(
                f"[Transcriber] hybrid_cost: short meeting ({duration_minutes:.1f}min) â†’ openai first"
            )
            try:
                return await self.openai_client.transcribe(audio_path)
            except Exception as e:
                logger.exception(
                    f"[Transcriber] openai failed in hybrid_cost, try local: {e}"
                )
                if self.local_client:
                    return await self._with_retries(
                        self.local_client,
                        audio_path,
                        fallback_openai=False,  # ì´ë¯¸ OpenAI ì‹œë„í–ˆìœ¼ë¯€ë¡œ fallback ì—†ìŒ
                    )
                raise

        # 2) ê¸´ íšŒì˜ â†’ ë¡œì»¬ ìš°ì„ 
        logger.info(
            f"[Transcriber] hybrid_cost: long meeting ({duration_minutes:.1f}min) â†’ "
            f"local first (backend={self.settings.WHISPER_LOCAL_BACKEND})"
        )
        if self.local_client:
            return await self._with_retries(
                self.local_client,
                audio_path,
                fallback_openai=True,  # ë¡œì»¬ ì‹¤íŒ¨ ì‹œ OpenAI fallback
            )

        logger.warning("[Transcriber] hybrid_cost: no local_client, fallback to openai")
        return await self._openai_only(audio_path)

    async def _hybrid_quality(
        self,
        audio_path: str,
        duration_seconds: float
    ) -> TranscriptionResult:
        """
        ëª¨ë“œ 4: í’ˆì§ˆ ìš°ì„  (Hybrid - Quality Optimized)

        ì „ëµ:
        - ê¸¸ì´ì— ìƒê´€ì—†ì´ ë¡œì»¬ large-v3 ëª¨ë¸ ìš°ì„ 
        - ë¡œì»¬ ì‹¤íŒ¨ ì‹œ OpenAI fallback
        - ê¸¸ì´ì— ë”°ë¼ ëª¨ë¸ í”„ë¡œí•„ ìë™ ì„ íƒ:
          - â‰¤15ë¶„: FAST (small)
          - 15~60ë¶„: BALANCED (medium)
          - â‰¥60ë¶„: ACCURATE (large-v3)

        ìš©ë„:
        - ì¤‘ìš” íšŒì˜ (ê²½ì˜ì§„, ê³ ê° ë¯¸íŒ… ë“±)
        - ì •í™•ë„ê°€ ë¹„ìš©ë³´ë‹¤ ì¤‘ìš”í•œ ê²½ìš°
        """
        logger.info("[Transcriber] Mode: hybrid_quality (prefer local large model)")

        if self.local_client:
            try:
                # ê¸¸ì´ ê¸°ë°˜ ëª¨ë¸ í”„ë¡œí•„ ì„ íƒ
                model_profile = self._choose_model_profile(duration_seconds)
                logger.info(f"[Transcriber] Selected model profile: {model_profile}")

                return await self._with_retries(
                    self.local_client,
                    audio_path,
                    fallback_openai=True,
                    model_profile=model_profile,
                )
            except Exception as e:
                logger.exception(
                    f"[Transcriber] local failed in hybrid_quality, fallback to openai: {e}"
                )
                return await self._openai_only(audio_path)

        logger.warning("[Transcriber] hybrid_quality: no local_client, using openai")
        return await self._openai_only(audio_path)

    # ============================================================================
    # í—¬í¼ ë©”ì„œë“œ
    # ============================================================================

    async def _with_retries(
        self,
        client: BaseWhisperClient,
        audio_path: str,
        fallback_openai: bool = False,
        **kwargs,
    ) -> TranscriptionResult:
        """
        Retry ë¡œì§ with Graceful Degradation

        Args:
            client: Whisper í´ë¼ì´ì–¸íŠ¸ (local ë˜ëŠ” openai)
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            fallback_openai: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ OpenAI fallback ì—¬ë¶€
            **kwargs: í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°
                - model_profile: str
                - language: str

        Returns:
            TranscriptionResult: ë³€í™˜ ê²°ê³¼

        Raises:
            Exception: fallback_openai=Falseì´ê³  ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ
        """
        last_exc: Optional[Exception] = None
        max_retries = self.settings.WHISPER_MAX_RETRIES

        for attempt in range(max_retries):
            try:
                logger.info(
                    f"[Transcriber] Attempt {attempt + 1}/{max_retries} "
                    f"with {client.__class__.__name__}"
                )
                return await client.transcribe(audio_path, **kwargs)

            except Exception as e:
                last_exc = e
                logger.warning(
                    f"[Transcriber] Attempt {attempt + 1}/{max_retries} failed: {e}"
                )

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        if fallback_openai:
            logger.info("[Transcriber] All local attempts failed, fallback to openai")
            return await self._openai_only(audio_path)

        # fallback ì—†ì´ ì‹¤íŒ¨
        raise last_exc or RuntimeError("Transcriber: all attempts failed without fallback")

    def _choose_model_profile(self, duration_seconds: float) -> str:
        """
        íšŒì˜ ê¸¸ì´ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ í”„ë¡œí•„ ì„ íƒ

        ì „ëµ:
        - â‰¤15ë¶„: FAST (small) - ë¹ ë¥¸ ì‘ë‹µ ìš°ì„ 
        - 15~60ë¶„: BALANCED (medium) - ì†ë„/ì •í™•ë„ ê· í˜•
        - â‰¥60ë¶„: ACCURATE (large-v3) - ì •í™•ë„ ìš°ì„ 

        Args:
            duration_seconds: ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ)

        Returns:
            model_profile: "small" | "medium" | "large-v3"
        """
        duration_minutes = duration_seconds / 60

        if duration_seconds <= 15 * 60:
            profile = self.settings.WHISPER_PROFILE_FAST  # "small"
            logger.info(f"[Transcriber] Short meeting ({duration_minutes:.1f}min) â†’ {profile}")
            return profile

        if duration_seconds <= 60 * 60:
            profile = self.settings.WHISPER_PROFILE_BALANCED  # "medium"
            logger.info(f"[Transcriber] Medium meeting ({duration_minutes:.1f}min) â†’ {profile}")
            return profile

        profile = self.settings.WHISPER_PROFILE_ACCURATE  # "large-v3"
        logger.info(f"[Transcriber] Long meeting ({duration_minutes:.1f}min) â†’ {profile}")
        return profile
```

### 3-3. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `TranscriberService` í´ë˜ìŠ¤ ì‘ì„±
- [ ] 4ê°€ì§€ ëª¨ë“œ êµ¬í˜„ (`_openai_only`, `_local_only`, `_hybrid_cost`, `_hybrid_quality`)
- [ ] Retry ë¡œì§ êµ¬í˜„ (`_with_retries`)
- [ ] ëª¨ë¸ í”„ë¡œí•„ ìë™ ì„ íƒ ë¡œì§ êµ¬í˜„ (`_choose_model_profile`)
- [ ] í™˜ê²½ë³€ìˆ˜ í™•ì¸: `WHISPER_MODE`, `WHISPER_LOCAL_BACKEND`, `WHISPER_OPENAI_MAX_MINUTES`
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ëª¨ë“œë³„ ë™ì‘ ê²€ì¦
- [ ] í†µí•© í…ŒìŠ¤íŠ¸: ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì—”ë“œíˆ¬ì—”ë“œ í…ŒìŠ¤íŠ¸

---

## 4. Step 3: API ì—”ë“œí¬ì¸íŠ¸ ì—…ë°ì´íŠ¸

### 4-1. íŒŒì¼ ìœ„ì¹˜
**ê²½ë¡œ**: `backend/app/api/routes/meetings.py`

### 4-2. Request/Response ìŠ¤í‚¤ë§ˆ

```python
# app/schemas/transcriber.py (ì¼ë¶€)
from pydantic import BaseModel, Field
from typing import Optional, List

class TranscribeRequest(BaseModel):
    """
    POST /meetings/{id}/transcribe ìš”ì²­ ìŠ¤í‚¤ë§ˆ
    """
    force_mode: Optional[str] = Field(
        None,
        description="ê°•ì œ ëª¨ë“œ ì§€ì • (openai | local | hybrid_cost | hybrid_quality)",
        examples=["hybrid_cost"]
    )
    reprocess: bool = Field(
        False,
        description="ê¸°ì¡´ transcript ë¬´ì‹œí•˜ê³  ì¬ì²˜ë¦¬ ì—¬ë¶€"
    )
    importance: str = Field(
        "normal",
        description="íšŒì˜ ì¤‘ìš”ë„ (normal | high)",
        examples=["high"]
    )
    run_meeting_agent: bool = Field(
        True,
        description="ë³€í™˜ ì™„ë£Œ í›„ MeetingAgent ìë™ ì‹¤í–‰ ì—¬ë¶€"
    )


class TranscribeResponse(BaseModel):
    """
    POST /meetings/{id}/transcribe ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
    """
    meeting_id: int
    transcript_id: int
    source_type: str  # "whisper"
    backend: str  # "faster_whisper" | "whisper_cpp" | "openai"
    model: str  # "large-v3" | "medium" | "small" | "whisper-1"
    language: str  # "ko" | "en" | ...
    duration_seconds: float
    latency_ms: int
    is_primary: bool
    status: str  # "completed" | "failed"
    meeting_agent_triggered: bool
```

### 4-3. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„

```python
# app/api/routes/meetings.py
"""
Meeting ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ì‘ì„±ì¼: 2025-11-24
ì‘ì„±ì: BíŒ€ (Backend)
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.session import get_db
from app.models.meeting import Meeting
from app.models.meeting_transcript import MeetingTranscript
from app.schemas.transcriber import (
    TranscribeRequest,
    TranscribeResponse,
    TranscriptionResult,
)
from app.services.transcriber import TranscriberService
from app.services.meeting_audio import get_meeting_audio_source
from app.services.meeting_agent import run_meeting_agent_for_meeting
from app.core.settings import WhisperMode

router = APIRouter()


@router.post("/meetings/{meeting_id}/transcribe", response_model=TranscribeResponse)
async def transcribe_meeting(
    meeting_id: int,
    body: TranscribeRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
):
    """
    Meeting ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Flow:
    1. Meeting ì¡´ì¬ í™•ì¸
    2. ì˜¤ë””ì˜¤ ì†ŒìŠ¤ í™•ë³´ (ê²½ë¡œ + duration)
    3. TranscriberServiceë¡œ STT ì‹¤í–‰
    4. meeting_transcripts í…Œì´ë¸”ì— ì €ì¥
    5. (ì˜µì…˜) MeetingAgent ë¹„ë™ê¸° ì‹¤í–‰

    Args:
        meeting_id: Meeting ID
        body: TranscribeRequest
        background_tasks: FastAPI BackgroundTasks
        db: DB ì„¸ì…˜

    Returns:
        TranscribeResponse: ë³€í™˜ ê²°ê³¼ ë©”íƒ€ë°ì´í„°

    Raises:
        404: Meeting not found
        400: No audio source
        500: Transcription failed
    """
    # 1. Meeting ì¡´ì¬ í™•ì¸
    meeting = await Meeting.get_by_id(db, meeting_id)
    if not meeting:
        raise HTTPException(status_code=404, detail="Meeting not found")

    # 2. ì˜¤ë””ì˜¤ ì†ŒìŠ¤ í™•ë³´
    # TODO: get_meeting_audio_source êµ¬í˜„ í•„ìš”
    # - YouTube URL â†’ yt-dlpë¡œ ë‹¤ìš´ë¡œë“œ â†’ ë¡œì»¬ ê²½ë¡œ ë°˜í™˜
    # - ì—…ë¡œë“œ íŒŒì¼ â†’ MinIOì—ì„œ ë‹¤ìš´ë¡œë“œ â†’ ë¡œì»¬ ê²½ë¡œ ë°˜í™˜
    # - ffprobeë¡œ duration ê³„ì‚°
    audio_path, duration_seconds = await get_meeting_audio_source(db, meeting)
    if not audio_path:
        raise HTTPException(status_code=400, detail="No audio source for this meeting")

    # 3. TranscriberService ì¤€ë¹„
    transcriber = TranscriberService()
    original_mode = transcriber.mode

    # force_mode ë˜ëŠ” importanceë¡œ ëª¨ë“œ ì˜¤ë²„ë¼ì´ë“œ
    if body.force_mode:
        try:
            transcriber.mode = WhisperMode(body.force_mode)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid force_mode: {body.force_mode}. "
                       "Must be one of: openai, local, hybrid_cost, hybrid_quality"
            )
    elif body.importance == "high":
        # ì¤‘ìš” íšŒì˜ëŠ” í’ˆì§ˆ ìš°ì„  ëª¨ë“œ ê°•ì œ
        transcriber.mode = WhisperMode.hybrid_quality

    # 4. STT ì‹¤í–‰
    try:
        result: TranscriptionResult = await transcriber.transcribe(
            audio_path=audio_path,
            duration_seconds=duration_seconds,
            importance=body.importance,
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Transcription failed: {str(e)}"
        )
    finally:
        # ëª¨ë“œ ë³µì›
        transcriber.mode = original_mode

    # 5. ê¸°ì¡´ primary transcript ì²˜ë¦¬
    if body.reprocess:
        # ì¬ì²˜ë¦¬ ëª¨ë“œ: ê¸°ì¡´ primary í”Œë˜ê·¸ ì œê±°
        await MeetingTranscript.clear_primary_for_meeting(db, meeting_id)

    # 6. meeting_transcripts í…Œì´ë¸”ì— ì €ì¥
    transcript = MeetingTranscript(
        meeting_id=meeting_id,
        source_type="whisper",
        provider=meeting.provider or "upload",  # "youtube" | "zoom" | "upload"
        backend=result.backend,  # "faster_whisper" | "openai" | ...
        model=result.model,  # "large-v3" | "whisper-1" | ...
        language=result.language,  # "ko" | "en" | ...
        text=result.text,  # ì „ì²´ í…ìŠ¤íŠ¸
        segments_json=[s.dict() for s in result.segments],  # ì„¸ê·¸ë¨¼íŠ¸ ë°°ì—´
        duration_seconds=result.duration_seconds,
        latency_ms=result.latency_ms,
        confidence=result.confidence,
        quality_score=None,  # TODO: í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì¶”í›„)
        is_primary=True,  # ìƒˆë¡œ ìƒì„±í•œ transcriptë¥¼ primaryë¡œ ì„¤ì •
    )
    db.add(transcript)
    await db.commit()
    await db.refresh(transcript)

    # 7. MeetingAgent ë¹„ë™ê¸° ì‹¤í–‰ (Background Task)
    meeting_agent_triggered = False
    if body.run_meeting_agent:
        # TODO: run_meeting_agent_for_meeting êµ¬í˜„ í•„ìš”
        # - transcript.textë¥¼ MeetingAgentì— ì „ë‹¬
        # - ìš”ì•½, ì•¡ì…˜ ì•„ì´í…œ, ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
        # - meeting_summaries í…Œì´ë¸”ì— ì €ì¥
        background_tasks.add_task(
            run_meeting_agent_for_meeting,
            meeting_id=meeting_id
        )
        meeting_agent_triggered = True

    # 8. ì‘ë‹µ ìƒì„±
    return TranscribeResponse(
        meeting_id=meeting_id,
        transcript_id=transcript.id,
        source_type=transcript.source_type,
        backend=transcript.backend,
        model=transcript.model,
        language=transcript.language,
        duration_seconds=float(transcript.duration_seconds),
        latency_ms=transcript.latency_ms,
        is_primary=transcript.is_primary,
        status="completed",
        meeting_agent_triggered=meeting_agent_triggered,
    )
```

### 4-4. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `TranscribeRequest`, `TranscribeResponse` ìŠ¤í‚¤ë§ˆ ì •ì˜
- [ ] `POST /meetings/{id}/transcribe` ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [ ] `get_meeting_audio_source` í—¬í¼ í•¨ìˆ˜ êµ¬í˜„ (YouTube/ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬)
- [ ] `MeetingTranscript.clear_primary_for_meeting` ë©”ì„œë“œ êµ¬í˜„
- [ ] `run_meeting_agent_for_meeting` Background Task êµ¬í˜„ (ë˜ëŠ” ê¸°ì¡´ ì½”ë“œ í™œìš©)
- [ ] API í…ŒìŠ¤íŠ¸: Postman/curlë¡œ ìš”ì²­ â†’ ì‘ë‹µ ê²€ì¦
- [ ] ì—ëŸ¬ í•¸ë“¤ë§: 404, 400, 500 ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸

---

## 5. A/CíŒ€ í˜‘ì—… ê°€ì´ë“œ

### 5-1. AíŒ€ (QA) - Golden Set ì„¤ê³„ í¬ì¸íŠ¸

AíŒ€ì´ í…ŒìŠ¤íŠ¸í•´ì•¼ í•  í•µì‹¬ ì‹œë‚˜ë¦¬ì˜¤:

#### í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ 1: ëª¨ë“œë³„ ë™ì‘ ê²€ì¦

| í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ | ì…ë ¥ | ê¸°ëŒ€ ê²°ê³¼ |
|------------|------|---------|
| `test_openai_mode` | `WHISPER_MODE=openai`, 15ë¶„ ì˜¤ë””ì˜¤ | `backend="openai"`, `model="whisper-1"` |
| `test_local_mode` | `WHISPER_MODE=local`, 15ë¶„ ì˜¤ë””ì˜¤ | `backend="faster_whisper"`, `model="large-v3"` |
| `test_hybrid_cost_short` | `WHISPER_MODE=hybrid_cost`, 10ë¶„ ì˜¤ë””ì˜¤ | OpenAI ìš°ì„  ì‚¬ìš© |
| `test_hybrid_cost_long` | `WHISPER_MODE=hybrid_cost`, 90ë¶„ ì˜¤ë””ì˜¤ | ë¡œì»¬ ìš°ì„  ì‚¬ìš© |
| `test_hybrid_quality` | `WHISPER_MODE=hybrid_quality`, 60ë¶„ ì˜¤ë””ì˜¤ | ë¡œì»¬ large-v3 ì‚¬ìš© |

#### í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ 2: Fallback & Retry ê²€ì¦

| í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ | ì…ë ¥ | ê¸°ëŒ€ ê²°ê³¼ |
|------------|------|---------|
| `test_local_failure_fallback` | ë¡œì»¬ ì„œë²„ ë‹¤ìš´ + `hybrid_cost` | OpenAI fallback ì„±ê³µ |
| `test_retry_3_times` | ë¡œì»¬ Timeout 3íšŒ | 3íšŒ ì¬ì‹œë„ í›„ OpenAI fallback |
| `test_no_fallback_mode` | `WHISPER_MODE=local`, ë¡œì»¬ ë‹¤ìš´, fallback ë¹„í™œì„±í™” | ì—ëŸ¬ ë°œìƒ |

#### í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ 3: DB ì €ì¥ ê²€ì¦

| í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ | ê²€ì¦ í•­ëª© |
|------------|---------|
| `test_transcript_saved` | `meeting_transcripts` í…Œì´ë¸”ì— ë ˆì½”ë“œ ìƒì„± |
| `test_primary_flag` | `is_primary=True` ì„¤ì • |
| `test_reprocess_clears_primary` | `reprocess=true` ì‹œ ê¸°ì¡´ primary í”Œë˜ê·¸ ì œê±° |
| `test_segments_json_format` | `segments_json` í•„ë“œ í˜•ì‹ ê²€ì¦ |

#### í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ 4: API ê³„ì•½ ê²€ì¦

```python
# tests/test_transcribe_api.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_transcribe_api_success(client: AsyncClient):
    """ì„±ê³µ ì¼€ì´ìŠ¤: 15ë¶„ YouTube íšŒì˜ ë³€í™˜"""
    response = await client.post(
        "/api/v1/meetings/123/transcribe",
        json={
            "force_mode": "hybrid_cost",
            "reprocess": False,
            "importance": "normal",
            "run_meeting_agent": True
        }
    )
    assert response.status_code == 200
    data = response.json()
    assert data["meeting_id"] == 123
    assert data["backend"] in ["faster_whisper", "openai"]
    assert data["status"] == "completed"
    assert data["meeting_agent_triggered"] is True

@pytest.mark.asyncio
async def test_transcribe_api_invalid_mode(client: AsyncClient):
    """ì—ëŸ¬ ì¼€ì´ìŠ¤: ì˜ëª»ëœ force_mode"""
    response = await client.post(
        "/api/v1/meetings/123/transcribe",
        json={"force_mode": "invalid_mode"}
    )
    assert response.status_code == 400
    assert "Invalid force_mode" in response.json()["detail"]
```

### 5-2. CíŒ€ (Frontend) - API ì—°ë™ ê°€ì´ë“œ

#### TypeScript íƒ€ì… ì •ì˜

```typescript
// types/transcriber.ts
export interface TranscribeRequest {
  force_mode?: 'openai' | 'local' | 'hybrid_cost' | 'hybrid_quality';
  reprocess?: boolean;
  importance?: 'normal' | 'high';
  run_meeting_agent?: boolean;
}

export interface TranscribeResponse {
  meeting_id: number;
  transcript_id: number;
  source_type: string;
  backend: string;  // "faster_whisper" | "openai" | ...
  model: string;  // "large-v3" | "whisper-1" | ...
  language: string;
  duration_seconds: number;
  latency_ms: number;
  is_primary: boolean;
  status: 'completed' | 'failed';
  meeting_agent_triggered: boolean;
}
```

#### API í˜¸ì¶œ ì˜ˆì‹œ (React)

```typescript
// hooks/useTranscribeMeeting.ts
import { useMutation } from '@tanstack/react-query';
import { TranscribeRequest, TranscribeResponse } from '@/types/transcriber';

export function useTranscribeMeeting(meetingId: number) {
  return useMutation<TranscribeResponse, Error, TranscribeRequest>({
    mutationFn: async (request: TranscribeRequest) => {
      const response = await fetch(
        `/api/v1/meetings/${meetingId}/transcribe`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(request),
        }
      );

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.detail || 'Transcription failed');
      }

      return response.json();
    },
  });
}

// ì»´í¬ë„ŒíŠ¸ì—ì„œ ì‚¬ìš©
function MeetingDetailPage({ meetingId }: { meetingId: number }) {
  const { mutate: transcribe, isLoading, isError, error, data } =
    useTranscribeMeeting(meetingId);

  const handleTranscribe = () => {
    transcribe({
      force_mode: 'hybrid_cost',
      importance: 'high',
      run_meeting_agent: true,
    });
  };

  return (
    <div>
      <button onClick={handleTranscribe} disabled={isLoading}>
        {isLoading ? 'Transcribing...' : 'Start Transcription'}
      </button>

      {isError && <div className="error">{error.message}</div>}

      {data && (
        <div className="result">
          <p>Backend: {data.backend}</p>
          <p>Model: {data.model}</p>
          <p>Duration: {data.duration_seconds}s</p>
          <p>Latency: {data.latency_ms}ms</p>
        </div>
      )}
    </div>
  );
}
```

#### UI ì»´í¬ë„ŒíŠ¸ ê°€ì´ë“œ

CíŒ€ì´ êµ¬í˜„í•  UI ìš”ì†Œ:

1. **Transcribe ë²„íŠ¼** (Meeting ìƒì„¸ í˜ì´ì§€)
   - ìœ„ì¹˜: Meeting ìƒì„¸ í˜ì´ì§€ ìƒë‹¨
   - ìƒíƒœ: ë¡œë”©/ì™„ë£Œ/ì—ëŸ¬
   - ì˜µì…˜: ì¤‘ìš”ë„ ì„ íƒ (normal/high)

2. **Transcript ë·°ì–´** (ë³€í™˜ ì™„ë£Œ í›„)
   - ì „ì²´ í…ìŠ¤íŠ¸ í‘œì‹œ
   - ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ
   - ë©”íƒ€ë°ì´í„° (backend, model, duration, latency)

3. **ë””ë²„ê·¸ íŒ¨ë„** (ê°œë°œììš©)
   - ì‚¬ìš©ëœ ë°±ì—”ë“œ í‘œì‹œ
   - ëª¨ë¸ í”„ë¡œí•„ í‘œì‹œ
   - ë³€í™˜ ì†Œìš” ì‹œê°„ (latency_ms)

---

## 6. ë‹¤ìŒ ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 6-1. BíŒ€ êµ¬í˜„ ìˆœì„œ

```
Phase 1: ê¸°ë³¸ êµ¬í˜„ (2-3ì¼)
â”œâ”€â”€ [x] ìŠ¤í™ ë¬¸ì„œ ì‘ì„± ì™„ë£Œ (MEETING_AI_TRANSCRIBER_SPEC.md)
â”œâ”€â”€ [ ] transcriber_clients.py êµ¬í˜„ (FasterWhisperClient ìš°ì„ )
â”œâ”€â”€ [ ] transcriber.py êµ¬í˜„ (4-Mode ë¡œì§)
â””â”€â”€ [ ] meetings.py ì—”ë“œí¬ì¸íŠ¸ ì—…ë°ì´íŠ¸

Phase 2: í†µí•© í…ŒìŠ¤íŠ¸ (1-2ì¼)
â”œâ”€â”€ [ ] RTX Desktop ì„œë²„ ì—°ë™ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ [ ] ì‹¤ì œ YouTube íšŒì˜ ë³€í™˜ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ [ ] ëª¨ë“œë³„ ë™ì‘ ê²€ì¦
â””â”€â”€ [ ] Fallback/Retry ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

Phase 3: A/CíŒ€ ì—°ë™ (1ì¼)
â”œâ”€â”€ [ ] AíŒ€: Golden Set ê¸°ë°˜ íšŒê·€ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ [ ] CíŒ€: Meeting ìƒì„¸ í˜ì´ì§€ UI ì—°ë™
â””â”€â”€ [ ] E2E í…ŒìŠ¤íŠ¸ (í”„ë¡ íŠ¸ â†’ ë°±ì—”ë“œ â†’ RTX Desktop)

Phase 4: Mac mini ë°°í¬ (1ì¼)
â”œâ”€â”€ [ ] Mac mini ì„œë²„ì— ì½”ë“œ ë™ê¸°í™”
â”œâ”€â”€ [ ] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (`WHISPER_MODE=hybrid_cost`)
â”œâ”€â”€ [ ] Docker Compose ì¬ì‹œì‘
â””â”€â”€ [ ] Health Check ë° Smoke Test
```

### 6-2. í•„ìˆ˜ TODO í•­ëª©

#### ìš°ì„ ìˆœìœ„ 1 (ë¸”ë¡œí‚¹)

- [ ] **RTX Desktop faster-whisper ì„œë²„ êµ¬ë™ í™•ì¸**
  - ëª…ë ¹ì–´: `curl http://100.120.180.42:9000/health`
  - ê¸°ëŒ€ ê²°ê³¼: `{"status": "ok"}`

- [ ] **í™˜ê²½ë³€ìˆ˜ ì„¤ì • (backend/.env)**
  ```bash
  WHISPER_MODE=hybrid_cost
  WHISPER_LOCAL_BACKEND=faster_whisper
  WHISPER_FAST_ENDPOINT=http://100.120.180.42:9000/transcribe
  WHISPER_OPENAI_MAX_MINUTES=20
  WHISPER_MAX_RETRIES=3
  WHISPER_TIMEOUT_SECONDS=300
  WHISPER_PROFILE_FAST=small
  WHISPER_PROFILE_BALANCED=medium
  WHISPER_PROFILE_ACCURATE=large-v3
  ```

- [ ] **FasterWhisperClient ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
  ```bash
  cd backend
  pytest tests/test_transcriber_clients.py::test_faster_whisper_client -v
  ```

#### ìš°ì„ ìˆœìœ„ 2 (ì¤‘ìš”)

- [ ] **TranscriberService ëª¨ë“œë³„ í…ŒìŠ¤íŠ¸**
  ```bash
  pytest tests/test_transcriber_service.py -v
  ```

- [ ] **API ì—”ë“œí¬ì¸íŠ¸ í†µí•© í…ŒìŠ¤íŠ¸**
  ```bash
  pytest tests/test_transcribe_api.py -v
  ```

- [ ] **ì‹¤ì œ YouTube íšŒì˜ë¡œ E2E í…ŒìŠ¤íŠ¸**
  - í…ŒìŠ¤íŠ¸ URL: [ì§§ì€ íšŒì˜ ì˜ˆì‹œ](https://www.youtube.com/watch?v=dQw4w9WgXcQ)
  - ì˜ˆìƒ ì†Œìš” ì‹œê°„: 10-20ì´ˆ

#### ìš°ì„ ìˆœìœ„ 3 (ì„ íƒ)

- [ ] **OpenAI Whisper API ì—°ë™ êµ¬í˜„** (OpenAIWhisperClient)
- [ ] **whisper.cpp ì„œë²„ ì—°ë™ êµ¬í˜„** (WhisperCppClient)
- [ ] **MeetingAgent ìë™ ì‹¤í–‰** (background task)
- [ ] **í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë¡œì§** (confidence â†’ quality_score)

### 6-3. ë°°í¬ ì „ í™•ì¸ì‚¬í•­

#### Mac mini ì„œë²„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Docker Compose ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™
  ```bash
  ssh woosun@100.123.51.5
  cd ~/sparklio_ai_marketing_studio/docker/mac-mini
  docker compose ps
  ```

- [ ] í™˜ê²½ë³€ìˆ˜ ë™ê¸°í™” í™•ì¸
  ```bash
  docker compose exec backend env | grep WHISPER
  ```

- [ ] Backend Health Check
  ```bash
  curl http://100.123.51.5:8000/health
  ```

- [ ] RTX Desktop ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
  ```bash
  curl http://100.120.180.42:9000/health
  ```

- [ ] PostgreSQL `meeting_transcripts` í…Œì´ë¸” í™•ì¸
  ```sql
  SELECT * FROM meeting_transcripts ORDER BY created_at DESC LIMIT 5;
  ```

---

## 7. ì°¸ê³  ìë£Œ

### 7-1. ê´€ë ¨ ë¬¸ì„œ

- [MEETING_AI_TRANSCRIBER_SPEC.md](./MEETING_AI_TRANSCRIBER_SPEC.md) - ìƒì„¸ ìŠ¤í™
- [MEETING_AI_ARCHITECTURE.md](../../docs/MEETING_AI_ARCHITECTURE.md) - ì „ì²´ ì•„í‚¤í…ì²˜
- [MAC_MINI_SERVER_GUIDELINES.md](../../docs/MAC_MINI_SERVER_GUIDELINES.md) - ì„œë²„ ìš´ì˜ ê°€ì´ë“œ
- [B_TEAM_HANDOVER_GUIDE_2025-11-23.md](./B_TEAM_HANDOVER_GUIDE_2025-11-23.md) - BíŒ€ ì¸ìˆ˜ì¸ê³„

### 7-2. ì™¸ë¶€ ë¦¬ì†ŒìŠ¤

- [faster-whisper GitHub](https://github.com/guillaumekln/faster-whisper)
- [OpenAI Whisper API ë¬¸ì„œ](https://platform.openai.com/docs/guides/speech-to-text)
- [whisper.cpp GitHub](https://github.com/ggerganov/whisper.cpp)

### 7-3. ì—°ë½ì²˜

- **BíŒ€ ë¦¬ë“œ**: Backend ë‹´ë‹¹ì
- **AíŒ€ QA**: QA ë‹´ë‹¹ì
- **RTX Desktop ê´€ë¦¬ì**: ì¸í”„ë¼ ë‹´ë‹¹ì

---

**ì‘ì„± ì™„ë£Œ**: 2025-11-24 (ì¼ìš”ì¼)
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: êµ¬í˜„ ì§„í–‰ ìƒí™©ì— ë”°ë¼ ìˆ˜ì‹œ ì—…ë°ì´íŠ¸
