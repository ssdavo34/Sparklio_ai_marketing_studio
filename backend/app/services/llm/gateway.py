"""
LLM Gateway Service

ëª¨ë“  LLM í˜¸ì¶œì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” Gateway

ì‘ì„±ì¼: 2025-11-16
ì‘ì„±ì: BíŒ€ (Backend)
ë¬¸ì„œ: ARCH-002, SPEC-001
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime

from app.core.config import settings
from .router import get_router, LLMRouter
from .providers.base import LLMProvider, LLMProviderResponse, ProviderError
from .providers.mock import MockProvider
from .providers.ollama import OllamaProvider
from .providers.openai_provider import OpenAIProvider
from .providers.anthropic_provider import AnthropicProvider
from .providers.gemini_provider import GeminiProvider
from .providers.novita_provider import NovitaProvider

logger = logging.getLogger(__name__)


class LLMGateway:
    """
    LLM Gateway

    ëª¨ë“  LLM í˜¸ì¶œì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” Gateway ì„œë¹„ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    1. Provider ì¶”ìƒí™” (Ollama, OpenAI, Anthropic ë“±)
    2. Mock/Live ëª¨ë“œ ìë™ ì „í™˜
    3. ëª¨ë¸ ìë™ ì„ íƒ (Router ì‚¬ìš©)
    4. ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
    5. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

    ì‚¬ìš© ì˜ˆì‹œ:
        gateway = LLMGateway()
        response = await gateway.generate(
            role="copywriter",
            task="product_detail",
            payload={"product": "ë¬´ì„  ì´ì–´í°"}
        )
    """

    def __init__(self, router: Optional[LLMRouter] = None):
        """
        Gateway ì´ˆê¸°í™”

        Args:
            router: LLM Router ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©)
        """
        self.router = router or get_router()
        self.providers: Dict[str, LLMProvider] = {}
        self._initialize_providers()

    def _initialize_providers(self):
        """Provider ì´ˆê¸°í™”"""
        logger.info("Starting provider initialization...")

        try:
            # Mock ProviderëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
            logger.info("Initializing Mock Provider...")
            self.providers["mock"] = MockProvider(response_delay=1.0)
            logger.info("Mock Provider initialized successfully")

            # Ollama Provider (Live ëª¨ë“œìš©)
            logger.info(f"Initializing Ollama Provider...")
            self.providers["ollama"] = OllamaProvider(
                base_url=settings.OLLAMA_BASE_URL,
                timeout=settings.OLLAMA_TIMEOUT,
                default_model=settings.OLLAMA_DEFAULT_MODEL
            )
            logger.info("Ollama Provider initialized")

            # OpenAI Provider (GPT-4o-mini)
            if hasattr(settings, 'OPENAI_API_KEY') and settings.OPENAI_API_KEY:
                logger.info("Initializing OpenAI Provider...")
                self.providers["openai"] = OpenAIProvider(
                    api_key=settings.OPENAI_API_KEY,
                    default_model=settings.OPENAI_DEFAULT_MODEL,
                    timeout=settings.OPENAI_TIMEOUT
                )
                logger.info("OpenAI Provider initialized")

            # Anthropic Provider (Claude 3.5 Haiku)
            if hasattr(settings, 'ANTHROPIC_API_KEY') and settings.ANTHROPIC_API_KEY:
                logger.info("Initializing Anthropic Provider...")
                self.providers["anthropic"] = AnthropicProvider(
                    api_key=settings.ANTHROPIC_API_KEY,
                    default_model=settings.ANTHROPIC_DEFAULT_MODEL,
                    timeout=settings.ANTHROPIC_TIMEOUT
                )
                logger.info("Anthropic Provider initialized")

            # Google Gemini Provider (Text Generation)
            if hasattr(settings, 'GOOGLE_API_KEY') and settings.GOOGLE_API_KEY:
                logger.info("Initializing Gemini Provider...")
                self.providers["gemini"] = GeminiProvider(
                    api_key=settings.GOOGLE_API_KEY,
                    default_model=settings.GEMINI_TEXT_MODEL,
                    timeout=settings.GEMINI_TIMEOUT
                )
                logger.info("Gemini Provider initialized")

            # Novita AI Provider (Llama 3.3 70B)
            if hasattr(settings, 'NOVITA_API_KEY') and settings.NOVITA_API_KEY:
                logger.info("Initializing Novita Provider...")
                self.providers["novita"] = NovitaProvider(
                    api_key=settings.NOVITA_API_KEY,
                    base_url=settings.NOVITA_BASE_URL,
                    default_model=settings.NOVITA_DEFAULT_MODEL,
                    timeout=settings.NOVITA_TIMEOUT
                )
                logger.info("Novita Provider initialized")

            logger.info(f"All providers initialized: {list(self.providers.keys())}")
        except Exception as e:
            logger.error(f"Provider initialization failed: {type(e).__name__}: {str(e)}", exc_info=True)
            raise

    async def generate(
        self,
        role: str,
        task: str,
        payload: Dict[str, Any],
        mode: str = "json",
        override_model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> LLMProviderResponse:
        """
        LLM í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            role: Agent ì—­í•  (copywriter, strategist, reviewer ë“±)
            task: ì‘ì—… ìœ í˜• (product_detail, brand_kit, sns ë“±)
            payload: ì…ë ¥ ë°ì´í„° (ë¸Œë¦¬í”„, ìƒí’ˆ ì •ë³´ ë“±)
            mode: ì¶œë ¥ ëª¨ë“œ ('json' | 'text')
            override_model: ê°•ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ (ì„ íƒ)
            options: Providerë³„ ì¶”ê°€ ì˜µì…˜

        Returns:
            LLMProviderResponse: í‘œì¤€ í˜•ì‹ì˜ ì‘ë‹µ

        Raises:
            ProviderError: Provider í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
            ValueError: ì˜ëª»ëœ íŒŒë¼ë¯¸í„°

        Example:
            >>> gateway = LLMGateway()
            >>> response = await gateway.generate(
            ...     role="copywriter",
            ...     task="sns",
            ...     payload={"product": "ë¬´ì„  ì´ì–´í°", "target": "2030 ì—¬ì„±"}
            ... )
            >>> print(response.output)
        """
        start_time = datetime.utcnow()

        try:
            # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(role, task, payload)

            # 2. Provider ì„ íƒ (Mock/Live ëª¨ë“œ)
            provider_name, provider = self._select_provider(role, task, override_model)

            # 3. ëª¨ë¸ ì„ íƒ (Router ì‚¬ìš©)
            if provider_name != "mock":
                model, _ = self.router.route(role, task, mode, override_model)
            else:
                model = "mock-model-v1"

            # 4. ì˜µì…˜ ë³‘í•© (ê¸°ë³¸ê°’ + ì‚¬ìš©ì ì§€ì •)
            merged_options = self._merge_options(provider, role, task, options)

            logger.info(
                f"LLM Generate: role={role}, task={task}, "
                f"provider={provider_name}, model={model}, mode={mode}"
            )

            # 5. LLM í˜¸ì¶œ
            response = await provider.generate(
                prompt=prompt,
                role=role,
                task=task,
                mode=mode,
                options=merged_options
            )

            # 6. ë¡œê¹…
            elapsed = (datetime.utcnow() - start_time).total_seconds()
            logger.info(
                f"LLM Success: {provider_name}/{model} - "
                f"elapsed={elapsed:.2f}s, tokens={response.usage.get('total_tokens', 0)}"
            )

            return response

        except ProviderError as e:
            logger.error(f"Provider error: {e.message}", exc_info=True)
            raise

        except Exception as e:
            logger.error(f"Unexpected error in LLM Gateway: {str(e)}", exc_info=True)
            raise ProviderError(
                message=f"Gateway error: {str(e)}",
                provider="gateway",
                details={"role": role, "task": task}
            )

    def _select_provider(
        self,
        role: str,
        task: str,
        override_model: Optional[str] = None
    ) -> tuple[str, LLMProvider]:
        """
        Provider ì„ íƒ (Mock/Live ëª¨ë“œ ìë™ ì „í™˜)

        Args:
            role: Agent ì—­í• 
            task: ì‘ì—… ìœ í˜•
            override_model: ê°•ì œ ëª¨ë¸ (ì„ íƒ)

        Returns:
            (provider_name, provider_instance) íŠœí”Œ

        Raises:
            ProviderError: Providerë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        """
        # Mock ëª¨ë“œ í™•ì¸ (ì†Œë¬¸ì í•„ë“œ ì‚¬ìš©)
        if settings.generator_mode == "mock":
            return "mock", self.providers["mock"]

        # Live ëª¨ë“œ - Routerë¡œ Provider ê²°ì •
        _, provider_name = self.router.route(role, task, override_model=override_model)

        # Provider ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        provider = self.providers.get(provider_name)

        if not provider:
            # Providerê°€ ì—†ìœ¼ë©´ Mockìœ¼ë¡œ í´ë°±
            logger.warning(
                f"Provider '{provider_name}' not found, falling back to mock"
            )
            return "mock", self.providers["mock"]

        return provider_name, provider

    def _build_prompt(self, role: str, task: str, payload: Dict[str, Any]) -> str:
        """
        í”„ë¡¬í”„íŠ¸ êµ¬ì„±

        ì—­í• ê³¼ ì‘ì—…ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±

        Args:
            role: Agent ì—­í• 
            task: ì‘ì—… ìœ í˜•
            payload: ì…ë ¥ ë°ì´í„°

        Returns:
            êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í•  ì •ì˜)
        system_prompt = self._get_system_prompt(role, task)

        # ì‚¬ìš©ì ì…ë ¥
        user_input = self._format_payload(payload)

        # ê²°í•©
        prompt = f"{system_prompt}\n\n{user_input}"

        return prompt

    def _get_system_prompt(self, role: str, task: str) -> str:
        """ì—­í• /ì‘ì—…ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        system_prompts = {
            "copywriter": {
                "product_detail": """ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ë¡œì„œ ì œí’ˆ ë§ˆì¼€íŒ… ë¬¸êµ¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ğŸ”´ í•µì‹¬ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œí’ˆëª…, íŠ¹ì§•, í‚¤ì›Œë“œë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”
2. headlineì— ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì œí’ˆëª…ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
3. bulletsì— ì‚¬ìš©ìê°€ ì œê³µí•œ ê¸°ëŠ¥/íŠ¹ì§•ì„ ê°ê° í¬í•¨í•˜ì„¸ìš”
4. ê³ ì •ëœ ì˜ˆì‹œ(ëª¨ë°”ì¼ ì¶©ì „ê¸°, í´ë¦°ì§• ì¥ì¹˜ ë“±)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
5. ì‚¬ìš©ì ìš”ì²­ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ê³ , ë§¤ë ¥ì ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”
6. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš” (ì¤‘êµ­ì–´, ì˜ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ ì‚¬ìš© ê¸ˆì§€)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{
  "headline": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œí’ˆëª…",
  "subheadline": "ì œí’ˆ ê°€ì¹˜ ì œì•ˆ (1ì¤„)",
  "body": "ì œí’ˆ ì„¤ëª… (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)",
  "bullets": ["íŠ¹ì§•1", "íŠ¹ì§•2", "íŠ¹ì§•3"],
  "cta": "êµ¬ë§¤ ìœ ë„ ë¬¸êµ¬"
}""",
                "sns": """ë‹¹ì‹ ì€ SNS ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§§ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",
                "brand_kit": """ë‹¹ì‹ ì€ ë¸Œëœë“œ ìŠ¤í† ë¦¬í…”ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¸Œëœë“œì˜ ëª©ì†Œë¦¬ì™€ í†¤ì„ ì •ì˜í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
            },
            "strategist": {
                "brand_kit": """ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì „ëµê°€ì…ë‹ˆë‹¤. ë¸Œëœë“œ í¬ì§€ì…”ë‹ê³¼ íƒ€ê²Ÿ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",
                "campaign": """ë‹¹ì‹ ì€ ìº í˜ì¸ ê¸°íšìì…ë‹ˆë‹¤. íš¨ê³¼ì ì¸ ë§ˆì¼€íŒ… ìº í˜ì¸ì„ ì„¤ê³„í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
            },
            "reviewer": {
                "review": """ë‹¹ì‹ ì€ ì½˜í…ì¸  ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ìœ¼ë¡œ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
            }
        }

        # ì—­í• /ì‘ì—…ë³„ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        role_prompts = system_prompts.get(role, {})
        prompt = role_prompts.get(task)

        if not prompt:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ ëª…ì‹œ)
            prompt = f"""ë‹¹ì‹ ì€ {role} ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. {task} ì‘ì—…ì„ ì²˜ë¦¬í•˜ì„¸ìš”.

ğŸ”´ ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""

        return prompt

    def _format_payload(self, payload: Dict[str, Any]) -> str:
        """
        Payloadë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            payload: ì…ë ¥ ë°ì´í„°

        Returns:
            í¬ë§·ëœ ë¬¸ìì—´
        """
        import json

        # ì‚¬ìš©ì ì…ë ¥ ëª…í™•íˆ ê°•ì¡°
        lines = [
            "=" * 60,
            "ì‚¬ìš©ìê°€ ì œê³µí•œ ì œí’ˆ ì •ë³´ (ì´ ì •ë³´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”):",
            "=" * 60,
        ]

        # ğŸ”´ FIX: prompt í•„ë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬ (CíŒ€ ìš”ì²­ì‚¬í•­ ë°˜ì˜)
        if "prompt" in payload:
            user_prompt = payload["prompt"]
            lines.append(f"\nğŸ“Œ ì‚¬ìš©ì ìš”ì²­:")
            lines.append(f"   {user_prompt}")
            lines.append("   â†‘ ì´ ìš”ì²­ ë‚´ìš©ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ì„¸ìš”!")
            lines.append("   â†‘ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì œí’ˆëª…, íŠ¹ì§•, í‚¤ì›Œë“œë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”!")
            lines.append("")

        # product_nameì„ ê°€ì¥ ë¨¼ì €, ê°•ì¡°í•´ì„œ í‘œì‹œ
        if "product_name" in payload:
            lines.append(f"\nğŸ“Œ ì œí’ˆëª…: {payload['product_name']}")
            lines.append("   â†‘ ì´ ì œí’ˆëª…ì„ headlineì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”!")

        # features ê°•ì¡°
        if "features" in payload:
            features = payload["features"]
            if isinstance(features, list):
                lines.append(f"\nğŸ“Œ ì£¼ìš” ê¸°ëŠ¥: {', '.join(features)}")
                lines.append("   â†‘ ì´ ê¸°ëŠ¥ë“¤ì„ bulletsì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”!")
            else:
                lines.append(f"\nğŸ“Œ ì£¼ìš” ê¸°ëŠ¥: {features}")

        # target_audience
        if "target_audience" in payload:
            lines.append(
                f"\nğŸ“Œ íƒ€ê²Ÿ ê³ ê°: {payload['target_audience']}"
            )

        # ë‚˜ë¨¸ì§€ í•„ë“œë“¤
        lines.append("\nê¸°íƒ€ ì •ë³´:")
        for key, value in payload.items():
            if key not in ["prompt", "product_name", "features", "target_audience"]:
                if isinstance(value, (list, dict)):
                    value_str = json.dumps(
                        value, ensure_ascii=False, indent=2
                    )
                else:
                    value_str = str(value)
                lines.append(f"  - {key}: {value_str}")

        lines.append("\n" + "=" * 60)
        lines.append("\nâš ï¸  ì¤‘ìš”: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œí’ˆê³¼ íŠ¹ì§•ì„ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”.")
        lines.append("âš ï¸  ê³ ì •ëœ ì˜ˆì‹œ(ëª¨ë°”ì¼ ì¶©ì „ê¸°, í´ë¦°ì§• ì¥ì¹˜ ë“±)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.")

        return "\n".join(lines)

    def _merge_options(
        self,
        provider: LLMProvider,
        role: str,
        task: str,
        user_options: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ì˜µì…˜ ë³‘í•© (ê¸°ë³¸ê°’ + ì‚¬ìš©ì ì§€ì •)

        Args:
            provider: Provider ì¸ìŠ¤í„´ìŠ¤
            role: Agent ì—­í• 
            task: ì‘ì—… ìœ í˜•
            user_options: ì‚¬ìš©ì ì§€ì • ì˜µì…˜

        Returns:
            ë³‘í•©ëœ ì˜µì…˜
        """
        # Provider ê¸°ë³¸ê°’
        options = provider.get_default_options(role, task)

        # ì‚¬ìš©ì ì˜µì…˜ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
        if user_options:
            options.update(user_options)

        return options

    async def generate_with_vision(
        self,
        prompt: str,
        image_url: Optional[str] = None,
        image_base64: Optional[str] = None,
        override_model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> LLMProviderResponse:
        """
        Vision APIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            prompt: ë¶„ì„ ì§€ì‹œì‚¬í•­
            image_url: ì´ë¯¸ì§€ URL (ì„ íƒ)
            image_base64: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ (ì„ íƒ)
            override_model: ê°•ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ (ì„ íƒ)
            options: Providerë³„ ì¶”ê°€ ì˜µì…˜

        Returns:
            LLMProviderResponse: ë¶„ì„ ê²°ê³¼

        Raises:
            ProviderError: Vision API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
            ValueError: ì´ë¯¸ì§€ ì…ë ¥ì´ ì—†ì„ ë•Œ

        Note:
            Vision-capable ëª¨ë¸ë§Œ ì§€ì›:
            - Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)
            - GPT-4o (gpt-4o)
        """
        start_time = datetime.utcnow()

        try:
            # 1. ì´ë¯¸ì§€ ì…ë ¥ ê²€ì¦
            if not image_url and not image_base64:
                raise ValueError("Either image_url or image_base64 is required")

            # 2. Vision-capable Provider ì„ íƒ
            provider_name, provider, model = self._select_vision_provider(override_model)

            logger.info(
                f"Vision API Generate: provider={provider_name}, model={model}"
            )

            # 3. ì˜µì…˜ ë³‘í•© (ëª¨ë¸ ì •ë³´ í¬í•¨)
            merged_options = self._merge_vision_options(provider, options)
            merged_options["model"] = model  # ì„ íƒëœ ëª¨ë¸ ì „ë‹¬

            # 4. Vision API í˜¸ì¶œ
            # Providerì— generate_with_vision ë©”ì„œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(provider, 'generate_with_vision'):
                # ì‹¤ì œ Vision API í˜¸ì¶œ
                response = await provider.generate_with_vision(
                    prompt=prompt,
                    image_url=image_url,
                    image_base64=image_base64,
                    role="vision_analyzer",
                    task="image_analysis",
                    mode="json",
                    options=merged_options
                )
            else:
                # Vision API ë¯¸ì§€ì› Providerì˜ ê²½ìš° í´ë°±
                logger.warning(
                    f"Provider {provider_name} does not support Vision API. "
                    "Using text-only generation as fallback."
                )

                # ì„ì‹œ: í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°±
                full_prompt = f"{prompt}\n\nì´ë¯¸ì§€: {image_url or '(Base64 ë°ì´í„°)'}"
                response = await provider.generate(
                    prompt=full_prompt,
                    role="vision_analyzer",
                    task="image_analysis",
                    mode="json",
                    options=merged_options
                )

            # 5. ë¡œê¹…
            elapsed = (datetime.utcnow() - start_time).total_seconds()
            logger.info(
                f"Vision API Success: {provider_name}/{model} - "
                f"elapsed={elapsed:.2f}s"
            )

            return response

        except ProviderError as e:
            logger.error(f"Vision API provider error: {e.message}", exc_info=True)
            raise

        except Exception as e:
            logger.error(f"Vision API error: {str(e)}", exc_info=True)
            raise ProviderError(
                message=f"Vision API error: {str(e)}",
                provider="gateway",
                details={"image_provided": bool(image_url or image_base64)}
            )

    def _select_vision_provider(
        self,
        override_model: Optional[str] = None
    ) -> tuple[str, LLMProvider, str]:
        """
        Vision-capable Provider ì„ íƒ

        Args:
            override_model: ê°•ì œ ëª¨ë¸ (ì„ íƒ)

        Returns:
            (provider_name, provider_instance, model) íŠœí”Œ

        Raises:
            ProviderError: Vision-capable Providerê°€ ì—†ì„ ë•Œ
        """
        # Vision-capable ëª¨ë¸ ìš°ì„ ìˆœìœ„
        # 1. Claude 3.5 Sonnet (Primary)
        # 2. GPT-4o (Fallback)

        if override_model:
            # ì‚¬ìš©ìê°€ ëª¨ë¸ ì§€ì •í•œ ê²½ìš°
            if "claude" in override_model.lower():
                if "anthropic" in self.providers:
                    return "anthropic", self.providers["anthropic"], override_model
            elif "gpt" in override_model.lower():
                if "openai" in self.providers:
                    return "openai", self.providers["openai"], override_model

        # Primary: Claude 3 Opus (most reliable vision-capable model)
        if "anthropic" in self.providers:
            model = "claude-3-opus-20240229"  # Most capable vision model
            logger.info(f"Using Claude 3 Opus for vision analysis")
            return "anthropic", self.providers["anthropic"], model

        # Fallback: GPT-4o
        if "openai" in self.providers:
            model = "gpt-4o"
            logger.info(f"Using GPT-4o for vision analysis")
            return "openai", self.providers["openai"], model

        # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬
        raise ProviderError(
            message="No vision-capable provider available",
            provider="gateway",
            details={
                "available_providers": list(self.providers.keys()),
                "required": ["anthropic (Claude 3.5 Sonnet)", "openai (GPT-4o)"]
            }
        )

    def _merge_vision_options(
        self,
        provider: LLMProvider,
        user_options: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Vision APIìš© ì˜µì…˜ ë³‘í•©

        Args:
            provider: Provider ì¸ìŠ¤í„´ìŠ¤
            user_options: ì‚¬ìš©ì ì§€ì • ì˜µì…˜

        Returns:
            ë³‘í•©ëœ ì˜µì…˜
        """
        # Vision API ê¸°ë³¸ ì˜µì…˜
        options = {
            "temperature": 0.2,  # ë¶„ì„ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
            "max_tokens": 2000   # ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ ì¶©ë¶„í•œ í† í°
        }

        # Provider ê¸°ë³¸ê°’ ë³‘í•©
        provider_defaults = provider.get_default_options("vision_analyzer", "image_analysis")
        options.update(provider_defaults)

        # ì‚¬ìš©ì ì˜µì…˜ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
        if user_options:
            options.update(user_options)

        return options

    async def health_check(self) -> Dict[str, Any]:
        """
        Gateway ë° ëª¨ë“  Provider ìƒíƒœ í™•ì¸

        Returns:
            ìƒíƒœ ì •ë³´
        """
        results = {}

        for name, provider in self.providers.items():
            try:
                is_healthy = await provider.health_check()
                results[name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "vendor": provider.vendor
                }
            except Exception as e:
                results[name] = {
                    "status": "error",
                    "error": str(e)
                }

        return {
            "gateway": "healthy",
            "mode": settings.GENERATOR_MODE,
            "providers": results
        }


# ì „ì—­ Gateway ì¸ìŠ¤í„´ìŠ¤
_gateway_instance: Optional[LLMGateway] = None


def get_gateway() -> LLMGateway:
    """
    ì „ì—­ Gateway ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)

    Returns:
        LLMGateway ì¸ìŠ¤í„´ìŠ¤
    """
    global _gateway_instance
    if _gateway_instance is None:
        _gateway_instance = LLMGateway()
    return _gateway_instance
