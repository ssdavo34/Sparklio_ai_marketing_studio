"""
LLM Gateway API í…ŒìŠ¤íŠ¸ (AíŒ€ ê²€ì¦ìš©)

ì˜¬ë°”ë¥¸ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©: role, task, payload
"""
import httpx
import asyncio
import json


async def test_llm_health():
    """LLM Gateway Health Check"""
    print("=" * 60)
    print("Test 1: LLM Gateway Health Check")
    print("=" * 60)

    url = "http://localhost:8001/api/v1/llm/llm/health"

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(url)
            response.raise_for_status()

            result = response.json()
            print(f"\nâœ… Status: {response.status_code}")
            print(f"\nGateway: {result.get('gateway')}")
            print(f"Mode: {result.get('mode')}")
            print(f"\nProviders:")
            for name, info in result.get('providers', {}).items():
                print(f"  {name}: {info.get('status')} (vendor: {info.get('vendor')})")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def test_llm_json_mode():
    """JSON Mode í…ŒìŠ¤íŠ¸ - Copywriter Role"""
    print("\n" + "=" * 60)
    print("Test 2: LLM Generate - JSON Mode (Copywriter)")
    print("=" * 60)

    url = "http://localhost:8001/api/v1/llm/llm/generate"
    data = {
        "role": "copywriter",
        "task": "product_detail",
        "payload": {
            "product_name": "í”„ë¦¬ë¯¸ì—„ ë¬´ì„  ì´ì–´í°",
            "features": ["ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§", "24ì‹œê°„ ë°°í„°ë¦¬", "IPX7 ë°©ìˆ˜"],
            "target_audience": "2030 ì„¸ëŒ€"
        },
        "mode": "json",
        "options": {
            "temperature": 0.7
        }
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=data)
            response.raise_for_status()

            result = response.json()
            print(f"\nâœ… Status: {response.status_code}")
            print(f"\nProvider: {result['provider']}")
            print(f"Model: {result['model']}")
            print(f"\nOutput Type: {result['output']['type']}")
            
            if result['output']['type'] == 'json':
                print(f"Output Value: {json.dumps(result['output']['value'], indent=2, ensure_ascii=False)}")
            else:
                print(f"Output Value: {result['output']['value'][:200]}...")
            
            print(f"\nUsage: {result['usage']}")
            print(f"Meta: {result['meta']}")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def test_llm_text_mode():
    """Text Mode í…ŒìŠ¤íŠ¸ - Strategist Role"""
    print("\n" + "=" * 60)
    print("Test 3: LLM Generate - Text Mode (Strategist)")
    print("=" * 60)

    url = "http://localhost:8001/api/v1/llm/llm/generate"
    data = {
        "role": "strategist",
        "task": "campaign_strategy",
        "payload": {
            "brand": "EcoLife",
            "product": "ì¹œí™˜ê²½ í…€ë¸”ëŸ¬",
            "budget": "500ë§Œì›",
            "period": "1ê°œì›”"
        },
        "mode": "text",
        "options": {
            "temperature": 0.8
        }
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=data)
            response.raise_for_status()

            result = response.json()
            print(f"\nâœ… Status: {response.status_code}")
            print(f"\nProvider: {result['provider']}")
            print(f"Model: {result['model']}")
            print(f"\nOutput Type: {result['output']['type']}")
            print(f"Output Value (first 200 chars): {str(result['output']['value'])[:200]}...")
            print(f"\nUsage: {result['usage']}")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def test_debug_settings():
    """Debug Settings í™•ì¸"""
    print("\n" + "=" * 60)
    print("Test 4: Debug Settings")
    print("=" * 60)

    url = "http://localhost:8001/api/v1/debug/settings"

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(url)
            response.raise_for_status()

            result = response.json()
            print(f"\nâœ… Status: {response.status_code}")
            print(f"\nGenerator Mode: {result.get('generator_mode')}")
            print(f"Ollama Base URL: {result.get('ollama_base_url')}")
            print(f"Ollama Model: {result.get('ollama_default_model')}")
            print(f"ComfyUI Base URL: {result.get('comfyui_base_url')}")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def main():
    print("\nğŸš€ LLM Gateway ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    await test_debug_settings()
    await test_llm_health()
    await test_llm_json_mode()
    await test_llm_text_mode()
    
    print("\n" + "=" * 60)
    print("âœ… LLM Gateway í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
