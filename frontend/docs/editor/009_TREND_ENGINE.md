# Canvas Studio v3 â€” Trend Learning Engine

**ê´€ë ¨ ë¬¸ì„œ**: [000_MASTER_PLAN.md](./000_MASTER_PLAN.md), [002_DATA_MODEL.md](./002_DATA_MODEL.md), [008_AGENTS_INTEGRATION.md](./008_AGENTS_INTEGRATION.md)
**ì‘ì„±ì¼**: 2025-11-19

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜](#íŒŒì´í”„ë¼ì¸-ì•„í‚¤í…ì²˜)
3. [Stage 1: Collector (ë°ì´í„° ìˆ˜ì§‘)](#stage-1-collector-ë°ì´í„°-ìˆ˜ì§‘)
4. [Stage 2: Cleaner & Normalizer (ë°ì´í„° ì •ì œ)](#stage-2-cleaner--normalizer-ë°ì´í„°-ì •ì œ)
5. [Stage 3: Pattern Miner (íŒ¨í„´ ì¶”ì¶œ)](#stage-3-pattern-miner-íŒ¨í„´-ì¶”ì¶œ)
6. [Stage 4: Template Generator (í…œí”Œë¦¿ ìƒì„±)](#stage-4-template-generator-í…œí”Œë¦¿-ìƒì„±)
7. [Stage 5: Exporter (API ì œê³µ)](#stage-5-exporter-api-ì œê³µ)
8. [Learning Plan ê´€ë¦¬](#learning-plan-ê´€ë¦¬)
9. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

---

## ê°œìš”

### Trend Engineì´ë€?

**Trend Learning Engine**ì€ Sparklioì˜ í•µì‹¬ ì°¨ë³„í™” ìš”ì†Œë¡œ, **ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  í•™ìŠµí•˜ì—¬ ê³ ì„±ëŠ¥ í…œí”Œë¦¿ì„ ìƒì„±**í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ì™œ í•„ìš”í•œê°€?

**ë¬¸ì œì **:
- ìì²´ í…œí”Œë¦¿/ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•¨
- ë§ˆì¼€íŒ… íŠ¸ë Œë“œëŠ” ì‹œì¥/ì±„ë„/ì‹œê¸°ë³„ë¡œ ë¹ ë¥´ê²Œ ë³€í™”í•¨
- ìˆ˜ë™ìœ¼ë¡œ í…œí”Œë¦¿ì„ ì œì‘í•˜ë©´ íŠ¸ë Œë“œë¥¼ ë”°ë¼ê°€ê¸° ì–´ë ¤ì›€
- ì–´ë–¤ ë ˆì´ì•„ì›ƒì´ ì‹¤ì œë¡œ ì„±ê³¼ê°€ ì¢‹ì€ì§€ ì•Œê¸° ì–´ë ¤ì›€

**í•´ê²°ì±…**:
- **ìë™ í¬ë¡¤ë§**: ê³µì‹ ë ¥ ìˆëŠ” ë§ˆì¼€íŒ… ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘
- **íŒ¨í„´ í•™ìŠµ**: ì„±ê³µ ì‚¬ë¡€ì—ì„œ ë ˆì´ì•„ì›ƒ/êµ¬ì„±/ìŠ¤íƒ€ì¼ íŒ¨í„´ ì¶”ì¶œ
- **í…œí”Œë¦¿ ìë™ ìƒì„±**: í•™ìŠµí•œ íŒ¨í„´ì„ EditorDocumentë¡œ ë³€í™˜
- **ì„±ê³¼ ì¶”ì **: ìƒì„±ëœ í…œí”Œë¦¿ì˜ ì‹¤ì œ ì„±ê³¼ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì§€ì† ê°œì„ 

### í•µì‹¬ ê°€ì¹˜

1. **Zero to Hero**: ë°ì´í„°ê°€ ì—†ëŠ” ìƒíƒœì—ì„œë„ ë¹ ë¥´ê²Œ ê³ í’ˆì§ˆ í…œí”Œë¦¿ í™•ë³´
2. **Always Up-to-date**: ì‹œì¥ íŠ¸ë Œë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜
3. **Data-Driven**: ì‹¤ì œ ì„±ê³¼ ë°ì´í„° ê¸°ë°˜ì˜ í…œí”Œë¦¿ ë­í‚¹
4. **Automated Learning**: ì‚¬ëŒ ê°œì… ì—†ì´ ì§€ì†ì ìœ¼ë¡œ ê°œì„ 

---

## íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Trend Learning Pipeline                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Stage 1    â”‚      â”‚   Stage 2    â”‚      â”‚   Stage 3    â”‚
   â”‚  Collector   â”‚ â”€â”€â”€â–¶ â”‚   Cleaner    â”‚ â”€â”€â”€â–¶ â”‚Pattern Miner â”‚
   â”‚  (í¬ë¡¤ë§)     â”‚      â”‚   (ì •ì œ)      â”‚      â”‚  (íŒ¨í„´ì¶”ì¶œ)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                               â”‚
         â–¼                                               â–¼
   [Raw HTML/JSON]                              [TrendPattern[]]
         â”‚                                               â”‚
         â”‚                                               â”‚
         â–¼                                               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Stage 5    â”‚ â—€â”€â”€â”€ â”‚   Stage 4    â”‚      â”‚              â”‚
   â”‚   Exporter   â”‚      â”‚  Template    â”‚      â”‚  PostgreSQL  â”‚
   â”‚  (API ì œê³µ)   â”‚      â”‚  Generator   â”‚      â”‚  (ì €ì¥ì†Œ)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   [/api/v1/templates/auto-generate]
         â”‚
         â–¼
   [Editor v2.0 Frontend]
```

### ê¸°ìˆ  ìŠ¤íƒ

- **Backend**: Python (FastAPI)
- **í¬ë¡¤ëŸ¬**: Scrapy / Playwright (JavaScript ë Œë”ë§)
- **ì´ë¯¸ì§€ ì²˜ë¦¬**: Pillow / OpenCV
- **íŒ¨í„´ í•™ìŠµ**: scikit-learn (Clustering, Classification)
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL (ë©”íƒ€ë°ì´í„°), MinIO (ì´ë¯¸ì§€)
- **ìŠ¤ì¼€ì¤„ëŸ¬**: Celery + Redis (ì •ê¸° ì‹¤í–‰)
- **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana

---

## Stage 1: Collector (ë°ì´í„° ìˆ˜ì§‘)

### ì—­í• 

ê³µì‹ ë ¥ ìˆëŠ” ë§ˆì¼€íŒ… ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° **ì„±ê³µ ì‚¬ë¡€**ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

### ë°ì´í„° ì†ŒìŠ¤

#### 1. SNS í”Œë«í¼ ê´‘ê³  ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
DATA_SOURCES = {
    # Meta (Facebook/Instagram) ê´‘ê³  ë¼ì´ë¸ŒëŸ¬ë¦¬
    'meta_ad_library': {
        'url': 'https://www.facebook.com/ads/library/',
        'method': 'api',  # Meta Ad Library API ì‚¬ìš©
        'filters': {
            'ad_reached_countries': ['KR', 'US', 'JP'],
            'ad_active_status': 'ACTIVE',
            'ad_delivery_date_min': '2025-11-01',
            'impressions_min': 10000,  # ìµœì†Œ ë…¸ì¶œ ìˆ˜
        },
        'extract': ['image_url', 'text', 'cta_type', 'layout'],
        'frequency': 'daily'
    },

    # TikTok Creative Center
    'tiktok_creative_center': {
        'url': 'https://ads.tiktok.com/business/creativecenter',
        'method': 'scraper',  # Playwrightë¡œ ë Œë”ë§ í›„ ìŠ¤í¬ë˜í•‘
        'filters': {
            'region': ['KR', 'US'],
            'industry': ['E-commerce', 'Fashion', 'Beauty'],
            'trend_period': 'last_7_days'
        },
        'extract': ['video_thumbnail', 'caption', 'hashtags', 'music'],
        'frequency': 'daily'
    },

    # Pinterest Trends
    'pinterest_trends': {
        'url': 'https://trends.pinterest.com/',
        'method': 'scraper',
        'filters': {
            'country': 'KR',
            'category': ['fashion', 'home-decor', 'food']
        },
        'extract': ['image_url', 'description', 'category', 'search_volume'],
        'frequency': 'weekly'
    }
}
```

#### 2. ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸ & í†µê³„ ì‚¬ì´íŠ¸

```python
REPORT_SOURCES = {
    # Think with Google (ê´‘ê³  ì„±ê³¼ ë¦¬í¬íŠ¸)
    'think_with_google': {
        'url': 'https://www.thinkwithgoogle.com/',
        'extract': ['case_studies', 'best_practices', 'performance_metrics'],
        'frequency': 'weekly'
    },

    # HubSpot Marketing Statistics
    'hubspot_stats': {
        'url': 'https://www.hubspot.com/marketing-statistics',
        'extract': ['ctr_benchmarks', 'layout_performance', 'color_psychology'],
        'frequency': 'monthly'
    }
}
```

#### 3. E-commerce ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìƒì„¸í˜ì´ì§€

```python
ECOMMERCE_SOURCES = {
    # ì¿ íŒ¡ ë² ìŠ¤íŠ¸ì…€ëŸ¬
    'coupang_bestsellers': {
        'url': 'https://www.coupang.com/np/bestsellers',
        'filters': {
            'category': ['fashion', 'beauty', 'electronics'],
            'rank_max': 100  # ìƒìœ„ 100ê°œë§Œ
        },
        'extract': ['product_images', 'description_layout', 'review_section'],
        'frequency': 'daily'
    },

    # Amazon Best Sellers
    'amazon_bestsellers': {
        'url': 'https://www.amazon.com/Best-Sellers',
        'filters': {
            'category': ['All Departments'],
            'rank_max': 50
        },
        'extract': ['product_detail_layout', 'a_plus_content', 'bullet_points'],
        'frequency': 'weekly'
    }
}
```

### Collector êµ¬í˜„

```python
# backend/app/trend_engine/collector.py

from typing import List, Dict
from datetime import datetime
import asyncio
from playwright.async_api import async_playwright
from models.trend import RawTrendData, DataSource

class TrendCollector:
    """íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self, source_config: Dict):
        self.source_config = source_config
        self.collected_data: List[RawTrendData] = []

    async def collect_from_meta_ad_library(self, filters: Dict) -> List[RawTrendData]:
        """Meta Ad Library APIë¡œë¶€í„° ë°ì´í„° ìˆ˜ì§‘"""
        # Meta Graph API ì‚¬ìš©
        access_token = os.getenv('META_AD_LIBRARY_TOKEN')
        url = f"https://graph.facebook.com/v18.0/ads_archive"

        params = {
            'access_token': access_token,
            'ad_reached_countries': filters['ad_reached_countries'],
            'ad_active_status': filters['ad_active_status'],
            'ad_delivery_date_min': filters['ad_delivery_date_min'],
            'limit': 100,
            'fields': 'ad_creative_bodies,ad_creative_link_captions,ad_snapshot_url'
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                data = await response.json()

                raw_data_list = []
                for ad in data.get('data', []):
                    raw_data = RawTrendData(
                        source='meta_ad_library',
                        source_url=ad['ad_snapshot_url'],
                        market='kr' if 'KR' in filters['ad_reached_countries'] else 'global',
                        channel='instagram',
                        format='feed',
                        raw_content={
                            'body': ad.get('ad_creative_bodies', ''),
                            'caption': ad.get('ad_creative_link_captions', ''),
                            'snapshot_url': ad['ad_snapshot_url']
                        },
                        collected_at=datetime.utcnow()
                    )
                    raw_data_list.append(raw_data)

                return raw_data_list

    async def collect_from_tiktok_creative_center(self, filters: Dict) -> List[RawTrendData]:
        """TikTok Creative Centerì—ì„œ Playwrightë¡œ ìŠ¤í¬ë˜í•‘"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            await page.goto('https://ads.tiktok.com/business/creativecenter/inspiration/popular/pc/en')
            await page.wait_for_selector('.creative-card')

            # í¬ë¦¬ì—ì´í‹°ë¸Œ ì¹´ë“œ ìŠ¤í¬ë˜í•‘
            cards = await page.query_selector_all('.creative-card')
            raw_data_list = []

            for card in cards[:20]:  # ìƒìœ„ 20ê°œë§Œ
                try:
                    thumbnail = await card.query_selector('img')
                    thumbnail_url = await thumbnail.get_attribute('src')

                    caption_elem = await card.query_selector('.caption')
                    caption = await caption_elem.inner_text()

                    raw_data = RawTrendData(
                        source='tiktok_creative_center',
                        source_url='https://ads.tiktok.com/business/creativecenter',
                        market=filters['region'][0].lower(),
                        channel='tiktok',
                        format='short',
                        raw_content={
                            'thumbnail_url': thumbnail_url,
                            'caption': caption
                        },
                        collected_at=datetime.utcnow()
                    )
                    raw_data_list.append(raw_data)

                except Exception as e:
                    print(f"Error extracting card: {e}")
                    continue

            await browser.close()
            return raw_data_list

    async def run_collection(self, source_name: str) -> List[RawTrendData]:
        """íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        config = self.source_config[source_name]

        if config['method'] == 'api':
            if source_name == 'meta_ad_library':
                return await self.collect_from_meta_ad_library(config['filters'])

        elif config['method'] == 'scraper':
            if source_name == 'tiktok_creative_center':
                return await self.collect_from_tiktok_creative_center(config['filters'])

        return []

# Celery Task
@celery_app.task
def collect_trend_data(source_name: str):
    """Celeryë¡œ ì •ê¸° ì‹¤í–‰ë˜ëŠ” ìˆ˜ì§‘ íƒœìŠ¤í¬"""
    collector = TrendCollector(DATA_SOURCES)
    raw_data_list = asyncio.run(collector.run_collection(source_name))

    # DB ì €ì¥
    for raw_data in raw_data_list:
        db.session.add(raw_data)
    db.session.commit()

    return {
        'source': source_name,
        'collected_count': len(raw_data_list),
        'timestamp': datetime.utcnow().isoformat()
    }
```

### Learning Plan (ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„)

```python
# backend/app/models/learning_plan.py

from sqlalchemy import Column, String, JSON, Boolean, DateTime
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class LearningPlan(Base):
    """í•™ìŠµ ê³„íš (ê´€ë¦¬ìê°€ ì„¤ì •)"""
    __tablename__ = 'learning_plans'

    id = Column(String, primary_key=True)
    name = Column(String, nullable=False)  # "Instagram Reels (KR)"
    description = Column(String)
    source_name = Column(String, nullable=False)  # DATA_SOURCESì˜ í‚¤
    schedule = Column(String, nullable=False)  # "0 9 * * *" (ë§¤ì¼ ì˜¤ì „ 9ì‹œ)
    enabled = Column(Boolean, default=True)
    filters = Column(JSON)  # ì¶”ê°€ í•„í„°
    created_at = Column(DateTime)
    updated_at = Column(DateTime)

# ì˜ˆì‹œ
learning_plan = LearningPlan(
    id='plan-ig-reels-kr',
    name='Instagram Reels (KR) - Daily',
    description='í•œêµ­ ì‹œì¥ ì¸ìŠ¤íƒ€ê·¸ë¨ ë¦´ìŠ¤ ìµœê·¼ 7ì¼ CTR > 5% ê²Œì‹œë¬¼ ìˆ˜ì§‘',
    source_name='meta_ad_library',
    schedule='0 9 * * *',  # ë§¤ì¼ ì˜¤ì „ 9ì‹œ
    enabled=True,
    filters={
        'ad_reached_countries': ['KR'],
        'impressions_min': 10000,
        'ctr_min': 5.0
    }
)
```

---

## Stage 2: Cleaner & Normalizer (ë°ì´í„° ì •ì œ)

### ì—­í• 

ìˆ˜ì§‘í•œ ì›ì‹œ ë°ì´í„°ë¥¼ **ì •ì œí•˜ê³  ì •ê·œí™”**í•˜ì—¬ íŒ¨í„´ ì¶”ì¶œì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

### ì²˜ë¦¬ ê³¼ì •

```python
# backend/app/trend_engine/cleaner.py

from typing import List
from models.trend import RawTrendData, CleanedTrendData
from PIL import Image
import requests
from io import BytesIO

class TrendCleaner:
    """ë°ì´í„° ì •ì œê¸°"""

    def clean_raw_data(self, raw_data: RawTrendData) -> CleanedTrendData:
        """ì›ì‹œ ë°ì´í„° ì •ì œ"""

        # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„
        image_analysis = self.analyze_image(raw_data.raw_content.get('thumbnail_url'))

        # 2. í…ìŠ¤íŠ¸ ì •ì œ
        clean_text = self.clean_text(raw_data.raw_content.get('body', ''))

        # 3. ë ˆì´ì•„ì›ƒ ë¶„ì„
        layout_info = self.analyze_layout(image_analysis)

        # 4. CleanedTrendData ìƒì„±
        cleaned_data = CleanedTrendData(
            raw_data_id=raw_data.id,
            market=raw_data.market,
            channel=raw_data.channel,
            format=raw_data.format,

            # ì´ë¯¸ì§€ ì •ë³´
            image_url=raw_data.raw_content.get('thumbnail_url'),
            image_width=image_analysis['width'],
            image_height=image_analysis['height'],
            dominant_colors=image_analysis['dominant_colors'],

            # í…ìŠ¤íŠ¸ ì •ë³´
            text_content=clean_text,
            text_length=len(clean_text),
            has_emoji=self.detect_emoji(clean_text),

            # ë ˆì´ì•„ì›ƒ ì •ë³´
            layout_type=layout_info['type'],  # 'left-image-right-text', 'hero-center', etc.
            text_position=layout_info['text_position'],  # 'top', 'bottom', 'left', 'right'
            text_area_ratio=layout_info['text_area_ratio'],  # 0.3 (30%)

            # ë©”íƒ€ë°ì´í„°
            source_url=raw_data.source_url,
            cleaned_at=datetime.utcnow()
        )

        return cleaned_data

    def analyze_image(self, image_url: str) -> dict:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„"""
        try:
            response = requests.get(image_url)
            img = Image.open(BytesIO(response.content))

            # í¬ê¸°
            width, height = img.size

            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (K-means clustering)
            pixels = np.array(img.convert('RGB'))
            pixels = pixels.reshape(-1, 3)

            kmeans = KMeans(n_clusters=5, random_state=42)
            kmeans.fit(pixels)
            dominant_colors = kmeans.cluster_centers_.astype(int).tolist()

            # MinIOì— ì €ì¥
            img_bytes = BytesIO()
            img.save(img_bytes, format='JPEG')
            img_bytes.seek(0)

            minio_client.put_object(
                bucket_name='trend-images',
                object_name=f"{uuid.uuid4()}.jpg",
                data=img_bytes,
                length=img_bytes.getbuffer().nbytes,
                content_type='image/jpeg'
            )

            return {
                'width': width,
                'height': height,
                'aspect_ratio': width / height,
                'dominant_colors': dominant_colors
            }

        except Exception as e:
            print(f"Image analysis failed: {e}")
            return {}

    def analyze_layout(self, image_analysis: dict) -> dict:
        """ë ˆì´ì•„ì›ƒ íƒ€ì… ë¶„ì„"""
        aspect_ratio = image_analysis.get('aspect_ratio', 1.0)

        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± (ì‹¤ì œë¡œëŠ” CV ëª¨ë¸ ì‚¬ìš©)
        if aspect_ratio > 1.5:  # ê°€ë¡œë¡œ ê¸´ ì´ë¯¸ì§€
            return {
                'type': 'left-image-right-text',
                'text_position': 'right',
                'text_area_ratio': 0.4
            }
        elif aspect_ratio < 0.7:  # ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€
            return {
                'type': 'top-image-bottom-text',
                'text_position': 'bottom',
                'text_area_ratio': 0.3
            }
        else:  # ì •ì‚¬ê°í˜• ê·¼ì²˜
            return {
                'type': 'hero-center',
                'text_position': 'center',
                'text_area_ratio': 0.2
            }

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = ' '.join(text.split())
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì´ëª¨ì§€ëŠ” ìœ ì§€)
        return text.strip()

    def detect_emoji(self, text: str) -> bool:
        """ì´ëª¨ì§€ í¬í•¨ ì—¬ë¶€ ê°ì§€"""
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "]+", flags=re.UNICODE
        )
        return bool(emoji_pattern.search(text))
```

---

## Stage 3: Pattern Miner (íŒ¨í„´ ì¶”ì¶œ)

### ì—­í• 

ì •ì œëœ ë°ì´í„°ë¡œë¶€í„° **ì„±ê³µ íŒ¨í„´**ì„ ì¶”ì¶œí•˜ì—¬ `TrendPattern` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### íŒ¨í„´ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜

```python
# backend/app/trend_engine/pattern_miner.py

from typing import List
from models.trend import CleanedTrendData, TrendPattern
from sklearn.cluster import DBSCAN
from collections import Counter

class PatternMiner:
    """íŒ¨í„´ ì¶”ì¶œ ì—”ì§„"""

    def mine_patterns(
        self,
        cleaned_data_list: List[CleanedTrendData],
        market: str,
        channel: str,
        format: str
    ) -> List[TrendPattern]:
        """íŒ¨í„´ ì¶”ì¶œ ì‹¤í–‰"""

        # 1. ë ˆì´ì•„ì›ƒ íƒ€ì…ë³„ ê·¸ë£¹í™”
        layout_groups = self.group_by_layout(cleaned_data_list)

        # 2. ê° ê·¸ë£¹ì—ì„œ íŒ¨í„´ ì¶”ì¶œ
        patterns = []
        for layout_type, group_data in layout_groups.items():
            if len(group_data) < 10:  # ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”
                continue

            pattern = self.extract_pattern(
                layout_type=layout_type,
                data_list=group_data,
                market=market,
                channel=channel,
                format=format
            )
            patterns.append(pattern)

        # 3. ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°
        patterns = self.calculate_popularity(patterns)

        return patterns

    def group_by_layout(self, data_list: List[CleanedTrendData]) -> dict:
        """ë ˆì´ì•„ì›ƒ íƒ€ì…ë³„ ê·¸ë£¹í™”"""
        groups = {}
        for data in data_list:
            layout_type = data.layout_type
            if layout_type not in groups:
                groups[layout_type] = []
            groups[layout_type].append(data)
        return groups

    def extract_pattern(
        self,
        layout_type: str,
        data_list: List[CleanedTrendData],
        market: str,
        channel: str,
        format: str
    ) -> TrendPattern:
        """ë‹¨ì¼ íŒ¨í„´ ì¶”ì¶œ"""

        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_text_area_ratio = np.mean([d.text_area_ratio for d in data_list])
        avg_image_aspect_ratio = np.mean([d.image_width / d.image_height for d in data_list])

        # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ìƒ‰ìƒ)
        all_colors = []
        for d in data_list:
            all_colors.extend(d.dominant_colors or [])

        # ë ˆì´ì•„ì›ƒ êµ¬ì¡° ì¶”ë¡ 
        layout_structure = self.infer_layout_structure(data_list)

        # ìƒ˜í”Œ ì†ŒìŠ¤ ìˆ˜ì§‘
        sample_sources = [d.source_url for d in data_list[:10]]  # ìƒìœ„ 10ê°œ

        # TrendPattern ìƒì„±
        pattern = TrendPattern(
            id=f"trend-{channel}-{market}-{layout_type}-{datetime.utcnow().strftime('%Y%m')}",
            name=f"{channel.title()} {format.title()} - {layout_type.replace('-', ' ').title()} ({market.upper()} {datetime.utcnow().strftime('%Y-%m')})",
            market=market,
            channel=channel,
            format=format,
            layout_pattern=layout_type,
            layout_structure=layout_structure,
            popularity_score=0,  # ë‚˜ì¤‘ì— ê³„ì‚°
            performance_metrics={
                'avgCtr': None,  # ì‹¤ì œ ì„±ê³¼ ë°ì´í„°ëŠ” Publishing í›„ ìˆ˜ì§‘
                'avgEngagement': None,
                'sampleSize': len(data_list)
            },
            sample_sources=sample_sources,
            collected_at=datetime.utcnow(),
            valid_until=datetime.utcnow() + timedelta(days=90),  # 3ê°œì›” ìœ íš¨
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )

        return pattern

    def infer_layout_structure(self, data_list: List[CleanedTrendData]) -> dict:
        """ë ˆì´ì•„ì›ƒ êµ¬ì¡° ì¶”ë¡ """
        # í†µê³„ ê¸°ë°˜ êµ¬ì¡° ì¶”ë¡ 
        text_positions = Counter([d.text_position for d in data_list])
        most_common_position = text_positions.most_common(1)[0][0]

        # ObjectRole ë§¤í•‘ (íœ´ë¦¬ìŠ¤í‹±)
        sections = []

        if 'image' in data_list[0].layout_type:
            sections.append({
                'role': 'product-image',
                'position': 'left' if 'left' in data_list[0].layout_type else 'top',
                'sizeRatio': 0.5
            })

        sections.append({
            'role': 'headline',
            'position': most_common_position,
            'sizeRatio': 0.3
        })

        sections.append({
            'role': 'cta-button',
            'position': 'bottom',
            'sizeRatio': 0.2
        })

        return {'sections': sections}

    def calculate_popularity(self, patterns: List[TrendPattern]) -> List[TrendPattern]:
        """ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°"""
        # ìƒ˜í”Œ í¬ê¸° ê¸°ë°˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        max_sample_size = max([p.performance_metrics.get('sampleSize', 0) for p in patterns])

        for pattern in patterns:
            sample_size = pattern.performance_metrics.get('sampleSize', 0)
            # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìˆ˜ (0-100)
            if sample_size > 0:
                score = min(100, int(math.log(sample_size + 1) / math.log(max_sample_size + 1) * 100))
                pattern.popularity_score = score
            else:
                pattern.popularity_score = 0

        return patterns
```

---

## Stage 4: Template Generator (í…œí”Œë¦¿ ìƒì„±)

### ì—­í• 

`TrendPattern`ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ `TemplateDefinition` (EditorDocument)ì„ ìƒì„±í•©ë‹ˆë‹¤.

### í…œí”Œë¦¿ ìƒì„± ë¡œì§

```python
# backend/app/trend_engine/template_generator.py

from typing import List
from models.trend import TrendPattern
from models.editor import TemplateDefinition, EditorPage, EditorObject

class TemplateGenerator:
    """í…œí”Œë¦¿ ìë™ ìƒì„±ê¸°"""

    def generate_template(
        self,
        pattern: TrendPattern,
        brand_tokens: Optional[DesignTokens] = None
    ) -> TemplateDefinition:
        """TrendPattern â†’ TemplateDefinition ë³€í™˜"""

        # 1. í˜ì´ì§€ ìƒì„±
        page = self.create_page_from_pattern(pattern)

        # 2. ë¸Œëœë“œ í† í° ì ìš© (ì˜µì…˜)
        if brand_tokens:
            page = self.apply_brand_tokens(page, brand_tokens)

        # 3. TemplateDefinition ìƒì„±
        template = TemplateDefinition(
            id=f"tpl-{pattern.id}",
            name=pattern.name,
            description=f"íŠ¸ë Œë“œ ê¸°ë°˜ ìë™ ìƒì„± í…œí”Œë¦¿ (ì¸ê¸°ë„: {pattern.popularity_score}/100)",
            category=self.map_channel_to_category(pattern.channel),
            tags=[pattern.channel, pattern.format, pattern.market, pattern.layout_pattern],

            mode=self.map_channel_to_mode(pattern.channel),
            pages=[page],
            tokens=brand_tokens,

            # íŠ¸ë Œë“œ ì—°ë™
            trend_pattern=pattern,
            popularity_score=pattern.popularity_score,
            performance_metrics={
                'avgCtr': pattern.performance_metrics.get('avgCtr'),
                'avgCvr': pattern.performance_metrics.get('avgCvr'),
                'usageCount': 0
            },

            thumbnail=None,  # ë‚˜ì¤‘ì— ë Œë”ë§
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            created_by='system'
        )

        return template

    def create_page_from_pattern(self, pattern: TrendPattern) -> EditorPage:
        """íŒ¨í„´ìœ¼ë¡œë¶€í„° EditorPage ìƒì„±"""

        # í˜ì´ì§€ í¬ê¸° ê²°ì •
        page_size = self.get_page_size(pattern.channel, pattern.format)

        # ê°ì²´ ìƒì„±
        objects = []
        for section in pattern.layout_structure.get('sections', []):
            obj = self.create_object_from_section(section, page_size)
            objects.append(obj)

        # EditorPage ìƒì„±
        page = EditorPage(
            id=f"page-{uuid.uuid4()}",
            name=f"{pattern.channel.title()} {pattern.format.title()}",
            kind='ad' if 'ad' in pattern.channel else 'social',
            width=page_size['width'],
            height=page_size['height'],
            objects=objects,
            background={'type': 'color', 'color': '#FFFFFF'}
        )

        return page

    def create_object_from_section(self, section: dict, page_size: dict) -> EditorObject:
        """ì„¹ì…˜ ì •ë³´ â†’ EditorObject ë³€í™˜"""
        role = section['role']
        position = section['position']
        size_ratio = section['sizeRatio']

        # ìœ„ì¹˜ ê³„ì‚°
        x, y, width, height = self.calculate_bounds(
            position, size_ratio, page_size['width'], page_size['height']
        )

        # ObjectRoleì— ë”°ë¼ ê°ì²´ íƒ€ì… ê²°ì •
        if role in ['headline', 'subheadline', 'body', 'caption', 'price', 'cta-text']:
            return TextObject(
                id=f"obj-{uuid.uuid4()}",
                type='text',
                role=role,
                source={'kind': 'auto-generated', 'trendId': pattern.id},
                name=role.replace('-', ' ').title(),
                x=x,
                y=y,
                width=width,
                height=height,
                rotation=0,
                opacity=1.0,
                visible=True,
                locked=False,

                text=f"{{{{{role}}}}}",  # í”Œë ˆì´ìŠ¤í™€ë”
                fontSize=self.get_default_font_size(role),
                fontFamily='Pretendard',
                fontWeight='bold' if role == 'headline' else 'normal',
                textAlign='left',
                fill='#000000'
            )

        elif role in ['product-image', 'hero-image', 'logo', 'icon']:
            return ImageObject(
                id=f"obj-{uuid.uuid4()}",
                type='image',
                role=role,
                source={'kind': 'auto-generated', 'trendId': pattern.id},
                name=role.replace('-', ' ').title(),
                x=x,
                y=y,
                width=width,
                height=height,
                rotation=0,
                opacity=1.0,
                visible=True,
                locked=False,

                src='placeholder.jpg',
                fit='cover',
                placeholder=True
            )

        elif role in ['cta-button', 'badge']:
            return ShapeObject(
                id=f"obj-{uuid.uuid4()}",
                type='shape',
                role=role,
                source={'kind': 'auto-generated', 'trendId': pattern.id},
                name=role.replace('-', ' ').title(),
                x=x,
                y=y,
                width=width,
                height=height,
                rotation=0,
                opacity=1.0,
                visible=True,
                locked=False,

                shapeType='rect',
                fill='#FF5733',
                cornerRadius=8
            )

        else:
            # ê¸°ë³¸ ë„í˜•
            return ShapeObject(...)

    def calculate_bounds(self, position: str, size_ratio: float, page_width: int, page_height: int) -> tuple:
        """ìœ„ì¹˜ ë¬¸ìì—´ â†’ ì‹¤ì œ ì¢Œí‘œ ë³€í™˜"""
        if position == 'left':
            return (0, 0, int(page_width * size_ratio), page_height)
        elif position == 'right':
            width = int(page_width * size_ratio)
            return (page_width - width, 0, width, page_height)
        elif position == 'top':
            height = int(page_height * size_ratio)
            return (0, 0, page_width, height)
        elif position == 'bottom':
            height = int(page_height * size_ratio)
            return (0, page_height - height, page_width, height)
        elif position == 'center':
            width = int(page_width * size_ratio)
            height = int(page_height * size_ratio)
            x = (page_width - width) // 2
            y = (page_height - height) // 2
            return (x, y, width, height)
        else:
            return (0, 0, 100, 100)

    def get_page_size(self, channel: str, format: str) -> dict:
        """ì±„ë„/í¬ë§· â†’ í˜ì´ì§€ í¬ê¸° ë§¤í•‘"""
        PAGE_SIZES = {
            ('instagram', 'feed'): {'width': 1080, 'height': 1080},
            ('instagram', 'story'): {'width': 1080, 'height': 1920},
            ('instagram', 'reels'): {'width': 1080, 'height': 1920},
            ('tiktok', 'short'): {'width': 1080, 'height': 1920},
            ('facebook', 'feed'): {'width': 1200, 'height': 630},
        }
        return PAGE_SIZES.get((channel, format), {'width': 1080, 'height': 1350})

    def get_default_font_size(self, role: str) -> int:
        """ì—­í•  â†’ ê¸°ë³¸ í°íŠ¸ í¬ê¸° ë§¤í•‘"""
        FONT_SIZES = {
            'headline': 48,
            'subheadline': 32,
            'body': 16,
            'caption': 14,
            'price': 36,
            'cta-text': 20
        }
        return FONT_SIZES.get(role, 16)
```

---

## Stage 5: Exporter (API ì œê³µ)

### API ì—”ë“œí¬ì¸íŠ¸: `/api/v1/templates/auto-generate`

```python
# backend/app/api/v1/templates.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from models.editor import EditorDocument, TemplateDefinition

router = APIRouter()

class AutoGenerateRequest(BaseModel):
    brandId: str
    contentType: str  # 'instagram-story', 'product-detail', etc.
    market: str  # 'kr', 'us', 'jp', 'global'
    count: int = 5  # ìƒì„±í•  ë³€í˜• ìˆ˜
    trendPreference: str = 'medium'  # 'high', 'medium', 'low'

class AutoGenerateResponse(BaseModel):
    documents: List[EditorDocument]
    templates_used: List[str]  # ì‚¬ìš©ëœ í…œí”Œë¦¿ ID
    generation_time: float

@router.post('/auto-generate', response_model=AutoGenerateResponse)
async def auto_generate_templates(request: AutoGenerateRequest):
    """
    íŠ¸ë Œë“œ ê¸°ë°˜ í…œí”Œë¦¿ ìë™ ìƒì„± API

    **í”Œë¡œìš°**:
    1. ë¸Œëœë“œ ì •ë³´ ë¡œë“œ (DesignTokens)
    2. íŠ¸ë Œë“œ íŒ¨í„´ ì¡°íšŒ (market, channel, trendPreference ê¸°ì¤€)
    3. ìƒìœ„ Nê°œ íŒ¨í„´ìœ¼ë¡œ í…œí”Œë¦¿ ìƒì„±
    4. ê° í…œí”Œë¦¿ì„ EditorDocumentë¡œ ë³€í™˜
    5. ë¸Œëœë“œ í† í° ì ìš©
    """
    start_time = time.time()

    # 1. ë¸Œëœë“œ ì •ë³´ ë¡œë“œ
    brand = db.query(Brand).filter(Brand.id == request.brandId).first()
    if not brand:
        raise HTTPException(status_code=404, detail="Brand not found")

    brand_tokens = brand.design_tokens

    # 2. contentType â†’ (channel, format) ë§¤í•‘
    channel, format = parse_content_type(request.contentType)

    # 3. íŠ¸ë Œë“œ íŒ¨í„´ ì¡°íšŒ (ì¸ê¸°ë„ ìˆœ)
    trend_patterns = db.query(TrendPattern).filter(
        TrendPattern.market == request.market,
        TrendPattern.channel == channel,
        TrendPattern.format == format
    ).order_by(TrendPattern.popularity_score.desc()).limit(request.count).all()

    if not trend_patterns:
        raise HTTPException(status_code=404, detail="No trend patterns found")

    # 4. ê° íŒ¨í„´ìœ¼ë¡œ í…œí”Œë¦¿ ìƒì„±
    generator = TemplateGenerator()
    documents = []
    template_ids = []

    for i, pattern in enumerate(trend_patterns):
        # TemplateDefinition ìƒì„±
        template = generator.generate_template(pattern, brand_tokens)

        # EditorDocumentë¡œ ë³€í™˜
        doc = EditorDocument(
            id=f"doc-{uuid.uuid4()}",
            title=f"{request.contentType.replace('-', ' ').title()} - Variant {i+1}",
            mode=template.mode,
            brandId=request.brandId,
            pages=template.pages,
            tokens=brand_tokens,
            createdAt=datetime.utcnow().isoformat(),
            updatedAt=datetime.utcnow().isoformat(),
            source={
                'kind': 'auto-generated',
                'sourceId': template.id
            },

            # ì„œë¹„ìŠ¤ ë ˆë²¨ í•„ë“œ
            templateId=template.id,
            trendSnapshotId=pattern.id,
            variantId=f"variant-{i+1}"
        )

        documents.append(doc)
        template_ids.append(template.id)

    generation_time = time.time() - start_time

    return AutoGenerateResponse(
        documents=documents,
        templates_used=template_ids,
        generation_time=generation_time
    )

def parse_content_type(content_type: str) -> tuple:
    """contentType ë¬¸ìì—´ â†’ (channel, format) íŒŒì‹±"""
    CONTENT_TYPE_MAP = {
        'instagram-feed': ('instagram', 'feed'),
        'instagram-story': ('instagram', 'story'),
        'instagram-reels': ('instagram', 'reels'),
        'tiktok-short': ('tiktok', 'short'),
        'facebook-feed': ('facebook', 'feed'),
        'product-detail': ('blog', 'section'),  # E-commerce
    }
    return CONTENT_TYPE_MAP.get(content_type, ('instagram', 'feed'))
```

### ì‚¬ìš© ì˜ˆì‹œ

```typescript
// Frontend: Auto Template ìƒì„± ìš”ì²­

const response = await fetch('/api/v1/templates/auto-generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    brandId: 'nike-kr',
    contentType: 'instagram-story',
    market: 'kr',
    count: 5,
    trendPreference: 'high'
  })
});

const data = await response.json();
// data.documents = EditorDocument[] (5ê°œ ë³€í˜•)

// Editorì— ë¡œë“œ
useEditorStore.getState().loadDocument(data.documents[0]);
```

---

## Learning Plan ê´€ë¦¬

### ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ

```python
# backend/app/api/v1/admin/learning_plans.py

@router.get('/learning-plans')
async def list_learning_plans():
    """Learning Plan ëª©ë¡ ì¡°íšŒ"""
    plans = db.query(LearningPlan).all()
    return plans

@router.post('/learning-plans')
async def create_learning_plan(plan: LearningPlanCreate):
    """ìƒˆ Learning Plan ìƒì„±"""
    new_plan = LearningPlan(**plan.dict())
    db.add(new_plan)
    db.commit()

    # Celery Beat ìŠ¤ì¼€ì¤„ ë“±ë¡
    register_celery_schedule(new_plan)

    return new_plan

@router.put('/learning-plans/{plan_id}/toggle')
async def toggle_learning_plan(plan_id: str):
    """Learning Plan í™œì„±í™”/ë¹„í™œì„±í™”"""
    plan = db.query(LearningPlan).filter(LearningPlan.id == plan_id).first()
    plan.enabled = not plan.enabled
    db.commit()
    return plan

def register_celery_schedule(plan: LearningPlan):
    """Celery Beat ìŠ¤ì¼€ì¤„ ë™ì  ë“±ë¡"""
    from celery.schedules import crontab

    celery_app.conf.beat_schedule[f"collect-{plan.id}"] = {
        'task': 'app.trend_engine.collector.collect_trend_data',
        'schedule': crontab(*plan.schedule.split()),  # '0 9 * * *' â†’ crontab(0, 9)
        'args': (plan.source_name,)
    }
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬

```python
# í•œ ë²ˆì— 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
BATCH_SIZE = 100

@celery_app.task
def process_cleaned_data_batch(cleaned_data_ids: List[str]):
    """CleanedData â†’ TrendPattern ë°°ì¹˜ ì¶”ì¶œ"""
    data_list = db.query(CleanedTrendData).filter(
        CleanedTrendData.id.in_(cleaned_data_ids)
    ).all()

    pattern_miner = PatternMiner()
    patterns = pattern_miner.mine_patterns(data_list, 'kr', 'instagram', 'feed')

    for pattern in patterns:
        db.add(pattern)

    db.commit()
```

### 2. ìºì‹±

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_top_trend_patterns(market: str, channel: str, format: str, limit: int = 10) -> List[TrendPattern]:
    """ì¸ê¸° íŠ¸ë Œë“œ íŒ¨í„´ ìºì‹± (1ì‹œê°„)"""
    return db.query(TrendPattern).filter(
        TrendPattern.market == market,
        TrendPattern.channel == channel,
        TrendPattern.format == format
    ).order_by(TrendPattern.popularity_score.desc()).limit(limit).all()
```

### 3. ë¹„ë™ê¸° ì²˜ë¦¬

```python
# í¬ë¡¤ë§ì€ ë¹„ë™ê¸°ë¡œ ë³‘ë ¬ ì‹¤í–‰
async def collect_all_sources():
    tasks = []
    for source_name in DATA_SOURCES.keys():
        task = collector.run_collection(source_name)
        tasks.append(task)

    results = await asyncio.gather(*tasks)
    return results
```

---

**ë¬¸ì„œ ë²„ì „**: v3.0.0
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-19
